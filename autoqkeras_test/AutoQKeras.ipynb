{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Copyright 2020 Google LLC\n",
    "#\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QC9sVuNrzT-f"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we show how to quantize a model using AutoQKeras.\n",
    "\n",
    "As usual, let's first make sure we are using Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1591840345558,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "0sY-O2IfzdB3",
    "outputId": "1c5a4e7a-1003-4b56-a30a-ca6bc196f18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6V7FxYH0zfY0"
   },
   "source": [
    "Now, let's load some packages we will need to run AutoQKeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuVqOAcbz3Go"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 11:50:29.402097: I tensorflow/core/util/util.cc:168] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-06-21 11:50:29.409510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-21 11:50:29.409647: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using tensorflow 2.10.0-dev20220407\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import six\n",
    "import tempfile\n",
    "import tensorflow.compat.v2 as tf\n",
    "# V2 Behavior is necessary to use TF2 APIs before TF2 is default TF version internally.\n",
    "tf.enable_v2_behavior()\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from qkeras.autoqkeras import *\n",
    "from qkeras import *\n",
    "from qkeras.utils import model_quantize\n",
    "from qkeras.qtools import run_qtools\n",
    "from qkeras.qtools import settings as qtools_settings\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"using tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define `get_data` and `get_model` as you may not have stand alone access to examples directory inside autoqkeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_name, fast=False):\n",
    "  \"\"\"Returns dataset from tfds.\"\"\"\n",
    "  ds_train = tfds.load(name=dataset_name, split=\"train\", batch_size=-1)\n",
    "  ds_test = tfds.load(name=dataset_name, split=\"test\", batch_size=-1)\n",
    "\n",
    "  dataset = tfds.as_numpy(ds_train)\n",
    "  x_train, y_train = dataset[\"image\"].astype(np.float32), dataset[\"label\"]\n",
    "\n",
    "  dataset = tfds.as_numpy(ds_test)\n",
    "  x_test, y_test = dataset[\"image\"].astype(np.float32), dataset[\"label\"]\n",
    "\n",
    "  if len(x_train.shape) == 3:\n",
    "    x_train = x_train.reshape(x_train.shape + (1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "  x_train /= 256.0\n",
    "  x_test /= 256.0\n",
    "\n",
    "  x_mean = np.mean(x_train, axis=0)\n",
    "\n",
    "  x_train -= x_mean\n",
    "  x_test -= x_mean\n",
    "\n",
    "  nb_classes = np.max(y_train) + 1\n",
    "  y_train = to_categorical(y_train, nb_classes)\n",
    "  y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "  print(x_train.shape[0], \"train samples\")\n",
    "  print(x_test.shape[0], \"test samples\")\n",
    "  return (x_train, y_train), (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 16:13:53.616478: I tensorflow/core/util/util.cc:168] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-06-23 16:13:53.622138: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-23 16:13:53.622155: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_136401/2456374014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mboosted_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../pb_file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mboosted_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "boosted_model =  tf.keras.models.load_model('../pb_file')\n",
    "\n",
    "boosted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXsGtqRcN7fY"
   },
   "source": [
    "`AutoQKeras` has some examples on how to run with `mnist`, `fashion_mnist`, `cifar10` and `cifar100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18554,
     "status": "ok",
     "timestamp": 1591840377936,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "lB8CBTehz9FK",
    "outputId": "09f791cf-8db5-40c5-b17d-89d433308716"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 11:50:59.532103: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /atlas/bonnet/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85611f33f1f84b2aaf2e57551eea8270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8850f213db4e9b80455c349c62465b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73bf52e4c034eb59092fc626e4ff184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080156951c2b44c1ba20945c3a5c8c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce117d833a5c41ec8c29629939391f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 11:51:01.894280: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-21 11:51:01.894318: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-21 11:51:01.894345: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martop): /proc/driver/nvidia/version does not exist\n",
      "2022-06-21 11:51:01.894830: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bda6783a394bb297a417af1a94381b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /atlas/bonnet/tensorflow_datasets/mnist/3.0.1.incompleteDRUTR4/mnist-train.tfrecord*...:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23446ef981a94beca7cd71bbe01eb8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966b63e4bf7849aeaa75289899204188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /atlas/bonnet/tensorflow_datasets/mnist/3.0.1.incompleteDRUTR4/mnist-test.tfrecord*...:   0%|       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /atlas/bonnet/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"mnist\"\n",
    "(x_train, y_train), (x_test, y_test) = get_data(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bk4rOks2OIbW"
   },
   "source": [
    "Before we create the model, let's see if we can perform distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1591840378251,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "EMbYcKb-wMOc",
    "outputId": "22e85769-4659-4212-ccdb-4b00be2fcefe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "for d in physical_devices:\n",
    "  print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14553,
     "status": "ok",
     "timestamp": 1591840392823,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "eMVill0TxUuG",
    "outputId": "97c07213-fdce-4eed-9af7-cc51393cd996"
   },
   "outputs": [],
   "source": [
    "has_tpus = np.any([d.device_type == \"TPU\" for d in physical_devices])\n",
    "\n",
    "if has_tpus:\n",
    "  TPU_WORKER = 'local'\n",
    "\n",
    "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "      tpu=TPU_WORKER, job_name='tpu_worker')\n",
    "  if TPU_WORKER != 'local':\n",
    "    tf.config.experimental_connect_to_cluster(resolver, protocol='grpc+loas')\n",
    "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "  print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "  cur_strategy = strategy\n",
    "else:\n",
    "  cur_strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FIAmXgOOPWg"
   },
   "source": [
    "Now we can create the model with the distributed strategy in place if TPUs are available. We have some test models that we can use, or you can build your own models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "height": 977
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1591840393983,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "P0_-26kwxZiQ",
    "outputId": "bf2828fe-2968-4d7d-82e7-0e2b87f063ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_0 (Conv2D)           (None, 14, 14, 16)        144       \n",
      "                                                                 \n",
      " bn_0 (BatchNormalization)   (None, 14, 14, 16)        64        \n",
      "                                                                 \n",
      " act_0 (Activation)          (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " drop_0 (Dropout)            (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 32)          4608      \n",
      "                                                                 \n",
      " bn_1 (BatchNormalization)   (None, 7, 7, 32)          128       \n",
      "                                                                 \n",
      " act_1 (Activation)          (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " drop_1 (Dropout)            (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 48)          13824     \n",
      "                                                                 \n",
      " bn_2 (BatchNormalization)   (None, 4, 4, 48)          192       \n",
      "                                                                 \n",
      " act_2 (Activation)          (None, 4, 4, 48)          0         \n",
      "                                                                 \n",
      " drop_2 (Dropout)            (None, 4, 4, 48)          0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 64)          27648     \n",
      "                                                                 \n",
      " bn_3 (BatchNormalization)   (None, 2, 2, 64)          256       \n",
      "                                                                 \n",
      " act_3 (Activation)          (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " drop_3 (Dropout)            (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1, 1, 128)         73728     \n",
      "                                                                 \n",
      " bn_4 (BatchNormalization)   (None, 1, 1, 128)         512       \n",
      "                                                                 \n",
      " act_4 (Activation)          (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " drop_4 (Dropout)            (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,394\n",
      "Trainable params: 121,818\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with cur_strategy.scope():\n",
    "  model = get_model(DATASET)\n",
    "  custom_objects = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jok7tJq1OVuJ"
   },
   "source": [
    "Let's see the accuracy on a unquantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "height": 360
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10292,
     "status": "ok",
     "timestamp": 1591840404285,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "nvFSJpeDxmWZ",
    "outputId": "ceac171d-2357-4d2a-ecbe-6c2775bc2a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 6s 111ms/step - loss: 0.5309 - acc: 0.8321 - val_loss: 1.1922 - val_acc: 0.8741\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 3s 91ms/step - loss: 0.1657 - acc: 0.9485 - val_loss: 0.2747 - val_acc: 0.9313\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 3s 100ms/step - loss: 0.1230 - acc: 0.9611 - val_loss: 0.1031 - val_acc: 0.9681\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 3s 107ms/step - loss: 0.1030 - acc: 0.9676 - val_loss: 0.1136 - val_acc: 0.9630\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 3s 93ms/step - loss: 0.0883 - acc: 0.9719 - val_loss: 0.0648 - val_acc: 0.9786\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 3s 89ms/step - loss: 0.0842 - acc: 0.9741 - val_loss: 0.0657 - val_acc: 0.9788\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 3s 91ms/step - loss: 0.0745 - acc: 0.9770 - val_loss: 0.0443 - val_acc: 0.9861\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 3s 104ms/step - loss: 0.0726 - acc: 0.9771 - val_loss: 0.0389 - val_acc: 0.9871\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 3s 88ms/step - loss: 0.0655 - acc: 0.9793 - val_loss: 0.0386 - val_acc: 0.9877\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 3s 95ms/step - loss: 0.0676 - acc: 0.9791 - val_loss: 0.0346 - val_acc: 0.9881\n"
     ]
    }
   ],
   "source": [
    "with cur_strategy.scope():\n",
    "  optimizer = Adam(lr=0.02)\n",
    "  model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "  model.fit(x_train, y_train, epochs=10, batch_size=2048, steps_per_epoch=29, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKArZ2VwQlph"
   },
   "source": [
    "For `mnist`, we should get 99% validation accuracy, and for `fashion_mnist`, we should get around 86% of validation accuracy. Let's get a metric for high-level estimation of energy of this model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1591840404708,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "AlIk3gtFS6iJ",
    "outputId": "780a9c28-6234-49ff-9a85-e52bf00a5c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'act_0': {'energy': {'inputs': 5964.34,\n",
      "                      'op_cost': 0.0,\n",
      "                      'outputs': 5964.34,\n",
      "                      'parameters': 0.0},\n",
      "           'total': 5964.34},\n",
      " 'act_1': {'energy': {'inputs': 2982.17,\n",
      "                      'op_cost': 0.0,\n",
      "                      'outputs': 2982.17,\n",
      "                      'parameters': 0.0},\n",
      "           'total': 2982.17},\n",
      " 'act_2': {'energy': {'inputs': 1460.66,\n",
      "                      'op_cost': 0.0,\n",
      "                      'outputs': 1460.66,\n",
      "                      'parameters': 0.0},\n",
      "           'total': 1460.66},\n",
      " 'act_3': {'energy': {'inputs': 486.89,\n",
      "                      'op_cost': 0.0,\n",
      "                      'outputs': 486.89,\n",
      "                      'parameters': 0.0},\n",
      "           'total': 486.89},\n",
      " 'act_4': {'energy': {'inputs': 243.44,\n",
      "                      'op_cost': 0.0,\n",
      "                      'outputs': 243.44,\n",
      "                      'parameters': 0.0},\n",
      "           'total': 243.44},\n",
      " 'bn_0': {'energy': {'inputs': 5964.34,\n",
      "                     'op_cost': 23206.4,\n",
      "                     'outputs': 5964.34,\n",
      "                     'parameters': 121.72},\n",
      "          'total': 121.72},\n",
      " 'bn_1': {'energy': {'inputs': 2982.17,\n",
      "                     'op_cost': 11603.2,\n",
      "                     'outputs': 2982.17,\n",
      "                     'parameters': 243.44},\n",
      "          'total': 243.44},\n",
      " 'bn_2': {'energy': {'inputs': 1460.66,\n",
      "                     'op_cost': 5683.2,\n",
      "                     'outputs': 1460.66,\n",
      "                     'parameters': 365.16},\n",
      "          'total': 365.16},\n",
      " 'bn_3': {'energy': {'inputs': 486.89,\n",
      "                     'op_cost': 1894.4,\n",
      "                     'outputs': 486.89,\n",
      "                     'parameters': 486.89},\n",
      "          'total': 486.89},\n",
      " 'bn_4': {'energy': {'inputs': 243.44,\n",
      "                     'op_cost': 947.2,\n",
      "                     'outputs': 243.44,\n",
      "                     'parameters': 973.77},\n",
      "          'total': 973.77},\n",
      " 'conv2d_0': {'energy': {'inputs': 372.77,\n",
      "                         'op_cost': 129830.4,\n",
      "                         'outputs': 5964.34,\n",
      "                         'parameters': 273.87},\n",
      "              'total': 130477.04},\n",
      " 'conv2d_1': {'energy': {'inputs': 5964.34,\n",
      "                         'op_cost': 1038643.2,\n",
      "                         'outputs': 2982.17,\n",
      "                         'parameters': 8763.93},\n",
      "              'total': 1053371.47},\n",
      " 'conv2d_2': {'energy': {'inputs': 2982.17,\n",
      "                         'op_cost': 1017446.4,\n",
      "                         'outputs': 1460.66,\n",
      "                         'parameters': 26291.8},\n",
      "              'total': 1046720.37},\n",
      " 'conv2d_3': {'energy': {'inputs': 1460.66,\n",
      "                         'op_cost': 508723.2,\n",
      "                         'outputs': 486.89,\n",
      "                         'parameters': 52583.59},\n",
      "              'total': 562767.45},\n",
      " 'conv2d_4': {'energy': {'inputs': 486.89,\n",
      "                         'op_cost': 339148.8,\n",
      "                         'outputs': 243.44,\n",
      "                         'parameters': 140222.91},\n",
      "              'total': 479858.6},\n",
      " 'dense': {'energy': {'inputs': 243.44,\n",
      "                      'op_cost': 5888.0,\n",
      "                      'outputs': 19.02,\n",
      "                      'parameters': 2453.44},\n",
      "           'total': 8584.880000000001},\n",
      " 'flatten': {'energy': {'inputs': 243.44,\n",
      "                        'op_cost': 0.0,\n",
      "                        'outputs': 243.44,\n",
      "                        'parameters': 0.0},\n",
      "             'total': 243.44},\n",
      " 'softmax': {'energy': {'inputs': 19.02,\n",
      "                        'op_cost': 0.0,\n",
      "                        'outputs': 19.02,\n",
      "                        'parameters': 0.0},\n",
      "             'total': 19.02}}\n",
      "\n",
      "Total energy: 3.30 uJ\n"
     ]
    }
   ],
   "source": [
    "  reference_internal = \"fp32\"\n",
    "  reference_accumulator = \"fp32\"\n",
    "\n",
    "  q = run_qtools.QTools(\n",
    "      model,\n",
    "      # energy calculation using a given process\n",
    "      # \"horowitz\" refers to 45nm process published at\n",
    "      # M. Horowitz, \"1.1 Computing's energy problem (and what we can do about\n",
    "      # it), \"2014 IEEE International Solid-State Circuits Conference Digest of\n",
    "      # Technical Papers (ISSCC), San Francisco, CA, 2014, pp. 10-14, \n",
    "      # doi: 10.1109/ISSCC.2014.6757323.\n",
    "      process=\"horowitz\",\n",
    "      # quantizers for model input\n",
    "      source_quantizers=[quantized_bits(8, 0, 1)],\n",
    "      is_inference=False,\n",
    "      # absolute path (including filename) of the model weights\n",
    "      # in the future, we will attempt to optimize the power model\n",
    "      # by using weight information, although it can be used to further\n",
    "      # optimize QBatchNormalization.\n",
    "      weights_path=None,\n",
    "      # keras_quantizer to quantize weight/bias in un-quantized keras layers\n",
    "      keras_quantizer=reference_internal,\n",
    "      # keras_quantizer to quantize MAC in un-quantized keras layers\n",
    "      keras_accumulator=reference_accumulator,\n",
    "      # whether calculate baseline energy\n",
    "      for_reference=True)\n",
    "  \n",
    "# caculate energy of the derived data type map.\n",
    "energy_dict = q.pe(\n",
    "    # whether to store parameters in dram, sram, or fixed\n",
    "    weights_on_memory=\"sram\",\n",
    "    # store activations in dram or sram\n",
    "    activations_on_memory=\"sram\",\n",
    "    # minimum sram size in number of bits. Let's assume a 16MB SRAM.\n",
    "    min_sram_size=8*16*1024*1024,\n",
    "    # whether load data from dram to sram (consider sram as a cache\n",
    "    # for dram. If false, we will assume data will be already in SRAM\n",
    "    rd_wr_on_io=False)\n",
    "\n",
    "# get stats of energy distribution in each layer\n",
    "energy_profile = q.extract_energy_profile(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "# extract sum of energy of each layer according to the rule specified in\n",
    "# qtools_settings.cfg.include_energy\n",
    "total_energy = q.extract_energy_sum(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "\n",
    "pprint.pprint(energy_profile)\n",
    "print()\n",
    "print(\"Total energy: {:.2f} uJ\".format(total_energy / 1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eDXxDSUVJ2m"
   },
   "source": [
    "During the computation, we had a dictionary that outlines the energy per layer (`energy_profile`), and total energy (`total_energy`). The reader should remember that `energy_profile` may need additional filtering as implementations will fuse some\n",
    "layers. When we compute the `total_energy`, we consider an approximation that some layers will be fused to compute the final energy number. For example, a convolution layer followed by an activation layer will be fused into a single layer so that the output of the convolution layer is not used.\n",
    "\n",
    "You have to remember that our high-level model for energy has several assumptions:\n",
    "\n",
    "The energy of a layer is estimated as `energy(layer) = energy(input) + energy(parameters) + energy(MAC) + energy(output)`.\n",
    "\n",
    "1) Reading inputs, parameters and outputs consider only _compulsory_ accesses, i.e. first access to the data, which is independent of the hardware architecture. If you remember _The 3 C's of Caches_ (https://courses.cs.washington.edu/courses/cse410/99au/lectures/Lecture-10-18/tsld035.htm) other types of accesses will depend on the accelerator architecture.\n",
    "\n",
    "2) For the multiply-and-add (MAC) energy estimation, we only consider the energy to compute the MAC, but not any other type energy. For example, in a real accelerator, you have registers, glue logic, pipeline logic that will affect the overall energy profile of the device.\n",
    "\n",
    "Although this model is simple and provides an initial estimate on what to expect, it has high-variance with respect to actual energy numbers you will find in practice, especially with respect to different architectural implementations.\n",
    "\n",
    "We assume that the real energy `Energy(layer)` is a linear combination of the high-level energy model, i.e.`Energy(layer) = k1 * energy(layer) + k2`, where `k1` and `k2` are constants that depend on the architecture of the accelerator. One can think of `k1` as the factor that accounts for the additional storage to keep the model running, and `k2` as the additional always on logic that is required to perform the operations. If we compare the energy of two implementations with different quantizations of the same layer, let's say `layer1` and `layer2`, `Energy(layer1) > Energy(layer2)` holds true iff `energy(layer1) > energy(layer2)` for the same architecture, but for different architectures, this will not be true in general.\n",
    "\n",
    "Despite its limitations to predict a single energy number, this model is quite good to compare the energy of two different models, or different types of quantizations, when we restrict it to a single architecture, and that's how we use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hr1FL8wVSy-q"
   },
   "source": [
    "# Quantizing a Model With `AutoQKeras`\n",
    "\n",
    "To quantize this model with `AutoQKeras`, we need to define the quantization for kernels, biases and activations; forgiving factors and quantization strategy.\n",
    "\n",
    "Below we define which quantizers are allowed for kernel, bias, activations and linear. Linear is a proxy that we use to capture `Activation(\"linear\")` to apply quantization without applying a non-linear operation.  In some networks, we found that this trick may be necessary to better represent the quantization space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSsEwDr_yRG4"
   },
   "outputs": [],
   "source": [
    "quantization_config = {\n",
    "        \"kernel\": {\n",
    "                \"binary\": 1,\n",
    "                \"stochastic_binary\": 1,\n",
    "                \"ternary\": 2,\n",
    "                \"stochastic_ternary\": 2,\n",
    "                \"quantized_bits(2,1,1,alpha=1.0)\": 2,\n",
    "                \"quantized_bits(4,0,1,alpha=1.0)\": 4,\n",
    "                \"quantized_bits(8,0,1,alpha=1.0)\": 8,\n",
    "                \"quantized_po2(4,1)\": 4\n",
    "        },\n",
    "        \"bias\": {\n",
    "                \"quantized_bits(4,0,1)\": 4,\n",
    "                \"quantized_bits(8,3,1)\": 8,\n",
    "                \"quantized_po2(4,8)\": 4\n",
    "        },\n",
    "        \"activation\": {\n",
    "                \"binary\": 1,\n",
    "                \"ternary\": 2,\n",
    "                \"quantized_relu_po2(4,4)\": 4,\n",
    "                \"quantized_relu(3,1)\": 3,\n",
    "                \"quantized_relu(4,2)\": 4,\n",
    "                \"quantized_relu(8,2)\": 8,\n",
    "                \"quantized_relu(8,4)\": 8,\n",
    "                \"quantized_relu(16,8)\": 16\n",
    "        },\n",
    "        \"linear\": {\n",
    "                \"binary\": 1,\n",
    "                \"ternary\": 2,\n",
    "                \"quantized_bits(4,1)\": 4,\n",
    "                \"quantized_bits(8,2)\": 8,\n",
    "                \"quantized_bits(16,10)\": 16\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmW_xaAvZo4D"
   },
   "source": [
    "Now let's define how to apply quantization. In the simplest form, we specify how many bits for kernels, biases and activations by layer types. Note that the entry `BatchNormalization` needs to be specified here, as we only quantize layer types specified by these patterns.  For example, a `Flatten` layer is not quantized as it does not change the data type of its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emTRLIZmR-P7"
   },
   "outputs": [],
   "source": [
    "limit = {\n",
    "    \"Dense\": [8, 8, 4],\n",
    "    \"Conv2D\": [4, 8, 4],\n",
    "    \"DepthwiseConv2D\": [4, 8, 4],\n",
    "    \"Activation\": [4],\n",
    "    \"BatchNormalization\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iu5gFNhaLNE"
   },
   "source": [
    "Here, we are specifying that we want to use at most 4 bits for weights and activations, and at most 8 bits for biases in convolutional and depthwise convolutions, but we allow up to 8 bits for kernels in dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUMQGEIDblSa"
   },
   "source": [
    "Let's define now the forgiving factor. We will consider energy minimization as a goal as follows.  Here, we are saying that we allow 8% reduction in accuracy for a 2x reduction in energy, both reference and trials have parameters and activations on SRAM, both reference model and quantization trials do not read/write from DRAM on I/O operations, and we should consider both experiments to use SRAMs with minimum tensor sizes (commonly called distributed SRAM implementation).\n",
    "\n",
    "We also need to specify the quantizers for the inputs. In this case, we want to use `int8` as source quantizers. Other possible types are `int16`, `int32`, `fp16` or `fp32`, besides `QKeras` quantizer types.\n",
    "\n",
    "Finally, to be fair, we want to compare our quantization against fixed-point 8-bit inputs, outputs, activations, weights and biases, and 32-bit accumulators.\n",
    "\n",
    "Remember that a `forgiving factor` forgives a drop in a metric such as `accuracy` if the gains of the model are much bigger than the drop. For example, it corresponds to the sentence *we allow $\\tt{delta}\\%$ reduction in accuracy if the quantized model has $\\tt{rate} \\times$ smaller energy than the original model*, being a multiplicative factor to the metric. It is computed by $1 + \\tt{delta} \\times  \\log_{\\tt{rate}}(\\tt{stress} \\times \\tt{reference\\_cost} / \\tt{trial\\_cost})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kS31TuZ-aKb1"
   },
   "outputs": [],
   "source": [
    "goal = {\n",
    "    \"type\": \"energy\",\n",
    "    \"params\": {\n",
    "        \"delta_p\": 8.0,\n",
    "        \"delta_n\": 8.0,\n",
    "        \"rate\": 2.0,\n",
    "        \"stress\": 1.0,\n",
    "        \"process\": \"horowitz\",\n",
    "        \"parameters_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"activations_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"rd_wr_on_io\": [False, False],\n",
    "        \"min_sram_size\": [0, 0],\n",
    "        \"source_quantizers\": [\"int8\"],\n",
    "        \"reference_internal\": \"int8\",\n",
    "        \"reference_accumulator\": \"int32\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QzyWPA-dCxm"
   },
   "source": [
    "There are a few more things we need to define. Let's bundle them on a dictionary and pass them to `AutoQKeras`.  We will try a maximum of 10 trials (`max_trials`) just to limit the time we will spend finding the best quantization here.  Please note that this parameter is not valid if you are running in `hyperband` mode.\n",
    "\n",
    "`output_dir` is the directory where we will store our results. Since we are running on a colab, we will let `tempfile` chooce a directory for us.\n",
    "\n",
    "`learning_rate_optimizer` allows `AutoQKeras` to change the optimization function and the `learning_rate` to try to improve the quantization results. Since it is still experimental, it may be the case that in some cases it will get worse results. \n",
    "\n",
    "Because we are tuning filters as well, we should set `transfer_weights` to `False` as the trainable parameters will have different shapes.\n",
    "\n",
    "In `AutoQKeras` we have three modes of operation: `random`, `bayesian` and `hyperband`. I recommend the user to refer to `KerasTuner` (https://keras-team.github.io/keras-tuner/) for a complete description of them.\n",
    "\n",
    "`tune_filters` can be set to `layer`, `block` or `none`. If `tune_filters` is `block`, we change the filters by the same amount for all layers being quantized in the trial. If `tune_filters` is `layer`, we will possibly change the number of filters for each layer independently. Finally, if `tune_filters` is `none`, we will not perform filter tuning.\n",
    "\n",
    "Together with `tune_filters`, `tune_filter_exceptions` allows the user to specify by a regular expression which filters we should not perform filter tuning, which is especially good for the last layers of the network.\n",
    "\n",
    "Filter tuning is a very important feature of `AutoQKeras`. When we deep quantize a model, we may need less or more filters for each layer (and you can guess we do not know a priori how many filters we will need for each layer). Let me give you a rationale behind this.\n",
    "\n",
    "- **less filters**: let us assume we have two set of filter coefficients we want quantize: $[-0.3, 0.2, 0.5, 0.15]$ and $[-0.5, 0.4, 0.1, 0.65]$. If we apply a $\\tt{binary}$ quantizer with $\\tt{scale} = \\big\\lceil \\log_2(\\frac{\\sum |w|}{N}) \\big\\rceil$, where $w$ are the filter coefficients and $N$ is the number of coefficients, we will end up with the same filter $\\tt{binary}([-0.3, 0.2, 0.5, 0.15]) = \\tt{binary}([-0.5, 0.4, 0.1, 0.65]) = [-1,1,1,1] \\times 0.5$. In this case we are assuming the $\\tt{scale}$ is a power-of-2 number so that it can be efficiently implemented by a shift operation;\n",
    "\n",
    "- **more filters**: it is clear that quantization will drop information (just look at the example above) and deep quantization will drop more information, so to recover some of the boundary regions in layers that perform feature extraction, we may need to add more filters to the layer when we quantize it.\n",
    "\n",
    "We do not want to quantize the `softmax` layer, which is the last layer of the network. In `AutoQKeras`, you can specify the indexes that you want to perform quantization by specifying the corresponding index of the layer in `Keras`, i.e. if you can get the layer as `model.layers[i]` in `Keras`, `i` is the index of the layer.\n",
    "\n",
    "Finally, for data parallel distributed training, we should pass the strategy in `distribution_strategy` to `KerasTuner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1591840405963,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "2-fyACb2dIAN",
    "outputId": "a180fa3f-8cc3-4f70-ce70-c05c28f88d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantizing layers: ['conv2d_0', 'bn_0', 'act_0', 'drop_0', 'conv2d_1', 'bn_1', 'act_1', 'drop_1', 'conv2d_2', 'bn_2', 'act_2', 'drop_2', 'conv2d_3', 'bn_3', 'act_3', 'drop_3', 'conv2d_4', 'bn_4', 'act_4', 'drop_4', 'flatten', 'dense']\n"
     ]
    }
   ],
   "source": [
    "run_config = {\n",
    "  \"output_dir\": tempfile.mkdtemp(),\n",
    "  \"goal\": goal,\n",
    "  \"quantization_config\": quantization_config,\n",
    "  \"learning_rate_optimizer\": False,\n",
    "  \"transfer_weights\": False,\n",
    "  \"mode\": \"random\",\n",
    "  \"seed\": 42,\n",
    "  \"limit\": limit,\n",
    "  \"tune_filters\": \"layer\",\n",
    "  \"tune_filters_exceptions\": \"^dense\",\n",
    "  \"distribution_strategy\": cur_strategy,\n",
    "  # first layer is input, layer two layers are softmax and flatten\n",
    "  \"layer_indexes\": range(1, len(model.layers) - 1),\n",
    "  \"max_trials\": 20\n",
    "}\n",
    "\n",
    "print(\"quantizing layers:\", [model.layers[i].name for i in run_config[\"layer_indexes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 471192,
     "status": "ok",
     "timestamp": 1591840877167,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "MxlZFpa3fBv2",
    "outputId": "4d339846-1832-4a79-89b3-c9c4944dd47a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 01m 43s]\n",
      "val_score: 1.355238437652588\n",
      "\n",
      "Best val_score So Far: 1.3851507902145386\n",
      "Total elapsed time: 00h 45m 09s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "autoqk = AutoQKeras(model, metrics=[\"acc\"], custom_objects=custom_objects, **run_config)\n",
    "autoqk.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=1024, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LW_qN8-lOwL0"
   },
   "source": [
    "Now, let's see which model is the best model we got.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3961,
     "status": "ok",
     "timestamp": 1591840881173,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "1L7KivAoffaL",
    "outputId": "f44b07a3-027d-4d69-9864-b3670815c407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.019999999552965164\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_0 (QConv2D)          (None, 14, 14, 24)        216       \n",
      "                                                                 \n",
      " bn_0 (QBatchNormalization)  (None, 14, 14, 24)        96        \n",
      "                                                                 \n",
      " act_0 (QActivation)         (None, 14, 14, 24)        0         \n",
      "                                                                 \n",
      " drop_0 (Dropout)            (None, 14, 14, 24)        0         \n",
      "                                                                 \n",
      " conv2d_1 (QConv2D)          (None, 7, 7, 16)          3456      \n",
      "                                                                 \n",
      " bn_1 (QBatchNormalization)  (None, 7, 7, 16)          64        \n",
      "                                                                 \n",
      " act_1 (QActivation)         (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " drop_1 (Dropout)            (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " conv2d_2 (QConv2D)          (None, 4, 4, 24)          3456      \n",
      "                                                                 \n",
      " bn_2 (QBatchNormalization)  (None, 4, 4, 24)          96        \n",
      "                                                                 \n",
      " act_2 (QActivation)         (None, 4, 4, 24)          0         \n",
      "                                                                 \n",
      " drop_2 (Dropout)            (None, 4, 4, 24)          0         \n",
      "                                                                 \n",
      " conv2d_3 (QConv2D)          (None, 2, 2, 64)          13824     \n",
      "                                                                 \n",
      " bn_3 (QBatchNormalization)  (None, 2, 2, 64)          256       \n",
      "                                                                 \n",
      " act_3 (QActivation)         (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " drop_3 (Dropout)            (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (QConv2D)          (None, 1, 1, 128)         73728     \n",
      "                                                                 \n",
      " bn_4 (QBatchNormalization)  (None, 1, 1, 128)         512       \n",
      "                                                                 \n",
      " act_4 (QActivation)         (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " drop_4 (Dropout)            (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (QDense)              (None, 10)                1290      \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,994\n",
      "Trainable params: 96,482\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=14738 reference_size=574175\n",
      "       delta=42.27%\n",
      "Total Cost Reduction:\n",
      "       14738 vs 574175 (-97.43%)\n",
      "conv2d_0             f=24 stochastic_binary(alpha='auto_po2') \n",
      "bn_0                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_0                binary\n",
      "conv2d_1             f=16 ternary(alpha='auto_po2') \n",
      "bn_1                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_1                quantized_relu(3,1)\n",
      "conv2d_2             f=24 stochastic_ternary(alpha='auto_po2') \n",
      "bn_2                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_2                binary\n",
      "conv2d_3             f=64 quantized_bits(2,1,1,alpha=1.0) \n",
      "bn_3                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_3                binary\n",
      "conv2d_4             f=128 stochastic_ternary(alpha='auto_po2') \n",
      "bn_4                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_4                quantized_relu(4,2)\n",
      "dense                u=10 quantized_po2(4,1) quantized_bits(8,3,1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel = autoqk.get_best_model()\n",
    "qmodel.save_weights(\"qmodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RB2xBRhJiwoh"
   },
   "source": [
    "We got here >90% reduction in energy when compared to 8-bit tensors and 32-bit accumulators. Remember that our original number was 3.3 uJ for fp32.  The end model has 11 nJ for the quantized model as opposed to 204 nJ for the 8-bit original quantized model. As these energy numbers are from high-level energy models, you should remember to consider the relations between them, and not the actual numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy0zcqvQoBnb"
   },
   "source": [
    "Let's train this model to see how much accuracy we can get of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 71353,
     "status": "ok",
     "timestamp": 1591840952535,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "_ipZSEfgoGdb",
    "outputId": "b184269d-1161-417a-e1ae-e852dc451561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "15/15 [==============================] - 14s 486ms/step - loss: 1.2808 - acc: 0.5802 - val_loss: 1.3874 - val_acc: 0.7358\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 5s 342ms/step - loss: 0.5959 - acc: 0.8041 - val_loss: 0.8395 - val_acc: 0.8385\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 5s 357ms/step - loss: 0.4537 - acc: 0.8545 - val_loss: 0.4389 - val_acc: 0.8949\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 5s 317ms/step - loss: 0.3832 - acc: 0.8787 - val_loss: 0.3548 - val_acc: 0.9037\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 5s 326ms/step - loss: 0.3353 - acc: 0.8931 - val_loss: 0.2432 - val_acc: 0.9340\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 5s 332ms/step - loss: 0.3011 - acc: 0.9056 - val_loss: 0.1975 - val_acc: 0.9420\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 5s 314ms/step - loss: 0.2697 - acc: 0.9151 - val_loss: 0.1561 - val_acc: 0.9536\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 5s 336ms/step - loss: 0.2516 - acc: 0.9216 - val_loss: 0.1840 - val_acc: 0.9474\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 5s 308ms/step - loss: 0.2386 - acc: 0.9261 - val_loss: 0.1338 - val_acc: 0.9585\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 5s 333ms/step - loss: 0.2260 - acc: 0.9302 - val_loss: 0.1543 - val_acc: 0.9522\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 5s 309ms/step - loss: 0.2183 - acc: 0.9305 - val_loss: 0.1215 - val_acc: 0.9643\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 5s 328ms/step - loss: 0.2136 - acc: 0.9328 - val_loss: 0.1268 - val_acc: 0.9602\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 0.2040 - acc: 0.9363 - val_loss: 0.1224 - val_acc: 0.9631\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 4s 275ms/step - loss: 0.1983 - acc: 0.9369 - val_loss: 0.1163 - val_acc: 0.9656\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 5s 318ms/step - loss: 0.1921 - acc: 0.9403 - val_loss: 0.1086 - val_acc: 0.9666\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 5s 340ms/step - loss: 0.1904 - acc: 0.9407 - val_loss: 0.1103 - val_acc: 0.9672\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 4s 296ms/step - loss: 0.1833 - acc: 0.9417 - val_loss: 0.1052 - val_acc: 0.9669\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 5s 350ms/step - loss: 0.1821 - acc: 0.9430 - val_loss: 0.0992 - val_acc: 0.9696\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 5s 312ms/step - loss: 0.1859 - acc: 0.9423 - val_loss: 0.0946 - val_acc: 0.9705\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 5s 328ms/step - loss: 0.1787 - acc: 0.9444 - val_loss: 0.0898 - val_acc: 0.9715\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 5s 365ms/step - loss: 0.1768 - acc: 0.9446 - val_loss: 0.1012 - val_acc: 0.9691\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 5s 342ms/step - loss: 0.1726 - acc: 0.9457 - val_loss: 0.1071 - val_acc: 0.9675\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 5s 361ms/step - loss: 0.1726 - acc: 0.9469 - val_loss: 0.0910 - val_acc: 0.9705\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 5s 359ms/step - loss: 0.1706 - acc: 0.9477 - val_loss: 0.0868 - val_acc: 0.9714\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 5s 347ms/step - loss: 0.1663 - acc: 0.9486 - val_loss: 0.0899 - val_acc: 0.9721\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 5s 325ms/step - loss: 0.1686 - acc: 0.9478 - val_loss: 0.1007 - val_acc: 0.9695\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 5s 337ms/step - loss: 0.1678 - acc: 0.9481 - val_loss: 0.0840 - val_acc: 0.9725\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 5s 340ms/step - loss: 0.1668 - acc: 0.9483 - val_loss: 0.0803 - val_acc: 0.9743\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 5s 352ms/step - loss: 0.1590 - acc: 0.9493 - val_loss: 0.0895 - val_acc: 0.9730\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 5s 356ms/step - loss: 0.1630 - acc: 0.9494 - val_loss: 0.0852 - val_acc: 0.9724\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 5s 316ms/step - loss: 0.1667 - acc: 0.9483 - val_loss: 0.0988 - val_acc: 0.9698\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 5s 313ms/step - loss: 0.1624 - acc: 0.9494 - val_loss: 0.0805 - val_acc: 0.9739\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 5s 325ms/step - loss: 0.1615 - acc: 0.9499 - val_loss: 0.0809 - val_acc: 0.9752\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 5s 347ms/step - loss: 0.1599 - acc: 0.9509 - val_loss: 0.0839 - val_acc: 0.9745\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 5s 313ms/step - loss: 0.1616 - acc: 0.9495 - val_loss: 0.0805 - val_acc: 0.9746\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 5s 309ms/step - loss: 0.1612 - acc: 0.9502 - val_loss: 0.0717 - val_acc: 0.9772\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 5s 343ms/step - loss: 0.1559 - acc: 0.9518 - val_loss: 0.0830 - val_acc: 0.9750\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 5s 342ms/step - loss: 0.1506 - acc: 0.9531 - val_loss: 0.0896 - val_acc: 0.9738\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 5s 314ms/step - loss: 0.1575 - acc: 0.9509 - val_loss: 0.0755 - val_acc: 0.9764\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 5s 351ms/step - loss: 0.1543 - acc: 0.9513 - val_loss: 0.0799 - val_acc: 0.9740\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 5s 354ms/step - loss: 0.1560 - acc: 0.9518 - val_loss: 0.0811 - val_acc: 0.9752\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 5s 316ms/step - loss: 0.1565 - acc: 0.9513 - val_loss: 0.0890 - val_acc: 0.9733\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 5s 330ms/step - loss: 0.1523 - acc: 0.9530 - val_loss: 0.0832 - val_acc: 0.9738\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 5s 309ms/step - loss: 0.1535 - acc: 0.9520 - val_loss: 0.0918 - val_acc: 0.9717\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 5s 315ms/step - loss: 0.1520 - acc: 0.9531 - val_loss: 0.0806 - val_acc: 0.9751\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 5s 318ms/step - loss: 0.1535 - acc: 0.9524 - val_loss: 0.0811 - val_acc: 0.9754\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 5s 320ms/step - loss: 0.1531 - acc: 0.9518 - val_loss: 0.0804 - val_acc: 0.9741\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 5s 310ms/step - loss: 0.1510 - acc: 0.9522 - val_loss: 0.0826 - val_acc: 0.9744\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 5s 332ms/step - loss: 0.1537 - acc: 0.9520 - val_loss: 0.0868 - val_acc: 0.9742\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 5s 312ms/step - loss: 0.1512 - acc: 0.9531 - val_loss: 0.0781 - val_acc: 0.9760\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 5s 327ms/step - loss: 0.1514 - acc: 0.9520 - val_loss: 0.0758 - val_acc: 0.9759\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 4s 307ms/step - loss: 0.1503 - acc: 0.9537 - val_loss: 0.0749 - val_acc: 0.9767\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 4s 282ms/step - loss: 0.1503 - acc: 0.9528 - val_loss: 0.0804 - val_acc: 0.9749\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 5s 339ms/step - loss: 0.1520 - acc: 0.9520 - val_loss: 0.0753 - val_acc: 0.9770\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 5s 335ms/step - loss: 0.1487 - acc: 0.9540 - val_loss: 0.0852 - val_acc: 0.9729\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 5s 342ms/step - loss: 0.1498 - acc: 0.9538 - val_loss: 0.0780 - val_acc: 0.9754\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 5s 309ms/step - loss: 0.1499 - acc: 0.9531 - val_loss: 0.0761 - val_acc: 0.9746\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 5s 337ms/step - loss: 0.1529 - acc: 0.9532 - val_loss: 0.0796 - val_acc: 0.9756\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 5s 311ms/step - loss: 0.1470 - acc: 0.9541 - val_loss: 0.0708 - val_acc: 0.9761\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 5s 326ms/step - loss: 0.1514 - acc: 0.9531 - val_loss: 0.0794 - val_acc: 0.9743\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 5s 339ms/step - loss: 0.1543 - acc: 0.9530 - val_loss: 0.0763 - val_acc: 0.9750\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 5s 342ms/step - loss: 0.1525 - acc: 0.9529 - val_loss: 0.0753 - val_acc: 0.9750\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 5s 346ms/step - loss: 0.1470 - acc: 0.9539 - val_loss: 0.0703 - val_acc: 0.9773\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 5s 335ms/step - loss: 0.1472 - acc: 0.9529 - val_loss: 0.0845 - val_acc: 0.9735\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 5s 314ms/step - loss: 0.1462 - acc: 0.9541 - val_loss: 0.0738 - val_acc: 0.9783\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 4s 280ms/step - loss: 0.1430 - acc: 0.9562 - val_loss: 0.0691 - val_acc: 0.9789\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 5s 317ms/step - loss: 0.1485 - acc: 0.9533 - val_loss: 0.0751 - val_acc: 0.9750\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 4s 272ms/step - loss: 0.1482 - acc: 0.9543 - val_loss: 0.0801 - val_acc: 0.9745\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 5s 340ms/step - loss: 0.1470 - acc: 0.9545 - val_loss: 0.0769 - val_acc: 0.9755\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 5s 316ms/step - loss: 0.1464 - acc: 0.9541 - val_loss: 0.0868 - val_acc: 0.9750\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 5s 332ms/step - loss: 0.1498 - acc: 0.9540 - val_loss: 0.0769 - val_acc: 0.9756\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 5s 335ms/step - loss: 0.1463 - acc: 0.9538 - val_loss: 0.0728 - val_acc: 0.9777\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 5s 344ms/step - loss: 0.1431 - acc: 0.9554 - val_loss: 0.0721 - val_acc: 0.9774\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 5s 347ms/step - loss: 0.1463 - acc: 0.9546 - val_loss: 0.0756 - val_acc: 0.9767\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 5s 328ms/step - loss: 0.1449 - acc: 0.9545 - val_loss: 0.0791 - val_acc: 0.9738\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 5s 319ms/step - loss: 0.1494 - acc: 0.9531 - val_loss: 0.0750 - val_acc: 0.9749\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 5s 324ms/step - loss: 0.1530 - acc: 0.9530 - val_loss: 0.0796 - val_acc: 0.9746\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 5s 312ms/step - loss: 0.1473 - acc: 0.9532 - val_loss: 0.0823 - val_acc: 0.9741\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 5s 319ms/step - loss: 0.1481 - acc: 0.9549 - val_loss: 0.0686 - val_acc: 0.9790\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 5s 311ms/step - loss: 0.1431 - acc: 0.9550 - val_loss: 0.0750 - val_acc: 0.9762\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 5s 341ms/step - loss: 0.1448 - acc: 0.9556 - val_loss: 0.0832 - val_acc: 0.9750\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 5s 319ms/step - loss: 0.1502 - acc: 0.9542 - val_loss: 0.0848 - val_acc: 0.9744\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 5s 321ms/step - loss: 0.1418 - acc: 0.9560 - val_loss: 0.0759 - val_acc: 0.9764\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 5s 321ms/step - loss: 0.1455 - acc: 0.9543 - val_loss: 0.0708 - val_acc: 0.9778\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 5s 306ms/step - loss: 0.1458 - acc: 0.9552 - val_loss: 0.0718 - val_acc: 0.9774\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 5s 342ms/step - loss: 0.1491 - acc: 0.9539 - val_loss: 0.0788 - val_acc: 0.9759\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 5s 310ms/step - loss: 0.1464 - acc: 0.9547 - val_loss: 0.0755 - val_acc: 0.9749\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 5s 305ms/step - loss: 0.1492 - acc: 0.9535 - val_loss: 0.0732 - val_acc: 0.9764\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 5s 334ms/step - loss: 0.1460 - acc: 0.9547 - val_loss: 0.0698 - val_acc: 0.9761\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 5s 348ms/step - loss: 0.1445 - acc: 0.9549 - val_loss: 0.0690 - val_acc: 0.9785\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 5s 329ms/step - loss: 0.1469 - acc: 0.9543 - val_loss: 0.0703 - val_acc: 0.9774\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 5s 317ms/step - loss: 0.1466 - acc: 0.9550 - val_loss: 0.0772 - val_acc: 0.9762\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 5s 292ms/step - loss: 0.1436 - acc: 0.9552 - val_loss: 0.0678 - val_acc: 0.9780\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 5s 324ms/step - loss: 0.1433 - acc: 0.9563 - val_loss: 0.0712 - val_acc: 0.9781\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 4s 300ms/step - loss: 0.1399 - acc: 0.9568 - val_loss: 0.0704 - val_acc: 0.9790\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 5s 327ms/step - loss: 0.1450 - acc: 0.9555 - val_loss: 0.0783 - val_acc: 0.9752\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 5s 314ms/step - loss: 0.1487 - acc: 0.9528 - val_loss: 0.0681 - val_acc: 0.9786\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 5s 350ms/step - loss: 0.1389 - acc: 0.9559 - val_loss: 0.0682 - val_acc: 0.9778\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 5s 335ms/step - loss: 0.1442 - acc: 0.9548 - val_loss: 0.0797 - val_acc: 0.9751\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 5s 338ms/step - loss: 0.1464 - acc: 0.9538 - val_loss: 0.0727 - val_acc: 0.9767\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 5s 347ms/step - loss: 0.1453 - acc: 0.9550 - val_loss: 0.0688 - val_acc: 0.9778\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 5s 314ms/step - loss: 0.1415 - acc: 0.9561 - val_loss: 0.0722 - val_acc: 0.9773\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 5s 312ms/step - loss: 0.1426 - acc: 0.9561 - val_loss: 0.0814 - val_acc: 0.9730\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 5s 306ms/step - loss: 0.1449 - acc: 0.9546 - val_loss: 0.0739 - val_acc: 0.9765\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 5s 343ms/step - loss: 0.1401 - acc: 0.9564 - val_loss: 0.0651 - val_acc: 0.9781\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 5s 350ms/step - loss: 0.1520 - acc: 0.9532 - val_loss: 0.0715 - val_acc: 0.9775\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 5s 319ms/step - loss: 0.1448 - acc: 0.9545 - val_loss: 0.0661 - val_acc: 0.9780\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 5s 325ms/step - loss: 0.1450 - acc: 0.9549 - val_loss: 0.0784 - val_acc: 0.9751\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 5s 322ms/step - loss: 0.1467 - acc: 0.9545 - val_loss: 0.0734 - val_acc: 0.9782\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 5s 293ms/step - loss: 0.1381 - acc: 0.9575 - val_loss: 0.0774 - val_acc: 0.9768\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 5s 330ms/step - loss: 0.1419 - acc: 0.9557 - val_loss: 0.0793 - val_acc: 0.9749\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 5s 342ms/step - loss: 0.1418 - acc: 0.9557 - val_loss: 0.0764 - val_acc: 0.9764\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 5s 318ms/step - loss: 0.1424 - acc: 0.9550 - val_loss: 0.0658 - val_acc: 0.9795\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 5s 329ms/step - loss: 0.1460 - acc: 0.9552 - val_loss: 0.0767 - val_acc: 0.9758\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - 5s 342ms/step - loss: 0.1467 - acc: 0.9545 - val_loss: 0.0707 - val_acc: 0.9776\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 5s 339ms/step - loss: 0.1418 - acc: 0.9561 - val_loss: 0.0768 - val_acc: 0.9749\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 5s 311ms/step - loss: 0.1428 - acc: 0.9549 - val_loss: 0.0712 - val_acc: 0.9788\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 5s 319ms/step - loss: 0.1431 - acc: 0.9559 - val_loss: 0.0734 - val_acc: 0.9773\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 5s 325ms/step - loss: 0.1428 - acc: 0.9553 - val_loss: 0.0788 - val_acc: 0.9749\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 5s 317ms/step - loss: 0.1412 - acc: 0.9553 - val_loss: 0.0787 - val_acc: 0.9764\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 5s 333ms/step - loss: 0.1420 - acc: 0.9554 - val_loss: 0.0840 - val_acc: 0.9750\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 5s 329ms/step - loss: 0.1413 - acc: 0.9548 - val_loss: 0.0690 - val_acc: 0.9794\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 5s 343ms/step - loss: 0.1405 - acc: 0.9563 - val_loss: 0.0736 - val_acc: 0.9768\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 5s 335ms/step - loss: 0.1448 - acc: 0.9546 - val_loss: 0.0739 - val_acc: 0.9758\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 5s 333ms/step - loss: 0.1442 - acc: 0.9557 - val_loss: 0.0708 - val_acc: 0.9779\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 5s 352ms/step - loss: 0.1438 - acc: 0.9548 - val_loss: 0.0768 - val_acc: 0.9762\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 5s 351ms/step - loss: 0.1403 - acc: 0.9564 - val_loss: 0.0729 - val_acc: 0.9783\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 5s 350ms/step - loss: 0.1431 - acc: 0.9553 - val_loss: 0.0618 - val_acc: 0.9805\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 9s 581ms/step - loss: 0.1465 - acc: 0.9540 - val_loss: 0.0705 - val_acc: 0.9777\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 5s 331ms/step - loss: 0.1465 - acc: 0.9547 - val_loss: 0.0657 - val_acc: 0.9778\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 5s 322ms/step - loss: 0.1446 - acc: 0.9553 - val_loss: 0.0676 - val_acc: 0.9798\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 5s 336ms/step - loss: 0.1410 - acc: 0.9565 - val_loss: 0.0747 - val_acc: 0.9775\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 5s 332ms/step - loss: 0.1380 - acc: 0.9574 - val_loss: 0.0764 - val_acc: 0.9771\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 5s 340ms/step - loss: 0.1437 - acc: 0.9552 - val_loss: 0.0735 - val_acc: 0.9766\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 5s 347ms/step - loss: 0.1448 - acc: 0.9553 - val_loss: 0.0706 - val_acc: 0.9786\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 5s 362ms/step - loss: 0.1398 - acc: 0.9561 - val_loss: 0.0781 - val_acc: 0.9761\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 4s 301ms/step - loss: 0.1439 - acc: 0.9555 - val_loss: 0.0780 - val_acc: 0.9750\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 5s 323ms/step - loss: 0.1404 - acc: 0.9565 - val_loss: 0.0698 - val_acc: 0.9778\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 6s 365ms/step - loss: 0.1400 - acc: 0.9560 - val_loss: 0.0721 - val_acc: 0.9776\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 5s 338ms/step - loss: 0.1419 - acc: 0.9556 - val_loss: 0.0793 - val_acc: 0.9750\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 5s 348ms/step - loss: 0.1442 - acc: 0.9554 - val_loss: 0.0793 - val_acc: 0.9750\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 5s 346ms/step - loss: 0.1455 - acc: 0.9543 - val_loss: 0.0725 - val_acc: 0.9770\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 5s 354ms/step - loss: 0.1349 - acc: 0.9570 - val_loss: 0.0673 - val_acc: 0.9772\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 5s 313ms/step - loss: 0.1404 - acc: 0.9561 - val_loss: 0.0710 - val_acc: 0.9779\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 5s 354ms/step - loss: 0.1399 - acc: 0.9568 - val_loss: 0.0684 - val_acc: 0.9777\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 5s 353ms/step - loss: 0.1379 - acc: 0.9577 - val_loss: 0.0601 - val_acc: 0.9815\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 5s 371ms/step - loss: 0.1454 - acc: 0.9551 - val_loss: 0.0740 - val_acc: 0.9758\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 5s 354ms/step - loss: 0.1449 - acc: 0.9558 - val_loss: 0.0678 - val_acc: 0.9785\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 5s 350ms/step - loss: 0.1398 - acc: 0.9572 - val_loss: 0.0712 - val_acc: 0.9770\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 5s 360ms/step - loss: 0.1402 - acc: 0.9563 - val_loss: 0.0681 - val_acc: 0.9764\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 5s 328ms/step - loss: 0.1384 - acc: 0.9571 - val_loss: 0.0751 - val_acc: 0.9775\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 5s 346ms/step - loss: 0.1407 - acc: 0.9560 - val_loss: 0.0749 - val_acc: 0.9759\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 5s 354ms/step - loss: 0.1371 - acc: 0.9573 - val_loss: 0.0651 - val_acc: 0.9802\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 5s 374ms/step - loss: 0.1415 - acc: 0.9551 - val_loss: 0.0811 - val_acc: 0.9749\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 5s 353ms/step - loss: 0.1397 - acc: 0.9566 - val_loss: 0.0714 - val_acc: 0.9770\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 5s 358ms/step - loss: 0.1478 - acc: 0.9549 - val_loss: 0.0729 - val_acc: 0.9770\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 5s 352ms/step - loss: 0.1434 - acc: 0.9559 - val_loss: 0.0662 - val_acc: 0.9781\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 5s 332ms/step - loss: 0.1429 - acc: 0.9561 - val_loss: 0.0784 - val_acc: 0.9753\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 5s 321ms/step - loss: 0.1441 - acc: 0.9546 - val_loss: 0.0677 - val_acc: 0.9781\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 6s 365ms/step - loss: 0.1411 - acc: 0.9553 - val_loss: 0.0712 - val_acc: 0.9781\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 5s 329ms/step - loss: 0.1415 - acc: 0.9553 - val_loss: 0.0658 - val_acc: 0.9795\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 5s 360ms/step - loss: 0.1420 - acc: 0.9561 - val_loss: 0.0665 - val_acc: 0.9782\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 5s 319ms/step - loss: 0.1443 - acc: 0.9552 - val_loss: 0.0700 - val_acc: 0.9782\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 5s 354ms/step - loss: 0.1401 - acc: 0.9564 - val_loss: 0.0703 - val_acc: 0.9767\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 5s 359ms/step - loss: 0.1419 - acc: 0.9565 - val_loss: 0.0695 - val_acc: 0.9789\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 4s 293ms/step - loss: 0.1393 - acc: 0.9569 - val_loss: 0.0761 - val_acc: 0.9767\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 5s 335ms/step - loss: 0.1398 - acc: 0.9558 - val_loss: 0.0728 - val_acc: 0.9771\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 5s 325ms/step - loss: 0.1411 - acc: 0.9556 - val_loss: 0.0702 - val_acc: 0.9766\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 5s 321ms/step - loss: 0.1409 - acc: 0.9565 - val_loss: 0.0744 - val_acc: 0.9774\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 5s 354ms/step - loss: 0.1371 - acc: 0.9574 - val_loss: 0.0701 - val_acc: 0.9771\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 4s 305ms/step - loss: 0.1396 - acc: 0.9562 - val_loss: 0.0735 - val_acc: 0.9760\n",
      "Epoch 172/200\n",
      "15/15 [==============================] - 5s 323ms/step - loss: 0.1437 - acc: 0.9554 - val_loss: 0.0731 - val_acc: 0.9756\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 5s 286ms/step - loss: 0.1369 - acc: 0.9577 - val_loss: 0.0743 - val_acc: 0.9757\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 5s 326ms/step - loss: 0.1441 - acc: 0.9552 - val_loss: 0.0647 - val_acc: 0.9801\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 5s 345ms/step - loss: 0.1439 - acc: 0.9549 - val_loss: 0.0741 - val_acc: 0.9750\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 5s 360ms/step - loss: 0.1430 - acc: 0.9553 - val_loss: 0.0620 - val_acc: 0.9803\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 5s 356ms/step - loss: 0.1409 - acc: 0.9557 - val_loss: 0.0667 - val_acc: 0.9787\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 5s 347ms/step - loss: 0.1430 - acc: 0.9550 - val_loss: 0.0854 - val_acc: 0.9724\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 5s 358ms/step - loss: 0.1412 - acc: 0.9564 - val_loss: 0.0678 - val_acc: 0.9786\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 5s 346ms/step - loss: 0.1425 - acc: 0.9554 - val_loss: 0.0685 - val_acc: 0.9787\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 5s 338ms/step - loss: 0.1428 - acc: 0.9560 - val_loss: 0.0795 - val_acc: 0.9734\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 5s 329ms/step - loss: 0.1400 - acc: 0.9558 - val_loss: 0.0786 - val_acc: 0.9744\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 5s 350ms/step - loss: 0.1433 - acc: 0.9550 - val_loss: 0.0676 - val_acc: 0.9787\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 5s 336ms/step - loss: 0.1416 - acc: 0.9564 - val_loss: 0.0655 - val_acc: 0.9791\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 6s 365ms/step - loss: 0.1407 - acc: 0.9566 - val_loss: 0.0652 - val_acc: 0.9793\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 8s 528ms/step - loss: 0.1389 - acc: 0.9568 - val_loss: 0.0728 - val_acc: 0.9752\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 5s 355ms/step - loss: 0.1385 - acc: 0.9565 - val_loss: 0.0689 - val_acc: 0.9783\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 5s 340ms/step - loss: 0.1432 - acc: 0.9549 - val_loss: 0.0867 - val_acc: 0.9732\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 5s 350ms/step - loss: 0.1414 - acc: 0.9562 - val_loss: 0.0727 - val_acc: 0.9769\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 5s 333ms/step - loss: 0.1445 - acc: 0.9556 - val_loss: 0.0850 - val_acc: 0.9731\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 5s 350ms/step - loss: 0.1437 - acc: 0.9565 - val_loss: 0.0649 - val_acc: 0.9791\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 5s 340ms/step - loss: 0.1448 - acc: 0.9545 - val_loss: 0.0691 - val_acc: 0.9783\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 5s 352ms/step - loss: 0.1417 - acc: 0.9550 - val_loss: 0.0714 - val_acc: 0.9764\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 5s 353ms/step - loss: 0.1405 - acc: 0.9559 - val_loss: 0.0633 - val_acc: 0.9787\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 5s 354ms/step - loss: 0.1398 - acc: 0.9566 - val_loss: 0.0694 - val_acc: 0.9796\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 5s 363ms/step - loss: 0.1446 - acc: 0.9556 - val_loss: 0.0736 - val_acc: 0.9785\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 5s 350ms/step - loss: 0.1392 - acc: 0.9567 - val_loss: 0.0650 - val_acc: 0.9785\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 5s 337ms/step - loss: 0.1445 - acc: 0.9556 - val_loss: 0.0768 - val_acc: 0.9755\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 5s 341ms/step - loss: 0.1414 - acc: 0.9557 - val_loss: 0.0699 - val_acc: 0.9760\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 5s 345ms/step - loss: 0.1414 - acc: 0.9557 - val_loss: 0.0714 - val_acc: 0.9777\n"
     ]
    }
   ],
   "source": [
    "qmodel.load_weights(\"qmodel.h5\")\n",
    "with cur_strategy.scope():\n",
    "  optimizer = Adam(lr=0.02)\n",
    "  qmodel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "  qmodel.fit(x_train, y_train, epochs=200, batch_size=4096, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fr95jcPROz7p"
   },
   "source": [
    "One of problems of trying to quantize the whole thing in one shot is that we may end up with too many choices to make, which will make the entire search space very high. In order to reduce the search space, `AutoQKeras` has two methods to enable users to cope with the explosion of choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zc7ZrnbPIJA"
   },
   "source": [
    "## Grouping Layers to Use the Same Choice\n",
    "\n",
    "In this case, we can provide regular expressions to `limit` to specify layer names that should be grouped together. In our example, suppose we want to group  convolution layers (except the first one) and all activations except the last one to use the same quantization.\n",
    "\n",
    "For the first convolution layer, we want to limit the quantization types to fewer choices as the input is already an 8-bit number.  The last activation will be fed to a feature classifier layer, so we may leave it with more bits. Because our `dense` is actually a `Conv2D` operation, we will enable 8-bits for the weights by layer name. \n",
    "\n",
    "We first need to look at the names of the layers for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "height": 428
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1591840952867,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "w-d8nhG0pJF0",
    "outputId": "6529b630-f382-4e2a-94ef-ba3d9e3f875c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input',\n",
      " 'conv2d_0',\n",
      " 'bn_0',\n",
      " 'act_0',\n",
      " 'drop_0',\n",
      " 'conv2d_1',\n",
      " 'bn_1',\n",
      " 'act_1',\n",
      " 'drop_1',\n",
      " 'conv2d_2',\n",
      " 'bn_2',\n",
      " 'act_2',\n",
      " 'drop_2',\n",
      " 'conv2d_3',\n",
      " 'bn_3',\n",
      " 'act_3',\n",
      " 'drop_3',\n",
      " 'conv2d_4',\n",
      " 'bn_4',\n",
      " 'act_4',\n",
      " 'drop_4',\n",
      " 'flatten',\n",
      " 'dense',\n",
      " 'softmax']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint([layer.name for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32Enp890pU_4"
   },
   "source": [
    "Convolution layers for `mnist` have names specified as `conv2d_[01234]`. Activation layers have names specified as `act_[01234]`. So, we can create the following regular expressions to reduce the search space in our model.\n",
    "\n",
    "Please note that layer class names always select different quantizers, so the user needs to specify a pattern for layer names if he/she wants to use the same quantization for the group of layers.\n",
    "\n",
    "You can see here another feature of the limit. You can specify the maximum number of bits, or cherry pick which quantizers you want to try for a specific layer if instead of the maximum number of bits you specify a list of quantizers fron `quantization_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5XItp95PHW6"
   },
   "outputs": [],
   "source": [
    "limit = {\n",
    "    \"Dense\": [8, 8, 4],\n",
    "    \"Conv2D\": [4, 8, 4],\n",
    "    \"DepthwiseConv2D\": [4, 8, 4],\n",
    "    \"Activation\": [4],\n",
    "    \"BatchNormalization\": [],\n",
    "\n",
    "    \"^conv2d_0$\": [\n",
    "                   [\"binary\", \"ternary\", \"quantized_bits(2,1,1,alpha=1.0)\"],\n",
    "                   8, 4\n",
    "    ],\n",
    "    \"^conv2d_[1234]$\": [4, 8, 4],\n",
    "    \"^act_[0123]$\": [4],\n",
    "    \"^act_4$\": [8],\n",
    "    \"^dense$\": [8, 8, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJs1L-jIie7w"
   },
   "outputs": [],
   "source": [
    "run_config = {\n",
    "  \"output_dir\": tempfile.mkdtemp(),\n",
    "  \"goal\": goal,\n",
    "  \"quantization_config\": quantization_config,\n",
    "  \"learning_rate_optimizer\": False,\n",
    "  \"transfer_weights\": False,\n",
    "  \"mode\": \"random\",\n",
    "  \"seed\": 42,\n",
    "  \"limit\": limit,\n",
    "  \"tune_filters\": \"layer\",\n",
    "  \"tune_filters_exceptions\": \"^dense\",\n",
    "  \"distribution_strategy\": cur_strategy,\n",
    "  \"layer_indexes\": range(1, len(model.layers) - 1),\n",
    "  \"max_trials\": 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 993665,
     "status": "ok",
     "timestamp": 1591841947161,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "c7eSwXyijhzc",
    "outputId": "6c76a21f-cbb3-4bc5-b899-b02c28821b78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 Complete [00h 02m 41s]\n",
      "val_score: 1.2728118896484375\n",
      "\n",
      "Best val_score So Far: 1.4286894798278809\n",
      "Total elapsed time: 02h 01m 04s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "autoqk = AutoQKeras(model, metrics=[\"acc\"], custom_objects=custom_objects, **run_config)\n",
    "autoqk.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=1024, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sYp8Z2pnLi1"
   },
   "source": [
    "Let's see the reduction now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7109,
     "status": "ok",
     "timestamp": 1591841954308,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "yj826gNhjsfK",
    "outputId": "2e7f17d7-794e-44f6-d23a-452759727a53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.019999999552965164\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_0 (QConv2D)          (None, 14, 14, 8)         72        \n",
      "                                                                 \n",
      " bn_0 (QBatchNormalization)  (None, 14, 14, 8)         32        \n",
      "                                                                 \n",
      " act_0 (QActivation)         (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " drop_0 (Dropout)            (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (QConv2D)          (None, 7, 7, 16)          1152      \n",
      "                                                                 \n",
      " bn_1 (QBatchNormalization)  (None, 7, 7, 16)          64        \n",
      "                                                                 \n",
      " act_1 (QActivation)         (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " drop_1 (Dropout)            (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " conv2d_2 (QConv2D)          (None, 4, 4, 24)          3456      \n",
      "                                                                 \n",
      " bn_2 (QBatchNormalization)  (None, 4, 4, 24)          96        \n",
      "                                                                 \n",
      " act_2 (QActivation)         (None, 4, 4, 24)          0         \n",
      "                                                                 \n",
      " drop_2 (Dropout)            (None, 4, 4, 24)          0         \n",
      "                                                                 \n",
      " conv2d_3 (QConv2D)          (None, 2, 2, 64)          13824     \n",
      "                                                                 \n",
      " bn_3 (QBatchNormalization)  (None, 2, 2, 64)          256       \n",
      "                                                                 \n",
      " act_3 (QActivation)         (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " drop_3 (Dropout)            (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (QConv2D)          (None, 1, 1, 64)          36864     \n",
      "                                                                 \n",
      " bn_4 (QBatchNormalization)  (None, 1, 1, 64)          256       \n",
      "                                                                 \n",
      " act_4 (QActivation)         (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " drop_4 (Dropout)            (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (QDense)              (None, 10)                650       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,722\n",
      "Trainable params: 56,370\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n",
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=10030 reference_size=574175\n",
      "       delta=46.71%\n",
      "Total Cost Reduction:\n",
      "       10030 vs 574175 (-98.25%)\n",
      "conv2d_0             f=8 ternary(alpha='auto_po2') \n",
      "bn_0                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_0                quantized_relu(3,1)\n",
      "conv2d_1             f=16 stochastic_ternary(alpha='auto_po2') \n",
      "bn_1                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_1                quantized_relu(3,1)\n",
      "conv2d_2             f=24 stochastic_ternary(alpha='auto_po2') \n",
      "bn_2                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_2                quantized_relu(3,1)\n",
      "conv2d_3             f=64 stochastic_ternary(alpha='auto_po2') \n",
      "bn_3                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_3                quantized_relu(3,1)\n",
      "conv2d_4             f=64 stochastic_ternary(alpha='auto_po2') \n",
      "bn_4                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_4                quantized_relu_po2(4,4)\n",
      "dense                u=10 stochastic_binary(alpha='auto_po2') quantized_po2(4,8) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel = autoqk.get_best_model()\n",
    "qmodel.save_weights(\"qmodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXMcqxLAnY8t"
   },
   "source": [
    "Let's train this model for more time to see how much we can get in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68145,
     "status": "ok",
     "timestamp": 1591842022471,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "qpT8QgkJnQPa",
    "outputId": "61e711db-6187-4047-dae8-9ce2d093f56c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "15/15 [==============================] - 16s 455ms/step - loss: 1.3290 - acc: 0.5535 - val_loss: 2.1514 - val_acc: 0.6137\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 0.6596 - acc: 0.7860 - val_loss: 1.5057 - val_acc: 0.7240\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 4s 247ms/step - loss: 0.4349 - acc: 0.8612 - val_loss: 1.3564 - val_acc: 0.7383\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 3s 227ms/step - loss: 0.3452 - acc: 0.8933 - val_loss: 0.5676 - val_acc: 0.8611\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 4s 237ms/step - loss: 0.2982 - acc: 0.9086 - val_loss: 0.3139 - val_acc: 0.9130\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 4s 253ms/step - loss: 0.2655 - acc: 0.9182 - val_loss: 0.1886 - val_acc: 0.9448\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 3s 225ms/step - loss: 0.2581 - acc: 0.9219 - val_loss: 0.1904 - val_acc: 0.9438\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 4s 239ms/step - loss: 0.2430 - acc: 0.9262 - val_loss: 0.2037 - val_acc: 0.9417\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 4s 246ms/step - loss: 0.2394 - acc: 0.9275 - val_loss: 0.1366 - val_acc: 0.9577\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 4s 246ms/step - loss: 0.2390 - acc: 0.9279 - val_loss: 0.1802 - val_acc: 0.9482\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 3s 233ms/step - loss: 0.2352 - acc: 0.9295 - val_loss: 0.1439 - val_acc: 0.9590\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 4s 259ms/step - loss: 0.2326 - acc: 0.9305 - val_loss: 0.1365 - val_acc: 0.9600\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 3s 214ms/step - loss: 0.2209 - acc: 0.9336 - val_loss: 0.1137 - val_acc: 0.9655\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 4s 262ms/step - loss: 0.2124 - acc: 0.9365 - val_loss: 0.1518 - val_acc: 0.9591\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 3s 211ms/step - loss: 0.2301 - acc: 0.9322 - val_loss: 0.1431 - val_acc: 0.9632\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 4s 262ms/step - loss: 0.2159 - acc: 0.9358 - val_loss: 0.1412 - val_acc: 0.9592\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 4s 260ms/step - loss: 0.2058 - acc: 0.9372 - val_loss: 0.1436 - val_acc: 0.9600\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 4s 270ms/step - loss: 0.2087 - acc: 0.9366 - val_loss: 0.1284 - val_acc: 0.9625\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 3s 232ms/step - loss: 0.1989 - acc: 0.9404 - val_loss: 0.1282 - val_acc: 0.9625\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 3s 214ms/step - loss: 0.2059 - acc: 0.9382 - val_loss: 0.1233 - val_acc: 0.9657\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 4s 250ms/step - loss: 0.1878 - acc: 0.9436 - val_loss: 0.1357 - val_acc: 0.9625\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 4s 245ms/step - loss: 0.1839 - acc: 0.9436 - val_loss: 0.0861 - val_acc: 0.9737\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 4s 235ms/step - loss: 0.1747 - acc: 0.9468 - val_loss: 0.1155 - val_acc: 0.9649\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 3s 229ms/step - loss: 0.1782 - acc: 0.9460 - val_loss: 0.1060 - val_acc: 0.9685\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 4s 272ms/step - loss: 0.1638 - acc: 0.9509 - val_loss: 0.0808 - val_acc: 0.9759\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 3s 226ms/step - loss: 0.1642 - acc: 0.9502 - val_loss: 0.0772 - val_acc: 0.9747\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 4s 248ms/step - loss: 0.1536 - acc: 0.9537 - val_loss: 0.0963 - val_acc: 0.9699\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 4s 242ms/step - loss: 0.1608 - acc: 0.9511 - val_loss: 0.0777 - val_acc: 0.9769\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 4s 269ms/step - loss: 0.1620 - acc: 0.9499 - val_loss: 0.0667 - val_acc: 0.9792\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 4s 259ms/step - loss: 0.1601 - acc: 0.9511 - val_loss: 0.0714 - val_acc: 0.9776\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 4s 263ms/step - loss: 0.1571 - acc: 0.9521 - val_loss: 0.0726 - val_acc: 0.9771\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 3s 224ms/step - loss: 0.1540 - acc: 0.9536 - val_loss: 0.0673 - val_acc: 0.9781\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 4s 268ms/step - loss: 0.1496 - acc: 0.9543 - val_loss: 0.0645 - val_acc: 0.9785\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 4s 239ms/step - loss: 0.1549 - acc: 0.9527 - val_loss: 0.0725 - val_acc: 0.9776\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 4s 270ms/step - loss: 0.1561 - acc: 0.9521 - val_loss: 0.0714 - val_acc: 0.9778\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 4s 257ms/step - loss: 0.1475 - acc: 0.9545 - val_loss: 0.0686 - val_acc: 0.9779\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 4s 261ms/step - loss: 0.1492 - acc: 0.9541 - val_loss: 0.0700 - val_acc: 0.9778\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 4s 264ms/step - loss: 0.1493 - acc: 0.9544 - val_loss: 0.0750 - val_acc: 0.9785\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 3s 223ms/step - loss: 0.1491 - acc: 0.9538 - val_loss: 0.0708 - val_acc: 0.9775\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 3s 228ms/step - loss: 0.1502 - acc: 0.9541 - val_loss: 0.0624 - val_acc: 0.9806\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 4s 265ms/step - loss: 0.1507 - acc: 0.9536 - val_loss: 0.0681 - val_acc: 0.9767\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 4s 226ms/step - loss: 0.1492 - acc: 0.9541 - val_loss: 0.0781 - val_acc: 0.9767\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 4s 270ms/step - loss: 0.1495 - acc: 0.9535 - val_loss: 0.0630 - val_acc: 0.9795\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 4s 247ms/step - loss: 0.1509 - acc: 0.9537 - val_loss: 0.0672 - val_acc: 0.9788\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 4s 262ms/step - loss: 0.1441 - acc: 0.9551 - val_loss: 0.0683 - val_acc: 0.9792\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 4s 256ms/step - loss: 0.1519 - acc: 0.9534 - val_loss: 0.0635 - val_acc: 0.9780\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 3s 217ms/step - loss: 0.1466 - acc: 0.9556 - val_loss: 0.0907 - val_acc: 0.9734\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 3s 234ms/step - loss: 0.1492 - acc: 0.9534 - val_loss: 0.0650 - val_acc: 0.9794\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 4s 260ms/step - loss: 0.1479 - acc: 0.9549 - val_loss: 0.0825 - val_acc: 0.9746\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 4s 244ms/step - loss: 0.1473 - acc: 0.9552 - val_loss: 0.0690 - val_acc: 0.9782\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 4s 264ms/step - loss: 0.1479 - acc: 0.9560 - val_loss: 0.0747 - val_acc: 0.9772\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 4s 235ms/step - loss: 0.1461 - acc: 0.9560 - val_loss: 0.0645 - val_acc: 0.9795\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 4s 257ms/step - loss: 0.1469 - acc: 0.9553 - val_loss: 0.0629 - val_acc: 0.9812\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 4s 259ms/step - loss: 0.1445 - acc: 0.9563 - val_loss: 0.0634 - val_acc: 0.9798\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 4s 263ms/step - loss: 0.1425 - acc: 0.9569 - val_loss: 0.0653 - val_acc: 0.9788\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 0.1403 - acc: 0.9567 - val_loss: 0.0727 - val_acc: 0.9763\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 4s 271ms/step - loss: 0.1435 - acc: 0.9556 - val_loss: 0.0642 - val_acc: 0.9770\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 4s 249ms/step - loss: 0.1428 - acc: 0.9564 - val_loss: 0.0686 - val_acc: 0.9799\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 4s 276ms/step - loss: 0.1456 - acc: 0.9550 - val_loss: 0.0575 - val_acc: 0.9804\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 0.1473 - acc: 0.9548 - val_loss: 0.0793 - val_acc: 0.9752\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 4s 272ms/step - loss: 0.1495 - acc: 0.9545 - val_loss: 0.0594 - val_acc: 0.9804\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 4s 265ms/step - loss: 0.1487 - acc: 0.9552 - val_loss: 0.0688 - val_acc: 0.9789\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 3s 231ms/step - loss: 0.1496 - acc: 0.9537 - val_loss: 0.0628 - val_acc: 0.9798\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 4s 255ms/step - loss: 0.1418 - acc: 0.9561 - val_loss: 0.0649 - val_acc: 0.9809\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 4s 260ms/step - loss: 0.1438 - acc: 0.9563 - val_loss: 0.0640 - val_acc: 0.9793\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 4s 253ms/step - loss: 0.1452 - acc: 0.9554 - val_loss: 0.0605 - val_acc: 0.9811\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 4s 244ms/step - loss: 0.1491 - acc: 0.9543 - val_loss: 0.0670 - val_acc: 0.9798\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 4s 244ms/step - loss: 0.1472 - acc: 0.9544 - val_loss: 0.0702 - val_acc: 0.9790\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 3s 218ms/step - loss: 0.1431 - acc: 0.9554 - val_loss: 0.0847 - val_acc: 0.9734\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 4s 266ms/step - loss: 0.1406 - acc: 0.9577 - val_loss: 0.0582 - val_acc: 0.9818\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 4s 236ms/step - loss: 0.1384 - acc: 0.9580 - val_loss: 0.0604 - val_acc: 0.9810\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 4s 266ms/step - loss: 0.1391 - acc: 0.9571 - val_loss: 0.0533 - val_acc: 0.9833\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 4s 270ms/step - loss: 0.1467 - acc: 0.9551 - val_loss: 0.0752 - val_acc: 0.9762\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 4s 268ms/step - loss: 0.1415 - acc: 0.9566 - val_loss: 0.0613 - val_acc: 0.9805\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 3s 226ms/step - loss: 0.1390 - acc: 0.9579 - val_loss: 0.0641 - val_acc: 0.9802\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 0.1433 - acc: 0.9567 - val_loss: 0.0557 - val_acc: 0.9813\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 4s 262ms/step - loss: 0.1428 - acc: 0.9563 - val_loss: 0.0641 - val_acc: 0.9805\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 4s 241ms/step - loss: 0.1419 - acc: 0.9573 - val_loss: 0.0586 - val_acc: 0.9808\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 4s 228ms/step - loss: 0.1426 - acc: 0.9567 - val_loss: 0.0812 - val_acc: 0.9741\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 4s 241ms/step - loss: 0.1467 - acc: 0.9555 - val_loss: 0.0687 - val_acc: 0.9784\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 4s 257ms/step - loss: 0.1456 - acc: 0.9555 - val_loss: 0.0575 - val_acc: 0.9805\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 4s 254ms/step - loss: 0.1405 - acc: 0.9571 - val_loss: 0.0561 - val_acc: 0.9829\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 4s 242ms/step - loss: 0.1400 - acc: 0.9576 - val_loss: 0.0632 - val_acc: 0.9798\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 3s 231ms/step - loss: 0.1369 - acc: 0.9586 - val_loss: 0.0606 - val_acc: 0.9806\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 4s 261ms/step - loss: 0.1440 - acc: 0.9569 - val_loss: 0.0675 - val_acc: 0.9790\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 3s 221ms/step - loss: 0.1416 - acc: 0.9570 - val_loss: 0.0580 - val_acc: 0.9808\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 3s 213ms/step - loss: 0.1403 - acc: 0.9578 - val_loss: 0.0611 - val_acc: 0.9800\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 4s 258ms/step - loss: 0.1361 - acc: 0.9588 - val_loss: 0.0655 - val_acc: 0.9796\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 4s 240ms/step - loss: 0.1386 - acc: 0.9581 - val_loss: 0.0732 - val_acc: 0.9785\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 4s 259ms/step - loss: 0.1417 - acc: 0.9564 - val_loss: 0.0678 - val_acc: 0.9796\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 3s 235ms/step - loss: 0.1468 - acc: 0.9549 - val_loss: 0.0603 - val_acc: 0.9802\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 4s 237ms/step - loss: 0.1368 - acc: 0.9586 - val_loss: 0.0719 - val_acc: 0.9775\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 4s 239ms/step - loss: 0.1339 - acc: 0.9589 - val_loss: 0.0604 - val_acc: 0.9816\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 4s 246ms/step - loss: 0.1434 - acc: 0.9563 - val_loss: 0.0606 - val_acc: 0.9806\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 4s 254ms/step - loss: 0.1423 - acc: 0.9562 - val_loss: 0.0616 - val_acc: 0.9808\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 3s 229ms/step - loss: 0.1376 - acc: 0.9577 - val_loss: 0.0591 - val_acc: 0.9816\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 3s 228ms/step - loss: 0.1328 - acc: 0.9599 - val_loss: 0.0540 - val_acc: 0.9821\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 3s 235ms/step - loss: 0.1393 - acc: 0.9574 - val_loss: 0.0726 - val_acc: 0.9762\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 4s 233ms/step - loss: 0.1450 - acc: 0.9559 - val_loss: 0.0641 - val_acc: 0.9797\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 4s 263ms/step - loss: 0.1388 - acc: 0.9577 - val_loss: 0.0607 - val_acc: 0.9811\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 3s 214ms/step - loss: 0.1412 - acc: 0.9565 - val_loss: 0.0609 - val_acc: 0.9801\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 4s 255ms/step - loss: 0.1488 - acc: 0.9555 - val_loss: 0.0643 - val_acc: 0.9805\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 3s 229ms/step - loss: 0.1415 - acc: 0.9571 - val_loss: 0.0621 - val_acc: 0.9795\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 4s 265ms/step - loss: 0.1433 - acc: 0.9570 - val_loss: 0.0674 - val_acc: 0.9803\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 4s 250ms/step - loss: 0.1434 - acc: 0.9557 - val_loss: 0.0683 - val_acc: 0.9779\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 4s 244ms/step - loss: 0.1481 - acc: 0.9559 - val_loss: 0.0621 - val_acc: 0.9814\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 4s 225ms/step - loss: 0.1429 - acc: 0.9554 - val_loss: 0.0875 - val_acc: 0.9745\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 4s 257ms/step - loss: 0.1512 - acc: 0.9553 - val_loss: 0.0769 - val_acc: 0.9772\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 4s 256ms/step - loss: 0.1383 - acc: 0.9573 - val_loss: 0.0612 - val_acc: 0.9829\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 3s 227ms/step - loss: 0.1443 - acc: 0.9568 - val_loss: 0.0730 - val_acc: 0.9781\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 4s 257ms/step - loss: 0.1434 - acc: 0.9569 - val_loss: 0.0610 - val_acc: 0.9817\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 4s 253ms/step - loss: 0.1420 - acc: 0.9579 - val_loss: 0.0653 - val_acc: 0.9813\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 4s 259ms/step - loss: 0.1448 - acc: 0.9563 - val_loss: 0.0584 - val_acc: 0.9824\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 3s 223ms/step - loss: 0.1382 - acc: 0.9583 - val_loss: 0.0707 - val_acc: 0.9757\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - 3s 225ms/step - loss: 0.1392 - acc: 0.9579 - val_loss: 0.0557 - val_acc: 0.9820\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 4s 263ms/step - loss: 0.1381 - acc: 0.9576 - val_loss: 0.0661 - val_acc: 0.9800\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 4s 243ms/step - loss: 0.1371 - acc: 0.9590 - val_loss: 0.0690 - val_acc: 0.9815\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 4s 265ms/step - loss: 0.1467 - acc: 0.9568 - val_loss: 0.0620 - val_acc: 0.9804\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 4s 248ms/step - loss: 0.1356 - acc: 0.9590 - val_loss: 0.0528 - val_acc: 0.9834\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 4s 263ms/step - loss: 0.1347 - acc: 0.9587 - val_loss: 0.0666 - val_acc: 0.9802\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 3s 236ms/step - loss: 0.1384 - acc: 0.9577 - val_loss: 0.0555 - val_acc: 0.9834\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 4s 258ms/step - loss: 0.1445 - acc: 0.9564 - val_loss: 0.0621 - val_acc: 0.9815\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 4s 240ms/step - loss: 0.1367 - acc: 0.9591 - val_loss: 0.0603 - val_acc: 0.9812\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 4s 250ms/step - loss: 0.1458 - acc: 0.9558 - val_loss: 0.0854 - val_acc: 0.9754\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 4s 266ms/step - loss: 0.1452 - acc: 0.9556 - val_loss: 0.0680 - val_acc: 0.9781\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 4s 243ms/step - loss: 0.1422 - acc: 0.9578 - val_loss: 0.0693 - val_acc: 0.9812\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 4s 252ms/step - loss: 0.1349 - acc: 0.9593 - val_loss: 0.0642 - val_acc: 0.9806\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 4s 237ms/step - loss: 0.1382 - acc: 0.9579 - val_loss: 0.0617 - val_acc: 0.9809\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 4s 258ms/step - loss: 0.1390 - acc: 0.9571 - val_loss: 0.0594 - val_acc: 0.9820\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 4s 243ms/step - loss: 0.1420 - acc: 0.9576 - val_loss: 0.0624 - val_acc: 0.9813\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 3s 214ms/step - loss: 0.1468 - acc: 0.9555 - val_loss: 0.0694 - val_acc: 0.9795\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 4s 263ms/step - loss: 0.1442 - acc: 0.9560 - val_loss: 0.0680 - val_acc: 0.9782\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 4s 265ms/step - loss: 0.1386 - acc: 0.9587 - val_loss: 0.0667 - val_acc: 0.9800\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 4s 261ms/step - loss: 0.1397 - acc: 0.9578 - val_loss: 0.0748 - val_acc: 0.9775\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 4s 258ms/step - loss: 0.1410 - acc: 0.9584 - val_loss: 0.0602 - val_acc: 0.9822\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 3s 217ms/step - loss: 0.1468 - acc: 0.9558 - val_loss: 0.0786 - val_acc: 0.9791\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 3s 212ms/step - loss: 0.1484 - acc: 0.9564 - val_loss: 0.0716 - val_acc: 0.9792\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 4s 251ms/step - loss: 0.1531 - acc: 0.9546 - val_loss: 0.0966 - val_acc: 0.9721\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 4s 248ms/step - loss: 0.1527 - acc: 0.9545 - val_loss: 0.0642 - val_acc: 0.9812\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 4s 257ms/step - loss: 0.1427 - acc: 0.9567 - val_loss: 0.0662 - val_acc: 0.9811\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 4s 240ms/step - loss: 0.1513 - acc: 0.9554 - val_loss: 0.0558 - val_acc: 0.9836\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 4s 255ms/step - loss: 0.1413 - acc: 0.9585 - val_loss: 0.0676 - val_acc: 0.9796\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 4s 239ms/step - loss: 0.1448 - acc: 0.9568 - val_loss: 0.0831 - val_acc: 0.9769\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 4s 242ms/step - loss: 0.1514 - acc: 0.9558 - val_loss: 0.0730 - val_acc: 0.9777\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 4s 266ms/step - loss: 0.1459 - acc: 0.9570 - val_loss: 0.0964 - val_acc: 0.9741\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 4s 248ms/step - loss: 0.1468 - acc: 0.9566 - val_loss: 0.0770 - val_acc: 0.9775\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 4s 251ms/step - loss: 0.1535 - acc: 0.9559 - val_loss: 0.0886 - val_acc: 0.9749\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 4s 256ms/step - loss: 0.1440 - acc: 0.9564 - val_loss: 0.0735 - val_acc: 0.9751\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 4s 253ms/step - loss: 0.1450 - acc: 0.9560 - val_loss: 0.0603 - val_acc: 0.9811\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 0.1559 - acc: 0.9539 - val_loss: 0.0844 - val_acc: 0.9776\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 4s 250ms/step - loss: 0.1509 - acc: 0.9551 - val_loss: 0.0691 - val_acc: 0.9810\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 4s 251ms/step - loss: 0.1539 - acc: 0.9545 - val_loss: 0.0810 - val_acc: 0.9752\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 4s 260ms/step - loss: 0.1558 - acc: 0.9543 - val_loss: 0.0599 - val_acc: 0.9811\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 4s 253ms/step - loss: 0.1505 - acc: 0.9549 - val_loss: 0.0765 - val_acc: 0.9782\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 4s 263ms/step - loss: 0.1608 - acc: 0.9525 - val_loss: 0.1314 - val_acc: 0.9666\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 3s 226ms/step - loss: 0.1586 - acc: 0.9532 - val_loss: 0.0709 - val_acc: 0.9785\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 4s 257ms/step - loss: 0.1512 - acc: 0.9563 - val_loss: 0.0670 - val_acc: 0.9800\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 0.1496 - acc: 0.9562 - val_loss: 0.0674 - val_acc: 0.9797\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 4s 246ms/step - loss: 0.1488 - acc: 0.9568 - val_loss: 0.0683 - val_acc: 0.9811\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 4s 233ms/step - loss: 0.1576 - acc: 0.9547 - val_loss: 0.0829 - val_acc: 0.9768\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 4s 238ms/step - loss: 0.1610 - acc: 0.9530 - val_loss: 0.0811 - val_acc: 0.9751\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 4s 238ms/step - loss: 0.1526 - acc: 0.9548 - val_loss: 0.0721 - val_acc: 0.9767\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 4s 245ms/step - loss: 0.1510 - acc: 0.9556 - val_loss: 0.0647 - val_acc: 0.9819\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 4s 252ms/step - loss: 0.1554 - acc: 0.9545 - val_loss: 0.0742 - val_acc: 0.9798\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 4s 241ms/step - loss: 0.1450 - acc: 0.9572 - val_loss: 0.0742 - val_acc: 0.9789\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 3s 235ms/step - loss: 0.1486 - acc: 0.9569 - val_loss: 0.0647 - val_acc: 0.9799\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 4s 249ms/step - loss: 0.1477 - acc: 0.9565 - val_loss: 0.1101 - val_acc: 0.9645\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 4s 237ms/step - loss: 0.1543 - acc: 0.9550 - val_loss: 0.0752 - val_acc: 0.9765\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 3s 206ms/step - loss: 0.1514 - acc: 0.9554 - val_loss: 0.0882 - val_acc: 0.9746\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 4s 253ms/step - loss: 0.1610 - acc: 0.9539 - val_loss: 0.0902 - val_acc: 0.9743\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 4s 242ms/step - loss: 0.1627 - acc: 0.9524 - val_loss: 0.1141 - val_acc: 0.9657\n",
      "Epoch 172/200\n",
      "15/15 [==============================] - 4s 249ms/step - loss: 0.1525 - acc: 0.9547 - val_loss: 0.0673 - val_acc: 0.9796\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 4s 255ms/step - loss: 0.1451 - acc: 0.9578 - val_loss: 0.0700 - val_acc: 0.9798\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 4s 256ms/step - loss: 0.1435 - acc: 0.9569 - val_loss: 0.0645 - val_acc: 0.9810\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 4s 240ms/step - loss: 0.1466 - acc: 0.9569 - val_loss: 0.0880 - val_acc: 0.9709\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 3s 228ms/step - loss: 0.1497 - acc: 0.9568 - val_loss: 0.0739 - val_acc: 0.9773\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 4s 231ms/step - loss: 0.1491 - acc: 0.9571 - val_loss: 0.1078 - val_acc: 0.9687\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 4s 233ms/step - loss: 0.1577 - acc: 0.9548 - val_loss: 0.0767 - val_acc: 0.9778\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 4s 254ms/step - loss: 0.1562 - acc: 0.9536 - val_loss: 0.0748 - val_acc: 0.9804\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 4s 238ms/step - loss: 0.1565 - acc: 0.9547 - val_loss: 0.0816 - val_acc: 0.9782\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 4s 247ms/step - loss: 0.1567 - acc: 0.9540 - val_loss: 0.0714 - val_acc: 0.9793\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 4s 245ms/step - loss: 0.1592 - acc: 0.9545 - val_loss: 0.0655 - val_acc: 0.9808\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 3s 210ms/step - loss: 0.1480 - acc: 0.9569 - val_loss: 0.0808 - val_acc: 0.9778\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 3s 221ms/step - loss: 0.1530 - acc: 0.9554 - val_loss: 0.0799 - val_acc: 0.9776\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 3s 224ms/step - loss: 0.1445 - acc: 0.9582 - val_loss: 0.0807 - val_acc: 0.9761\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 4s 248ms/step - loss: 0.1399 - acc: 0.9580 - val_loss: 0.0590 - val_acc: 0.9810\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 4s 246ms/step - loss: 0.1512 - acc: 0.9556 - val_loss: 0.0704 - val_acc: 0.9800\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 3s 237ms/step - loss: 0.1521 - acc: 0.9544 - val_loss: 0.0734 - val_acc: 0.9803\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 4s 237ms/step - loss: 0.1473 - acc: 0.9574 - val_loss: 0.0801 - val_acc: 0.9765\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 3s 223ms/step - loss: 0.1510 - acc: 0.9562 - val_loss: 0.0749 - val_acc: 0.9782\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 3s 190ms/step - loss: 0.1482 - acc: 0.9570 - val_loss: 0.0655 - val_acc: 0.9817\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 4s 254ms/step - loss: 0.1509 - acc: 0.9562 - val_loss: 0.0809 - val_acc: 0.9774\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 4s 236ms/step - loss: 0.1494 - acc: 0.9559 - val_loss: 0.0727 - val_acc: 0.9795\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 4s 233ms/step - loss: 0.1467 - acc: 0.9576 - val_loss: 0.0854 - val_acc: 0.9731\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 4s 238ms/step - loss: 0.1545 - acc: 0.9553 - val_loss: 0.0867 - val_acc: 0.9745\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 0.1550 - acc: 0.9546 - val_loss: 0.0704 - val_acc: 0.9805\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 3s 225ms/step - loss: 0.1503 - acc: 0.9559 - val_loss: 0.0635 - val_acc: 0.9823\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 3s 190ms/step - loss: 0.1420 - acc: 0.9584 - val_loss: 0.0681 - val_acc: 0.9801\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 3s 223ms/step - loss: 0.1490 - acc: 0.9566 - val_loss: 0.0712 - val_acc: 0.9795\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 3s 221ms/step - loss: 0.1542 - acc: 0.9546 - val_loss: 0.0686 - val_acc: 0.9803\n"
     ]
    }
   ],
   "source": [
    "qmodel.load_weights(\"qmodel.h5\")\n",
    "with cur_strategy.scope():\n",
    "  optimizer = Adam(lr=0.02)\n",
    "  qmodel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "  qmodel.fit(x_train, y_train, epochs=200, batch_size=4096, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAV6Kw0QoODq"
   },
   "source": [
    "## Quantization by Blocks\n",
    "\n",
    "In the previous section, we enforced that all decisions were the same in order to reduce the number of options to quantize a model. \n",
    "\n",
    "Another approach is still to allow models to have each block of layers to makde their own choice, but quantizing the blocks sequentially, either from inputs to outputs, or by quantizing higher energy blocks first.\n",
    "\n",
    "The rationale for this method is that if we quantize the blocks one by one, and assuming that each block has $N$ choices, and $B$ blocks, we end up trying $N B$ options, instead of $N^B$ choices.  The reader should note that this is an approximation as there is no guarantee that we will obtain the best quantization possible.\n",
    "\n",
    "Should you do sequential from inputs to outputs or starting from the block that has the highest impact?\n",
    "\n",
    "If you have a network like ResNet, and if you want to do filter tuning, you need to block the layers by the resnet definition of a block, i.e. including full identity or convolutional blocks, and quantize the model from inputs to outputs, so that you can preserve at each stage the number of channels for the residual block. \n",
    "\n",
    "In order to perform quantization by blocks, you need to specify two other parameters in our `run_config`. `blocks` is a list of regular expressions of the groups you want to quantize. If a layer does not match the block pattern, it will not be quantized.  `schedule_block` specifies the mode for block quantization scheduling. It can be `sequential` or `cost` if you want to schedule first the blocks by decreasing cost size (energy or bits).\n",
    "\n",
    "In this model, there are a few optimizations that we perform automatically. First, we dynamically reduce the learning rate of the blocks that we have already quantized as setting them to not-trainable does not seem to work, so we still allow them to train, but at a slower pace. In addition, we try to dynamically adjust the learning rate for the layer we are trying to quantize as opposed to the learning rate of the unquantized layers. Finally, we transfer the weights of the models we have already quantized whenever we can do (if the shapes remain the same). \n",
    "\n",
    "Regardless on how we schedule the operations, we amortize the nubmer of trials for the cost of the block (energy or bits with respect to the total energy or number of bits of the network).\n",
    "\n",
    "Instead of invoking `AutoQKeras` now, we will invoke `AutoQKeras` scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUz4A6SKnhUf"
   },
   "outputs": [],
   "source": [
    "run_config = {\n",
    "  \"output_dir\": tempfile.mkdtemp(),\n",
    "  \"goal\": goal,\n",
    "  \"quantization_config\": quantization_config,\n",
    "  \"learning_rate_optimizer\": False,\n",
    "  \"transfer_weights\": False,\n",
    "  \"mode\": \"random\",\n",
    "  \"seed\": 42,\n",
    "  \"limit\": limit,\n",
    "  \"tune_filters\": \"layer\",\n",
    "  \"tune_filters_exceptions\": \"^dense\",\n",
    "  \"distribution_strategy\": cur_strategy,\n",
    "  \"layer_indexes\": range(1, len(model.layers) - 1),\n",
    "  \"max_trials\": 40,\n",
    "\n",
    "  \"blocks\": [\n",
    "    \"^.*_0$\",\n",
    "    \"^.*_1$\",\n",
    "    \"^.*_2$\",\n",
    "    \"^.*_3$\",\n",
    "    \"^.*_4$\",\n",
    "    \"^dense\"\n",
    "  ],\n",
    "  \"schedule_block\": \"cost\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWJiZZ9vsORJ"
   },
   "source": [
    "Because specifying regular expressions is error prone, we recommend that you first try to run `AutoQKerasScheduler` in debug mode to print the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "height": 737
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1591842023212,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "uSOxKQGwsqf2",
    "outputId": "18647e4f-ef7a-4c6a-aeb8-0c9c2039fdbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input',\n",
      " 'conv2d_0',\n",
      " 'bn_0',\n",
      " 'act_0',\n",
      " 'drop_0',\n",
      " 'conv2d_1',\n",
      " 'bn_1',\n",
      " 'act_1',\n",
      " 'drop_1',\n",
      " 'conv2d_2',\n",
      " 'bn_2',\n",
      " 'act_2',\n",
      " 'drop_2',\n",
      " 'conv2d_3',\n",
      " 'bn_3',\n",
      " 'act_3',\n",
      " 'drop_3',\n",
      " 'conv2d_4',\n",
      " 'bn_4',\n",
      " 'act_4',\n",
      " 'drop_4',\n",
      " 'flatten',\n",
      " 'dense',\n",
      " 'softmax']\n",
      "... block cost: 199940 / 574175\n",
      "... adjusting max_trials for this block to 13\n",
      "Pattern 0 is : {'bn_1': [], '^(conv2d_1)$': [4, 8, 4], '^(act_1)$': [4]}\n",
      "... block cost: 195890 / 574175\n",
      "... adjusting max_trials for this block to 13\n",
      "Pattern 1 is : {'bn_2': [], '^(conv2d_2)$': [4, 8, 4], '^(act_2)$': [4]}\n",
      "... block cost: 98754 / 574175\n",
      "... adjusting max_trials for this block to 10\n",
      "Pattern 2 is : {'bn_3': [], '^(conv2d_3)$': [4, 8, 4], '^(act_3)$': [4]}\n",
      "... block cost: 69621 / 574175\n",
      "... adjusting max_trials for this block to 10\n",
      "Pattern 3 is : {'bn_4': [], '^(conv2d_4)$': [4, 8, 4], '^(act_4)$': [8]}\n",
      "... block cost: 8788 / 574175\n",
      "... adjusting max_trials for this block to 10\n",
      "Pattern 4 is : {'bn_0': [], '^(conv2d_0)$': [['binary', 'ternary', 'quantized_bits(2,1,1,alpha=1.0)'], 8, 4], '^(act_0)$': [4]}\n",
      "... block cost: 1167 / 574175\n",
      "... adjusting max_trials for this block to 10\n",
      "Pattern 5 is : {'^(dense)$': [8, 8, 4]}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint([layer.name for layer in model.layers])\n",
    "autoqk = AutoQKerasScheduler(model, metrics=[\"acc\"], custom_objects=custom_objects, debug=True, **run_config)\n",
    "autoqk.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=1024, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQPUKPZhC_SI"
   },
   "source": [
    "All blocks seem to be fine. Let's find the best quantization now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1938883,
     "status": "ok",
     "timestamp": 1591843962106,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "sXt-cRKvDEaL",
    "outputId": "36db3217-86ff-4425-ee12-f637a4fc1841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 50s]\n",
      "val_score: 1.4008370637893677\n",
      "\n",
      "Best val_score So Far: 1.416741967201233\n",
      "Total elapsed time: 00h 07m 44s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in /tmp/tmp7td4spyh_1/5\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x7f43efea96d0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_bits(8,0,1,alpha=1.0)\n",
      "^(dense)$_bias_quantizer: quantized_bits(8,3,1)\n",
      "Score: 1.416741967201233\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_bits(4,0,1,alpha=1.0)\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4149357080459595\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: stochastic_binary\n",
      "^(dense)$_bias_quantizer: quantized_bits(8,3,1)\n",
      "Score: 1.414434552192688\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: binary\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4135743379592896\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: stochastic_binary\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4087834358215332\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: ternary\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4080085754394531\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_po2(4,1)\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4034206867218018\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_bits(2,1,1,alpha=1.0)\n",
      "^(dense)$_bias_quantizer: quantized_po2(4,8)\n",
      "Score: 1.4017080068588257\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_bits(2,1,1,alpha=1.0)\n",
      "^(dense)$_bias_quantizer: quantized_bits(8,3,1)\n",
      "Score: 1.4008370637893677\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: stochastic_ternary\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.40045964717865\n",
      "learning_rate: 0.019999999552965164\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_0 (QConv2D)          (None, 14, 14, 8)         72        \n",
      "                                                                 \n",
      " bn_0 (QBatchNormalization)  (None, 14, 14, 8)         32        \n",
      "                                                                 \n",
      " act_0 (QActivation)         (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " drop_0 (Dropout)            (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (QConv2D)          (None, 7, 7, 16)          1152      \n",
      "                                                                 \n",
      " bn_1 (QBatchNormalization)  (None, 7, 7, 16)          64        \n",
      "                                                                 \n",
      " act_1 (QActivation)         (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " drop_1 (Dropout)            (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " conv2d_2 (QConv2D)          (None, 4, 4, 24)          3456      \n",
      "                                                                 \n",
      " bn_2 (QBatchNormalization)  (None, 4, 4, 24)          96        \n",
      "                                                                 \n",
      " act_2 (QActivation)         (None, 4, 4, 24)          0         \n",
      "                                                                 \n",
      " drop_2 (Dropout)            (None, 4, 4, 24)          0         \n",
      "                                                                 \n",
      " conv2d_3 (QConv2D)          (None, 2, 2, 32)          6912      \n",
      "                                                                 \n",
      " bn_3 (QBatchNormalization)  (None, 2, 2, 32)          128       \n",
      "                                                                 \n",
      " act_3 (QActivation)         (None, 2, 2, 32)          0         \n",
      "                                                                 \n",
      " drop_3 (Dropout)            (None, 2, 2, 32)          0         \n",
      "                                                                 \n",
      " conv2d_4 (QConv2D)          (None, 1, 1, 64)          18432     \n",
      "                                                                 \n",
      " bn_4 (QBatchNormalization)  (None, 1, 1, 64)          256       \n",
      "                                                                 \n",
      " act_4 (QActivation)         (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " drop_4 (Dropout)            (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (QDense)              (None, 10)                650       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,250\n",
      "Trainable params: 30,962\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n",
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=11506 reference_size=574175\n",
      "       delta=45.13%\n",
      "Total Cost Reduction:\n",
      "       11506 vs 574175 (-98.00%)\n",
      "conv2d_0             f=8 binary(alpha='auto_po2') \n",
      "bn_0                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_0                quantized_relu(4,2)\n",
      "conv2d_1             f=16 binary(alpha='auto_po2') \n",
      "bn_1                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_1                quantized_relu(3,1)\n",
      "conv2d_2             f=24 quantized_bits(4,0,1,alpha=1.0) \n",
      "bn_2                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_2                quantized_relu_po2(4,4)\n",
      "conv2d_3             f=32 quantized_bits(4,0,1,alpha=1.0) \n",
      "bn_3                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_3                quantized_relu(3,1)\n",
      "conv2d_4             f=64 ternary(alpha='auto_po2') \n",
      "bn_4                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_4                quantized_relu_po2(4,4)\n",
      "dense                u=10 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,3,1) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7td4spyh_1/model_block_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7td4spyh_1/model_block_5/assets\n"
     ]
    }
   ],
   "source": [
    "autoqk = AutoQKerasScheduler(model, metrics=[\"acc\"], custom_objects=custom_objects, **run_config)\n",
    "autoqk.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=1024, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "height": 291
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1591843962540,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "ArdGbsXFDK-I",
    "outputId": "43730cd5-93fc-4838-c49a-1f3f4151fa54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=11506 reference_size=574175\n",
      "       delta=45.13%\n",
      "Total Cost Reduction:\n",
      "       11506 vs 574175 (-98.00%)\n",
      "conv2d_0             f=8 binary(alpha='auto_po2') \n",
      "bn_0                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_0                quantized_relu(4,2)\n",
      "conv2d_1             f=16 binary(alpha='auto_po2') \n",
      "bn_1                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_1                quantized_relu(3,1)\n",
      "conv2d_2             f=24 quantized_bits(4,0,1,alpha=1.0) \n",
      "bn_2                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_2                quantized_relu_po2(4,4)\n",
      "conv2d_3             f=32 quantized_bits(4,0,1,alpha=1.0) \n",
      "bn_3                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_3                quantized_relu(3,1)\n",
      "conv2d_4             f=64 ternary(alpha='auto_po2') \n",
      "bn_4                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_4                quantized_relu_po2(4,4)\n",
      "dense                u=10 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,3,1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel = autoqk.get_best_model()\n",
    "qmodel.save_weights(\"qmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 69779,
     "status": "ok",
     "timestamp": 1591844032332,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "RHGb6YHFEgtV",
    "outputId": "5578ce49-1ee9-4063-deab-1b3db9f4b66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "15/15 [==============================] - 5s 158ms/step - loss: 1.3593 - acc: 0.5442 - val_loss: 2.8060 - val_acc: 0.4612\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.6045 - acc: 0.8052 - val_loss: 2.0059 - val_acc: 0.6075\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.4102 - acc: 0.8723 - val_loss: 1.0139 - val_acc: 0.7363\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.3345 - acc: 0.8951 - val_loss: 0.3237 - val_acc: 0.9072\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.2875 - acc: 0.9101 - val_loss: 0.1854 - val_acc: 0.9455\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.2643 - acc: 0.9179 - val_loss: 0.1530 - val_acc: 0.9547\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.2408 - acc: 0.9247 - val_loss: 0.1298 - val_acc: 0.9598\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.2356 - acc: 0.9272 - val_loss: 0.1254 - val_acc: 0.9604\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.2288 - acc: 0.9286 - val_loss: 0.1057 - val_acc: 0.9688\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.2184 - acc: 0.9317 - val_loss: 0.0924 - val_acc: 0.9702\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.2049 - acc: 0.9359 - val_loss: 0.1151 - val_acc: 0.9660\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.2027 - acc: 0.9370 - val_loss: 0.0893 - val_acc: 0.9738\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1987 - acc: 0.9384 - val_loss: 0.0844 - val_acc: 0.9736\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1917 - acc: 0.9416 - val_loss: 0.0917 - val_acc: 0.9713\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1880 - acc: 0.9425 - val_loss: 0.0760 - val_acc: 0.9740\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1789 - acc: 0.9450 - val_loss: 0.0941 - val_acc: 0.9714\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1799 - acc: 0.9432 - val_loss: 0.0783 - val_acc: 0.9768\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1762 - acc: 0.9459 - val_loss: 0.0856 - val_acc: 0.9720\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1725 - acc: 0.9470 - val_loss: 0.0734 - val_acc: 0.9773\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1696 - acc: 0.9467 - val_loss: 0.0680 - val_acc: 0.9770\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1738 - acc: 0.9460 - val_loss: 0.0692 - val_acc: 0.9758\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1735 - acc: 0.9466 - val_loss: 0.0720 - val_acc: 0.9773\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1718 - acc: 0.9479 - val_loss: 0.0779 - val_acc: 0.9749\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1685 - acc: 0.9466 - val_loss: 0.0717 - val_acc: 0.9758\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1680 - acc: 0.9481 - val_loss: 0.0798 - val_acc: 0.9751\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1634 - acc: 0.9493 - val_loss: 0.0848 - val_acc: 0.9753\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1622 - acc: 0.9492 - val_loss: 0.0610 - val_acc: 0.9788\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1592 - acc: 0.9516 - val_loss: 0.0706 - val_acc: 0.9769\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1624 - acc: 0.9495 - val_loss: 0.0712 - val_acc: 0.9789\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1557 - acc: 0.9517 - val_loss: 0.0678 - val_acc: 0.9774\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1633 - acc: 0.9498 - val_loss: 0.0848 - val_acc: 0.9720\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1620 - acc: 0.9495 - val_loss: 0.0708 - val_acc: 0.9770\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1668 - acc: 0.9482 - val_loss: 0.0764 - val_acc: 0.9768\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1565 - acc: 0.9509 - val_loss: 0.0705 - val_acc: 0.9776\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1553 - acc: 0.9516 - val_loss: 0.0638 - val_acc: 0.9798\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1562 - acc: 0.9515 - val_loss: 0.0744 - val_acc: 0.9767\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1562 - acc: 0.9513 - val_loss: 0.0645 - val_acc: 0.9786\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1545 - acc: 0.9517 - val_loss: 0.0626 - val_acc: 0.9801\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1517 - acc: 0.9535 - val_loss: 0.0667 - val_acc: 0.9786\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1607 - acc: 0.9507 - val_loss: 0.0738 - val_acc: 0.9771\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1542 - acc: 0.9525 - val_loss: 0.0689 - val_acc: 0.9786\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1540 - acc: 0.9514 - val_loss: 0.0673 - val_acc: 0.9789\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1557 - acc: 0.9522 - val_loss: 0.0643 - val_acc: 0.9801\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1528 - acc: 0.9528 - val_loss: 0.0727 - val_acc: 0.9776\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1563 - acc: 0.9518 - val_loss: 0.0746 - val_acc: 0.9768\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1497 - acc: 0.9534 - val_loss: 0.0628 - val_acc: 0.9799\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1509 - acc: 0.9532 - val_loss: 0.0671 - val_acc: 0.9784\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1528 - acc: 0.9526 - val_loss: 0.0664 - val_acc: 0.9793\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1551 - acc: 0.9518 - val_loss: 0.0590 - val_acc: 0.9815\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1473 - acc: 0.9542 - val_loss: 0.0630 - val_acc: 0.9788\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1489 - acc: 0.9536 - val_loss: 0.0636 - val_acc: 0.9792\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1586 - acc: 0.9503 - val_loss: 0.0627 - val_acc: 0.9793\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1528 - acc: 0.9532 - val_loss: 0.0646 - val_acc: 0.9795\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1508 - acc: 0.9535 - val_loss: 0.0615 - val_acc: 0.9800\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1529 - acc: 0.9530 - val_loss: 0.0707 - val_acc: 0.9786\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.1552 - acc: 0.9520 - val_loss: 0.0595 - val_acc: 0.9797\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1539 - acc: 0.9521 - val_loss: 0.0718 - val_acc: 0.9776\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1501 - acc: 0.9531 - val_loss: 0.0752 - val_acc: 0.9747\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1515 - acc: 0.9532 - val_loss: 0.0649 - val_acc: 0.9784\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1472 - acc: 0.9536 - val_loss: 0.0610 - val_acc: 0.9812\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1506 - acc: 0.9541 - val_loss: 0.0599 - val_acc: 0.9812\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1494 - acc: 0.9531 - val_loss: 0.0650 - val_acc: 0.9808\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1439 - acc: 0.9566 - val_loss: 0.0651 - val_acc: 0.9793\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1484 - acc: 0.9539 - val_loss: 0.0625 - val_acc: 0.9809\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.1491 - acc: 0.9534 - val_loss: 0.0605 - val_acc: 0.9796\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1476 - acc: 0.9544 - val_loss: 0.0841 - val_acc: 0.9743\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1466 - acc: 0.9546 - val_loss: 0.0568 - val_acc: 0.9827\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1492 - acc: 0.9531 - val_loss: 0.0656 - val_acc: 0.9787\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1444 - acc: 0.9547 - val_loss: 0.0654 - val_acc: 0.9801\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1394 - acc: 0.9567 - val_loss: 0.0619 - val_acc: 0.9787\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1449 - acc: 0.9552 - val_loss: 0.0592 - val_acc: 0.9824\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1464 - acc: 0.9546 - val_loss: 0.0577 - val_acc: 0.9821\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1452 - acc: 0.9550 - val_loss: 0.0636 - val_acc: 0.9804\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1451 - acc: 0.9550 - val_loss: 0.0637 - val_acc: 0.9798\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1478 - acc: 0.9534 - val_loss: 0.0792 - val_acc: 0.9761\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1473 - acc: 0.9542 - val_loss: 0.0593 - val_acc: 0.9818\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1458 - acc: 0.9551 - val_loss: 0.0601 - val_acc: 0.9801\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1450 - acc: 0.9546 - val_loss: 0.0571 - val_acc: 0.9806\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1457 - acc: 0.9535 - val_loss: 0.0616 - val_acc: 0.9808\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1485 - acc: 0.9534 - val_loss: 0.0619 - val_acc: 0.9813\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1479 - acc: 0.9533 - val_loss: 0.0615 - val_acc: 0.9797\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1456 - acc: 0.9547 - val_loss: 0.0643 - val_acc: 0.9784\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1492 - acc: 0.9532 - val_loss: 0.0631 - val_acc: 0.9804\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1436 - acc: 0.9547 - val_loss: 0.0625 - val_acc: 0.9804\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1439 - acc: 0.9561 - val_loss: 0.0580 - val_acc: 0.9812\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1459 - acc: 0.9544 - val_loss: 0.0668 - val_acc: 0.9775\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1465 - acc: 0.9543 - val_loss: 0.0648 - val_acc: 0.9803\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1426 - acc: 0.9560 - val_loss: 0.0583 - val_acc: 0.9821\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1412 - acc: 0.9566 - val_loss: 0.0634 - val_acc: 0.9807\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1444 - acc: 0.9552 - val_loss: 0.0598 - val_acc: 0.9810\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1436 - acc: 0.9560 - val_loss: 0.0614 - val_acc: 0.9798\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1456 - acc: 0.9543 - val_loss: 0.0614 - val_acc: 0.9796\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1437 - acc: 0.9553 - val_loss: 0.0601 - val_acc: 0.9802\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1430 - acc: 0.9558 - val_loss: 0.0619 - val_acc: 0.9797\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1442 - acc: 0.9559 - val_loss: 0.0620 - val_acc: 0.9797\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1412 - acc: 0.9565 - val_loss: 0.0513 - val_acc: 0.9838\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1445 - acc: 0.9551 - val_loss: 0.0564 - val_acc: 0.9822\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1399 - acc: 0.9564 - val_loss: 0.0970 - val_acc: 0.9708\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1419 - acc: 0.9561 - val_loss: 0.0549 - val_acc: 0.9834\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1351 - acc: 0.9582 - val_loss: 0.0627 - val_acc: 0.9799\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1397 - acc: 0.9564 - val_loss: 0.0704 - val_acc: 0.9779\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1426 - acc: 0.9557 - val_loss: 0.0546 - val_acc: 0.9832\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1437 - acc: 0.9557 - val_loss: 0.0596 - val_acc: 0.9822\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1429 - acc: 0.9557 - val_loss: 0.0555 - val_acc: 0.9823\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1457 - acc: 0.9546 - val_loss: 0.0589 - val_acc: 0.9824\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1438 - acc: 0.9564 - val_loss: 0.0540 - val_acc: 0.9817\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1435 - acc: 0.9544 - val_loss: 0.0604 - val_acc: 0.9801\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1457 - acc: 0.9550 - val_loss: 0.0618 - val_acc: 0.9796\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1406 - acc: 0.9562 - val_loss: 0.0603 - val_acc: 0.9810\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1386 - acc: 0.9566 - val_loss: 0.0530 - val_acc: 0.9833\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1402 - acc: 0.9555 - val_loss: 0.0564 - val_acc: 0.9817\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1445 - acc: 0.9556 - val_loss: 0.0552 - val_acc: 0.9827\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1418 - acc: 0.9563 - val_loss: 0.0577 - val_acc: 0.9824\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1447 - acc: 0.9548 - val_loss: 0.0552 - val_acc: 0.9820\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1431 - acc: 0.9551 - val_loss: 0.0607 - val_acc: 0.9805\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1412 - acc: 0.9560 - val_loss: 0.0655 - val_acc: 0.9803\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1385 - acc: 0.9567 - val_loss: 0.0541 - val_acc: 0.9823\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1424 - acc: 0.9564 - val_loss: 0.0604 - val_acc: 0.9804\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1353 - acc: 0.9575 - val_loss: 0.0553 - val_acc: 0.9831\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1359 - acc: 0.9568 - val_loss: 0.0539 - val_acc: 0.9837\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1357 - acc: 0.9577 - val_loss: 0.0587 - val_acc: 0.9817\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1415 - acc: 0.9553 - val_loss: 0.0728 - val_acc: 0.9768\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1386 - acc: 0.9571 - val_loss: 0.0608 - val_acc: 0.9805\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1413 - acc: 0.9556 - val_loss: 0.0625 - val_acc: 0.9806\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1413 - acc: 0.9567 - val_loss: 0.0584 - val_acc: 0.9811\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1438 - acc: 0.9551 - val_loss: 0.0566 - val_acc: 0.9803\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1399 - acc: 0.9554 - val_loss: 0.0538 - val_acc: 0.9829\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1382 - acc: 0.9567 - val_loss: 0.0536 - val_acc: 0.9824\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1377 - acc: 0.9568 - val_loss: 0.0555 - val_acc: 0.9817\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1387 - acc: 0.9576 - val_loss: 0.0686 - val_acc: 0.9801\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1387 - acc: 0.9568 - val_loss: 0.0509 - val_acc: 0.9822\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1378 - acc: 0.9573 - val_loss: 0.0599 - val_acc: 0.9806\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1436 - acc: 0.9556 - val_loss: 0.0586 - val_acc: 0.9810\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1390 - acc: 0.9570 - val_loss: 0.0534 - val_acc: 0.9827\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1363 - acc: 0.9579 - val_loss: 0.0520 - val_acc: 0.9835\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1366 - acc: 0.9571 - val_loss: 0.0597 - val_acc: 0.9810\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1435 - acc: 0.9558 - val_loss: 0.0566 - val_acc: 0.9807\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1419 - acc: 0.9559 - val_loss: 0.0588 - val_acc: 0.9818\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1409 - acc: 0.9559 - val_loss: 0.0611 - val_acc: 0.9801\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1434 - acc: 0.9561 - val_loss: 0.0613 - val_acc: 0.9790\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1431 - acc: 0.9551 - val_loss: 0.0557 - val_acc: 0.9829\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1426 - acc: 0.9557 - val_loss: 0.0593 - val_acc: 0.9828\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1344 - acc: 0.9585 - val_loss: 0.0612 - val_acc: 0.9799\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1406 - acc: 0.9560 - val_loss: 0.0620 - val_acc: 0.9798\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1423 - acc: 0.9563 - val_loss: 0.0600 - val_acc: 0.9803\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1419 - acc: 0.9558 - val_loss: 0.0617 - val_acc: 0.9801\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1421 - acc: 0.9560 - val_loss: 0.0572 - val_acc: 0.9823\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.1431 - acc: 0.9546 - val_loss: 0.0679 - val_acc: 0.9783\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1361 - acc: 0.9575 - val_loss: 0.0560 - val_acc: 0.9818\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1424 - acc: 0.9563 - val_loss: 0.0590 - val_acc: 0.9819\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1406 - acc: 0.9555 - val_loss: 0.0535 - val_acc: 0.9827\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1385 - acc: 0.9568 - val_loss: 0.0570 - val_acc: 0.9818\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1369 - acc: 0.9573 - val_loss: 0.0586 - val_acc: 0.9806\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1400 - acc: 0.9557 - val_loss: 0.0511 - val_acc: 0.9836\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1426 - acc: 0.9560 - val_loss: 0.0677 - val_acc: 0.9777\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1408 - acc: 0.9562 - val_loss: 0.0639 - val_acc: 0.9811\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1398 - acc: 0.9570 - val_loss: 0.0568 - val_acc: 0.9813\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1404 - acc: 0.9565 - val_loss: 0.0722 - val_acc: 0.9766\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1360 - acc: 0.9572 - val_loss: 0.0541 - val_acc: 0.9818\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1422 - acc: 0.9564 - val_loss: 0.0606 - val_acc: 0.9807\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1353 - acc: 0.9582 - val_loss: 0.0524 - val_acc: 0.9834\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1385 - acc: 0.9577 - val_loss: 0.0585 - val_acc: 0.9817\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1375 - acc: 0.9574 - val_loss: 0.0571 - val_acc: 0.9818\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1420 - acc: 0.9561 - val_loss: 0.0633 - val_acc: 0.9799\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1400 - acc: 0.9566 - val_loss: 0.0750 - val_acc: 0.9762\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1357 - acc: 0.9571 - val_loss: 0.0563 - val_acc: 0.9823\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1383 - acc: 0.9571 - val_loss: 0.0624 - val_acc: 0.9801\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1410 - acc: 0.9554 - val_loss: 0.0521 - val_acc: 0.9829\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1360 - acc: 0.9578 - val_loss: 0.0613 - val_acc: 0.9806\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1376 - acc: 0.9575 - val_loss: 0.0553 - val_acc: 0.9825\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1436 - acc: 0.9557 - val_loss: 0.0637 - val_acc: 0.9804\n",
      "Epoch 172/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1397 - acc: 0.9572 - val_loss: 0.0576 - val_acc: 0.9806\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1447 - acc: 0.9549 - val_loss: 0.0533 - val_acc: 0.9828\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1391 - acc: 0.9571 - val_loss: 0.0530 - val_acc: 0.9818\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1382 - acc: 0.9572 - val_loss: 0.0637 - val_acc: 0.9795\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1464 - acc: 0.9543 - val_loss: 0.0514 - val_acc: 0.9826\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1419 - acc: 0.9557 - val_loss: 0.0577 - val_acc: 0.9812\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1410 - acc: 0.9562 - val_loss: 0.0561 - val_acc: 0.9817\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1377 - acc: 0.9565 - val_loss: 0.0557 - val_acc: 0.9818\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1399 - acc: 0.9568 - val_loss: 0.0678 - val_acc: 0.9786\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1403 - acc: 0.9561 - val_loss: 0.1045 - val_acc: 0.9689\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1383 - acc: 0.9571 - val_loss: 0.0578 - val_acc: 0.9820\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1365 - acc: 0.9575 - val_loss: 0.0533 - val_acc: 0.9825\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1404 - acc: 0.9574 - val_loss: 0.0507 - val_acc: 0.9826\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1397 - acc: 0.9574 - val_loss: 0.0538 - val_acc: 0.9828\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1420 - acc: 0.9557 - val_loss: 0.0531 - val_acc: 0.9828\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1401 - acc: 0.9556 - val_loss: 0.0598 - val_acc: 0.9810\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1364 - acc: 0.9573 - val_loss: 0.0529 - val_acc: 0.9840\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1420 - acc: 0.9557 - val_loss: 0.0550 - val_acc: 0.9841\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1429 - acc: 0.9552 - val_loss: 0.0507 - val_acc: 0.9838\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1331 - acc: 0.9584 - val_loss: 0.0567 - val_acc: 0.9818\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1395 - acc: 0.9578 - val_loss: 0.0589 - val_acc: 0.9820\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1447 - acc: 0.9548 - val_loss: 0.0610 - val_acc: 0.9803\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.1398 - acc: 0.9562 - val_loss: 0.0544 - val_acc: 0.9819\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1390 - acc: 0.9570 - val_loss: 0.0528 - val_acc: 0.9822\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.1397 - acc: 0.9563 - val_loss: 0.0575 - val_acc: 0.9818\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 2s 115ms/step - loss: 0.1380 - acc: 0.9566 - val_loss: 0.0561 - val_acc: 0.9828\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 0.1380 - acc: 0.9574 - val_loss: 0.0574 - val_acc: 0.9823\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.1339 - acc: 0.9588 - val_loss: 0.0587 - val_acc: 0.9805\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.1373 - acc: 0.9570 - val_loss: 0.0586 - val_acc: 0.9804\n"
     ]
    }
   ],
   "source": [
    "qmodel.load_weights(\"qmodel.h5\")\n",
    "with cur_strategy.scope():\n",
    "  optimizer = Adam(lr=0.02)\n",
    "  qmodel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "  qmodel.fit(x_train, y_train, epochs=200, batch_size=4096, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJCkMdAcjnoh"
   },
   "source": [
    "Perfect! You have learned how to perform automatic quantization using AutoQKeras with QKeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/dm_python:dm_notebook3_tpu",
    "kind": "private"
   },
   "name": "AutoQKeras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('hls4ml-tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "978a78fee93b9f75d300423e922c5a4da2d32993b15c09db9f940a22d4b78528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
