{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QC9sVuNrzT-f"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we show how to quantize a model using AutoQKeras.\n",
    "\n",
    "As usual, let's first make sure we are using Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1591840345558,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "0sY-O2IfzdB3",
    "outputId": "1c5a4e7a-1003-4b56-a30a-ca6bc196f18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6V7FxYH0zfY0"
   },
   "source": [
    "Now, let's load some packages we will need to run AutoQKeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuVqOAcbz3Go"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 11:06:26.303619: I tensorflow/core/util/util.cc:168] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-11 11:06:26.309809: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-11 11:06:26.309826: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using tensorflow 2.10.0-dev20220407\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import six\n",
    "import tempfile\n",
    "import tensorflow.compat.v2 as tf\n",
    "# V2 Behavior is necessary to use TF2 APIs before TF2 is default TF version internally.\n",
    "tf.enable_v2_behavior()\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from qkeras.autoqkeras import *\n",
    "from qkeras import *\n",
    "from qkeras.utils import model_quantize\n",
    "from qkeras.qtools import run_qtools\n",
    "from qkeras.qtools import settings as qtools_settings\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"using tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define `get_data` and `get_model` as you may not have stand alone access to examples directory inside autoqkeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name convert optimizers ['fuse_bias_add', 'remove_useless_transpose', 'output_rounding_saturation_mode', 'qkeras_factorize_alpha', 'extract_ternary_threshold', 'fuse_consecutive_batch_normalization'] File: flow.py Line: 23\n",
      "name optimize optimizers ['eliminate_linear_activation', 'fuse_consecutive_batch_normalization', 'fuse_batch_normalization', 'replace_multidimensional_dense_with_conv'] File: flow.py Line: 23\n",
      "vivado:merge_batch_norm_quantized_tanh Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:quantize_dense_output Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:batchnormalizationquantizedtanh_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:batchnormalizationquantizedtanh_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:clone_output Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:clone_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:optimize_pointwise_conv Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pointwiseconv1d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pointwiseconv1d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pointwiseconv2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pointwiseconv2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:remove_final_reshape Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:reshape_stream Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:broadcast_stream Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:repack_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:broadcast_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:broadcast_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_activation Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_activation Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_base_layer Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_base_layer Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_conv1d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_conv1d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_conv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_conv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_dense Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_dense Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_depconv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_depconv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_garnet Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_garnet Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_garnet_stack Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_garnet_stack Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_sepconv1d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_sepconv1d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_sepconv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_sepconv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_softmax Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_softmax Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:write_hls Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:register_bram_weights Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:insert_zero_padding_before_conv1d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:insert_zero_padding_before_conv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:generate_conv_streaming_instructions Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:conv1d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:conv1d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:conv2d_conv2dbatchnorm_depthwiseconv2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:conv2d_conv2dbatchnorm_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:depthwiseconv2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:separableconv1d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:separableconv1d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:separableconv2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:separableconv2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:dense_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:dense_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:batchnormalization_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:batchnormalization_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:activation_parametrizedactivation_prelu_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:softmax_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:activation_softmax_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:parametrizedactivation_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:prelu_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:garnet_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:garnet_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:garnetstack_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:garnetstack_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:merge_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:merge_concatenate_dot_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:dot_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:concatenate_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pooling1d_pooling2d_globalpooling1d_globalpooling2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pooling1d_pooling2d_globalpooling1d_globalpooling2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:applyalpha_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:applyalpha_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:zeropadding1d_zeropadding2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:zeropadding1d_zeropadding2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:resize_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:resize_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:transpose_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:transpose_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:apply_resource_strategy Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:transform_types Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "name init_layers optimizers ['vivado:init_base_layer', 'vivado:init_activation', 'vivado:init_conv1d', 'vivado:init_conv2d', 'vivado:init_dense', 'vivado:init_garnet', 'vivado:init_sepconv1d', 'vivado:init_sepconv2d', 'vivado:init_depconv2d', 'vivado:init_garnet_stack', 'vivado:init_softmax'] File: flow.py Line: 23\n",
      "name streaming optimizers ['vivado:remove_final_reshape', 'vivado:reshape_stream', 'vivado:clone_output', 'vivado:insert_zero_padding_before_conv1d', 'vivado:insert_zero_padding_before_conv2d', 'vivado:broadcast_stream'] File: flow.py Line: 23\n",
      "name quantization optimizers ['vivado:merge_batch_norm_quantized_tanh', 'vivado:quantize_dense_output', 'fuse_consecutive_batch_normalization'] File: flow.py Line: 23\n",
      "name optimize optimizers ['vivado:optimize_pointwise_conv'] File: flow.py Line: 23\n",
      "name specific_types optimizers ['vivado:register_bram_weights', 'vivado:transform_types', 'vivado:generate_conv_streaming_instructions', 'vivado:apply_resource_strategy'] File: flow.py Line: 23\n",
      "vivado:merge_batch_norm_quantized_tanh Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:quantize_dense_output Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:batchnormalizationquantizedtanh_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:batchnormalizationquantizedtanh_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:clone_output Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:clone_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:optimize_pointwise_conv Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pointwiseconv1d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pointwiseconv1d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pointwiseconv2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pointwiseconv2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:remove_final_reshape Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:reshape_stream Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:broadcast_stream Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:repack_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:broadcast_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:broadcast_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_activation Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_base_layer Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_conv1d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_conv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_dense Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_depconv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_garnet Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_garnet_stack Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_sepconv1d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_sepconv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:init_softmax Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:write_hls Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:register_bram_weights Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:insert_zero_padding_before_conv1d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:insert_zero_padding_before_conv2d Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:generate_conv_streaming_instructions Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:conv1d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:conv1d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:conv2d_conv2dbatchnorm_depthwiseconv2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:conv2d_conv2dbatchnorm_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:depthwiseconv2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:separableconv1d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:separableconv1d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:separableconv2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:separableconv2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:dense_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:dense_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:batchnormalization_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:batchnormalization_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:activation_parametrizedactivation_prelu_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:softmax_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:activation_softmax_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:parametrizedactivation_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:prelu_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:garnet_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:garnet_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:garnetstack_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:garnetstack_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:merge_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:merge_concatenate_dot_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:dot_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:concatenate_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pooling1d_pooling2d_globalpooling1d_globalpooling2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:pooling1d_pooling2d_globalpooling1d_globalpooling2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:applyalpha_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:applyalpha_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:zeropadding1d_zeropadding2d_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:zeropadding1d_zeropadding2d_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:resize_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:resize_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:transpose_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:transpose_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:apply_resource_strategy Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "vivado:transform_types Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "name apply_templates optimizers ['vivado:batchnormalizationquantizedtanh_config_template', 'vivado:batchnormalizationquantizedtanh_function_template', 'vivado:clone_function_template', 'vivado:pointwiseconv1d_config_template', 'vivado:pointwiseconv1d_function_template', 'vivado:pointwiseconv2d_config_template', 'vivado:pointwiseconv2d_function_template', 'vivado:repack_function_template', 'vivado:broadcast_config_template', 'vivado:broadcast_function_template', 'vivado:conv1d_config_template', 'vivado:conv1d_function_template', 'vivado:conv2d_conv2dbatchnorm_depthwiseconv2d_config_template', 'vivado:conv2d_conv2dbatchnorm_function_template', 'vivado:depthwiseconv2d_function_template', 'vivado:separableconv1d_config_template', 'vivado:separableconv1d_function_template', 'vivado:separableconv2d_config_template', 'vivado:separableconv2d_function_template', 'vivado:dense_config_template', 'vivado:dense_function_template', 'vivado:batchnormalization_config_template', 'vivado:batchnormalization_function_template', 'vivado:activation_parametrizedactivation_prelu_config_template', 'vivado:softmax_config_template', 'vivado:activation_softmax_function_template', 'vivado:parametrizedactivation_function_template', 'vivado:prelu_function_template', 'vivado:garnet_config_template', 'vivado:garnet_function_template', 'vivado:garnetstack_config_template', 'vivado:garnetstack_function_template', 'vivado:merge_config_template', 'vivado:merge_concatenate_dot_function_template', 'vivado:dot_config_template', 'vivado:concatenate_config_template', 'vivado:pooling1d_pooling2d_globalpooling1d_globalpooling2d_config_template', 'vivado:pooling1d_pooling2d_globalpooling1d_globalpooling2d_function_template', 'vivado:applyalpha_config_template', 'vivado:applyalpha_function_template', 'vivado:zeropadding1d_zeropadding2d_config_template', 'vivado:zeropadding1d_zeropadding2d_function_template', 'vivado:resize_config_template', 'vivado:resize_function_template', 'vivado:transpose_config_template', 'vivado:transpose_function_template'] File: flow.py Line: 23\n",
      "name write optimizers ['vivado:write_hls'] File: flow.py Line: 23\n",
      "name ip optimizers None File: flow.py Line: 23\n",
      "name write optimizers ['vivadoaccelerator:write_hls'] File: flow.py Line: 23\n",
      "quartus:init_activation Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_activation Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_base_layer Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_base_layer Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_dense Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_dense Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_softmax Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_softmax Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:write_hls Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:lstm_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:lstm_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:simplernn_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:simplernn_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:dense_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:dense_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:batchnormalization_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:batchnormalization_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:activation_parametrizedactivation_prelu_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:softmax_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:activation_softmax_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:parametrizedactivation_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:prelu_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:transform_types Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "name init_layers optimizers ['quartus:init_base_layer', 'quartus:init_activation', 'quartus:init_dense', 'quartus:init_softmax'] File: flow.py Line: 23\n",
      "name specific_types optimizers ['quartus:transform_types'] File: flow.py Line: 23\n",
      "quartus:init_activation Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_base_layer Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_dense Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:init_softmax Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:write_hls Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:lstm_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:lstm_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:simplernn_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:simplernn_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:dense_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:dense_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:batchnormalization_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:batchnormalization_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:activation_parametrizedactivation_prelu_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:softmax_config_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:activation_softmax_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:parametrizedactivation_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:prelu_function_template Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "quartus:transform_types Get_Optimizer, optimizer/optimizer.py ligne: 168\n",
      "name apply_templates optimizers ['quartus:lstm_config_template', 'quartus:lstm_function_template', 'quartus:simplernn_config_template', 'quartus:simplernn_function_template', 'quartus:dense_config_template', 'quartus:dense_function_template', 'quartus:batchnormalization_config_template', 'quartus:batchnormalization_function_template', 'quartus:activation_parametrizedactivation_prelu_config_template', 'quartus:softmax_config_template', 'quartus:activation_softmax_function_template', 'quartus:parametrizedactivation_function_template', 'quartus:prelu_function_template'] File: flow.py Line: 23\n",
      "name write optimizers ['quartus:write_hls'] File: flow.py Line: 23\n",
      "name ip optimizers None File: flow.py Line: 23\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, LSTM, GRU, SimpleRNN, Conv2D, MaxPooling2D, Flatten, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from keras.regularizers import l2, l1, l1_l2\n",
    "from collections import deque\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import qkeras\n",
    "from qkeras import *\n",
    "\n",
    "import hls4ml\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 11:06:32.538147: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-11 11:06:32.538199: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-11 11:06:32.538237: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martop): /proc/driver/nvidia/version does not exist\n",
      "2022-07-11 11:06:32.538831: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 8)                 80        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import *\n",
    "boosted_model =  tf.keras.models.load_model('../pb_file')\n",
    "\n",
    "boosted_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXsGtqRcN7fY"
   },
   "source": [
    "`AutoQKeras` has some examples on how to run with `mnist`, `fashion_mnist`, `cifar10` and `cifar100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes (1999995, 5, 1) (1999995, 1)\n",
      "shapes (899992, 5, 1) (99995, 5, 1) (999998, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "from nnlar.datashaper import DataShaper\n",
    "ds = DataShaper.from_h5(\"../data/rdgap_mu140.h5\")\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bk4rOks2OIbW"
   },
   "source": [
    "Before we create the model, let's see if we can perform distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1591840378251,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "EMbYcKb-wMOc",
    "outputId": "22e85769-4659-4212-ccdb-4b00be2fcefe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "for d in physical_devices:\n",
    "  print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14553,
     "status": "ok",
     "timestamp": 1591840392823,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "eMVill0TxUuG",
    "outputId": "97c07213-fdce-4eed-9af7-cc51393cd996"
   },
   "outputs": [],
   "source": [
    "has_tpus = np.any([d.device_type == \"TPU\" for d in physical_devices])\n",
    "\n",
    "if has_tpus:\n",
    "  TPU_WORKER = 'local'\n",
    "\n",
    "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "      tpu=TPU_WORKER, job_name='tpu_worker')\n",
    "  if TPU_WORKER != 'local':\n",
    "    tf.config.experimental_connect_to_cluster(resolver, protocol='grpc+loas')\n",
    "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "  print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "  cur_strategy = strategy\n",
    "else:\n",
    "  cur_strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FIAmXgOOPWg"
   },
   "source": [
    "Now we can create the model with the distributed strategy in place if TPUs are available. We have some test models that we can use, or you can build your own models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_model (units_parameter):\n",
    "\n",
    "    \n",
    "    r_model = Sequential()\n",
    "    r_model.add(SimpleRNN(units_parameter, activation='relu', input_shape=(5, 1), return_sequences=False, name='SimpleRNN'))\n",
    "    r_model.add(Dense(1, activation='relu',name='dense'))\n",
    "    r_model.summary()\n",
    "    return r_model\n",
    "\n",
    "refmodels_path=\"/atlas/bonnet/Desktop/code/internship_CPPM/autoqkeras_test/tests/models/optimized_model.h5\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "height": 977
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1591840393983,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "P0_-26kwxZiQ",
    "outputId": "bf2828fe-2968-4d7d-82e7-0e2b87f063ae"
   },
   "outputs": [],
   "source": [
    "with cur_strategy.scope():\n",
    "  model =  tf.keras.models.load_model(refmodels_path)\n",
    "  custom_objects = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jok7tJq1OVuJ"
   },
   "source": [
    "Let's see the accuracy on a unquantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience_es=7\n",
    "patience_rlr=5\n",
    "delta = 0.00000001\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                    patience=patience_es, \n",
    "                                                    restore_best_weights=True, \n",
    "                                                    min_delta=delta,\n",
    "                                                    mode='min')\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                patience= patience_rlr, min_lr=0.000001, min_delta=delta, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "height": 360
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10292,
     "status": "ok",
     "timestamp": 1591840404285,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "nvFSJpeDxmWZ",
    "outputId": "ceac171d-2357-4d2a-ecbe-6c2775bc2a94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith cur_strategy.scope():\\n    optimizer = Adam(0.01)\\n    model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\\n    model.set_weights(model.get_weights())\\n    hist = model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs=1, batch_size=64, shuffle=True, callbacks=[early_stopping, reduce_lr])\\n    lr_change = []\\n    for i in range (len(hist.history[\\'lr\\'])-1):\\n    \\n        if (hist.history[\\'lr\\'][i]==hist.history[\\'lr\\'][i+1]):\\n            lr_change.append(None)\\n        else: \\n            lr_change.append(hist.history[\\'val_loss\\'][i+1])\\n    plt.plot(lr_change, \\'X\\')\\n    plt.plot(hist.history[\\'loss\\'])\\n    plt.plot(hist.history[\\'val_loss\\'])\\n    plt.ylabel(\\'loss\\')\\n    plt.xlabel(\\'epoch\\')\\n    plt.legend([\\'lr_changed\\',\\'train\\', \\'test\\'])\\n    plt.show()\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with cur_strategy.scope():\n",
    "    optimizer = Adam(0.01)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
    "    model.set_weights(model.get_weights())\n",
    "    hist = model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs=1, batch_size=64, shuffle=True, callbacks=[early_stopping, reduce_lr])\n",
    "    lr_change = []\n",
    "    for i in range (len(hist.history['lr'])-1):\n",
    "    \n",
    "        if (hist.history['lr'][i]==hist.history['lr'][i+1]):\n",
    "            lr_change.append(None)\n",
    "        else: \n",
    "            lr_change.append(hist.history['val_loss'][i+1])\n",
    "    plt.plot(lr_change, 'X')\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['lr_changed','train', 'test'])\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKArZ2VwQlph"
   },
   "source": [
    "For `mnist`, we should get 99% validation accuracy, and for `fashion_mnist`, we should get around 86% of validation accuracy. Let's get a metric for high-level estimation of energy of this model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1591840404708,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "AlIk3gtFS6iJ",
    "outputId": "780a9c28-6234-49ff-9a85-e52bf00a5c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SimpleRNN': {'energy': {'inputs': 3.8,\n",
      "                          'op_cost': 0.0,\n",
      "                          'outputs': 3.8,\n",
      "                          'parameters': 0.0},\n",
      "               'total': 3.8},\n",
      " 'dense': {'energy': {'inputs': 3.8,\n",
      "                      'op_cost': 36.8,\n",
      "                      'outputs': 3.8,\n",
      "                      'parameters': 19.02},\n",
      "           'total': 59.62}}\n",
      "\n",
      "Total energy: 0.00 uJ\n"
     ]
    }
   ],
   "source": [
    "  reference_internal = \"fp32\"\n",
    "  reference_accumulator = \"fp32\"\n",
    "\n",
    "  q = run_qtools.QTools(\n",
    "      model,\n",
    "      # energy calculation using a given process\n",
    "      # \"horowitz\" refers to 45nm process published at\n",
    "      # M. Horowitz, \"1.1 Computing's energy problem (and what we can do about\n",
    "      # it), \"2014 IEEE International Solid-State Circuits Conference Digest of\n",
    "      # Technical Papers (ISSCC), San Francisco, CA, 2014, pp. 10-14, \n",
    "      # doi: 10.1109/ISSCC.2014.6757323.\n",
    "      process=\"horowitz\",\n",
    "      # quantizers for model input\n",
    "      source_quantizers=[quantized_bits(8, 0, 1)],\n",
    "      is_inference=False,\n",
    "      # absolute path (including filename) of the model weights\n",
    "      # in the future, we will attempt to optimize the power model\n",
    "      # by using weight information, although it can be used to further\n",
    "      # optimize QBatchNormalization.\n",
    "      weights_path=None,\n",
    "      # keras_quantizer to quantize weight/bias in un-quantized keras layers\n",
    "      keras_quantizer=reference_internal,\n",
    "      # keras_quantizer to quantize MAC in un-quantized keras layers\n",
    "      keras_accumulator=reference_accumulator,\n",
    "      # whether calculate baseline energy\n",
    "      for_reference=True)\n",
    "  \n",
    "# caculate energy of the derived data type map.\n",
    "energy_dict = q.pe(\n",
    "    # whether to store parameters in dram, sram, or fixed\n",
    "    weights_on_memory=\"sram\",\n",
    "    # store activations in dram or sram\n",
    "    activations_on_memory=\"sram\",\n",
    "    # minimum sram size in number of bits. Let's assume a 16MB SRAM.\n",
    "    min_sram_size=8*16*1024*1024,\n",
    "    # whether load data from dram to sram (consider sram as a cache\n",
    "    # for dram. If false, we will assume data will be already in SRAM\n",
    "    rd_wr_on_io=False)\n",
    "\n",
    "# get stats of energy distribution in each layer\n",
    "energy_profile = q.extract_energy_profile(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "# extract sum of energy of each layer according to the rule specified in\n",
    "# qtools_settings.cfg.include_energy\n",
    "total_energy = q.extract_energy_sum(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "\n",
    "pprint.pprint(energy_profile)\n",
    "print()\n",
    "print(\"Total energy: {:.2f} uJ\".format(total_energy / 1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eDXxDSUVJ2m"
   },
   "source": [
    "During the computation, we had a dictionary that outlines the energy per layer (`energy_profile`), and total energy (`total_energy`). The reader should remember that `energy_profile` may need additional filtering as implementations will fuse some\n",
    "layers. When we compute the `total_energy`, we consider an approximation that some layers will be fused to compute the final energy number. For example, a convolution layer followed by an activation layer will be fused into a single layer so that the output of the convolution layer is not used.\n",
    "\n",
    "You have to remember that our high-level model for energy has several assumptions:\n",
    "\n",
    "The energy of a layer is estimated as `energy(layer) = energy(input) + energy(parameters) + energy(MAC) + energy(output)`.\n",
    "\n",
    "1) Reading inputs, parameters and outputs consider only _compulsory_ accesses, i.e. first access to the data, which is independent of the hardware architecture. If you remember _The 3 C's of Caches_ (https://courses.cs.washington.edu/courses/cse410/99au/lectures/Lecture-10-18/tsld035.htm) other types of accesses will depend on the accelerator architecture.\n",
    "\n",
    "2) For the multiply-and-add (MAC) energy estimation, we only consider the energy to compute the MAC, but not any other type energy. For example, in a real accelerator, you have registers, glue logic, pipeline logic that will affect the overall energy profile of the device.\n",
    "\n",
    "Although this model is simple and provides an initial estimate on what to expect, it has high-variance with respect to actual energy numbers you will find in practice, especially with respect to different architectural implementations.\n",
    "\n",
    "We assume that the real energy `Energy(layer)` is a linear combination of the high-level energy model, i.e.`Energy(layer) = k1 * energy(layer) + k2`, where `k1` and `k2` are constants that depend on the architecture of the accelerator. One can think of `k1` as the factor that accounts for the additional storage to keep the model running, and `k2` as the additional always on logic that is required to perform the operations. If we compare the energy of two implementations with different quantizations of the same layer, let's say `layer1` and `layer2`, `Energy(layer1) > Energy(layer2)` holds true iff `energy(layer1) > energy(layer2)` for the same architecture, but for different architectures, this will not be true in general.\n",
    "\n",
    "Despite its limitations to predict a single energy number, this model is quite good to compare the energy of two different models, or different types of quantizations, when we restrict it to a single architecture, and that's how we use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hr1FL8wVSy-q"
   },
   "source": [
    "# Quantizing a Model With `AutoQKeras`\n",
    "\n",
    "To quantize this model with `AutoQKeras`, we need to define the quantization for kernels, biases and activations; forgiving factors and quantization strategy.\n",
    "\n",
    "Below we define which quantizers are allowed for kernel, bias, activations and linear. Linear is a proxy that we use to capture `Activation(\"linear\")` to apply quantization without applying a non-linear operation.  In some networks, we found that this trick may be necessary to better represent the quantization space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSsEwDr_yRG4"
   },
   "outputs": [],
   "source": [
    "    \n",
    "quantization_config = {\n",
    "        \"kernel\": {\n",
    "                \n",
    "                \"quantized_bits(8,0,1,alpha=1.0)\": 8,\n",
    "                \"quantized_bits(10,0,1,alpha=1.0)\": 10,\n",
    "                \"quantized_bits(12,0,1,alpha=1.0)\": 12,\n",
    "                \"quantized_po2(8,1)\": 8\n",
    "        },\n",
    "        \"recurrent_kernel\":{\n",
    "                \"quantized_bits(2,1,1,alpha=1.0)\": 2,\n",
    "                \"quantized_bits(4,0,1,alpha=1.0)\": 4,\n",
    "                \"quantized_bits(6,0,1,alpha=1.0)\": 6,\n",
    "                \"quantized_bits(8,0,1,alpha=1.0)\": 8,\n",
    "                \"quantized_bits(10,0,1,alpha=1.0)\": 10,\n",
    "                \"quantized_bits(12,0,1,alpha=1.0)\": 12},\n",
    "        \"bias\": {\n",
    "                \n",
    "                \"quantized_bits(8,0,1,alpha=1.0)\": 8,\n",
    "                \"quantized_bits(10,0,1,alpha=1.0)\": 10,\n",
    "                \"quantized_bits(12,0,1,alpha=1.0)\": 12,\n",
    "                \"quantized_po2(8,0)\": 8\n",
    "        },\n",
    "        \"activation\": {\n",
    "                \"quantized_relu_po2(8,0)\": 8,\n",
    "                \n",
    "                \"quantized_relu(8,0)\": 8,\n",
    "                \"quantized_relu(10,0)\": 10,\n",
    "                \"quantized_relu(12,0)\": 12,\n",
    "        },\n",
    "        \"linear\": {\n",
    "                 \n",
    "                \"quantized_bits(8,0,1,alpha=1.0)\": 8,\n",
    "                \"quantized_bits(10,0,1,alpha=1.0)\": 10,\n",
    "                \"quantized_bits(12,0,1,alpha=1.0)\": 12\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmW_xaAvZo4D"
   },
   "source": [
    "Now let's define how to apply quantization. In the simplest form, we specify how many bits for kernels, biases and activations by layer types. Note that the entry `BatchNormalization` needs to be specified here, as we only quantize layer types specified by these patterns.  For example, a `Flatten` layer is not quantized as it does not change the data type of its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emTRLIZmR-P7"
   },
   "outputs": [],
   "source": [
    "limit = {\n",
    "    \"SimpleRNN\": [12, 12, 12, 12],\n",
    "    \"Dense\": [16, 16, 16],\n",
    "    \"BatchNormalization\": [],\n",
    "    \"Activation\": [12]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iu5gFNhaLNE"
   },
   "source": [
    "Here, we are specifying that we want to use at most 4 bits for weights and activations, and at most 8 bits for biases in convolutional and depthwise convolutions, but we allow up to 8 bits for kernels in dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUMQGEIDblSa"
   },
   "source": [
    "Let's define now the forgiving factor. We will consider energy minimization as a goal as follows.  Here, we are saying that we allow 8% reduction in accuracy for a 2x reduction in energy, both reference and trials have parameters and activations on SRAM, both reference model and quantization trials do not read/write from DRAM on I/O operations, and we should consider both experiments to use SRAMs with minimum tensor sizes (commonly called distributed SRAM implementation).\n",
    "\n",
    "We also need to specify the quantizers for the inputs. In this case, we want to use `int8` as source quantizers. Other possible types are `int16`, `int32`, `fp16` or `fp32`, besides `QKeras` quantizer types.\n",
    "\n",
    "Finally, to be fair, we want to compare our quantization against fixed-point 8-bit inputs, outputs, activations, weights and biases, and 32-bit accumulators.\n",
    "\n",
    "Remember that a `forgiving factor` forgives a drop in a metric such as `accuracy` if the gains of the model are much bigger than the drop. For example, it corresponds to the sentence *we allow $\\tt{delta}\\%$ reduction in accuracy if the quantized model has $\\tt{rate} \\times$ smaller energy than the original model*, being a multiplicative factor to the metric. It is computed by $1 + \\tt{delta} \\times  \\log_{\\tt{rate}}(\\tt{stress} \\times \\tt{reference\\_cost} / \\tt{trial\\_cost})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kS31TuZ-aKb1"
   },
   "outputs": [],
   "source": [
    "goal = {\n",
    "    \"type\": \"energy\",\n",
    "    \"params\": {\n",
    "        \"delta_p\": 8.0,\n",
    "        \"delta_n\": 8.0,\n",
    "        \"rate\": 2.0,\n",
    "        \"stress\": 1.0,\n",
    "        \"process\": \"horowitz\",\n",
    "        \"parameters_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"activations_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"rd_wr_on_io\": [False, False],\n",
    "        \"min_sram_size\": [0, 0],\n",
    "        \"source_quantizers\": [\"int8\"],\n",
    "        \"reference_internal\": \"int8\",\n",
    "        \"reference_accumulator\": \"int32\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QzyWPA-dCxm"
   },
   "source": [
    "There are a few more things we need to define. Let's bundle them on a dictionary and pass them to `AutoQKeras`.  We will try a maximum of 10 trials (`max_trials`) just to limit the time we will spend finding the best quantization here.  Please note that this parameter is not valid if you are running in `hyperband` mode.\n",
    "\n",
    "`output_dir` is the directory where we will store our results. Since we are running on a colab, we will let `tempfile` chooce a directory for us.\n",
    "\n",
    "`learning_rate_optimizer` allows `AutoQKeras` to change the optimization function and the `learning_rate` to try to improve the quantization results. Since it is still experimental, it may be the case that in some cases it will get worse results. \n",
    "\n",
    "Because we are tuning filters as well, we should set `transfer_weights` to `False` as the trainable parameters will have different shapes.\n",
    "\n",
    "In `AutoQKeras` we have three modes of operation: `random`, `bayesian` and `hyperband`. I recommend the user to refer to `KerasTuner` (https://keras-team.github.io/keras-tuner/) for a complete description of them.\n",
    "\n",
    "`tune_filters` can be set to `layer`, `block` or `none`. If `tune_filters` is `block`, we change the filters by the same amount for all layers being quantized in the trial. If `tune_filters` is `layer`, we will possibly change the number of filters for each layer independently. Finally, if `tune_filters` is `none`, we will not perform filter tuning.\n",
    "\n",
    "Together with `tune_filters`, `tune_filter_exceptions` allows the user to specify by a regular expression which filters we should not perform filter tuning, which is especially good for the last layers of the network.\n",
    "\n",
    "Filter tuning is a very important feature of `AutoQKeras`. When we deep quantize a model, we may need less or more filters for each layer (and you can guess we do not know a priori how many filters we will need for each layer). Let me give you a rationale behind this.\n",
    "\n",
    "- **less filters**: let us assume we have two set of filter coefficients we want quantize: $[-0.3, 0.2, 0.5, 0.15]$ and $[-0.5, 0.4, 0.1, 0.65]$. If we apply a $\\tt{binary}$ quantizer with $\\tt{scale} = \\big\\lceil \\log_2(\\frac{\\sum |w|}{N}) \\big\\rceil$, where $w$ are the filter coefficients and $N$ is the number of coefficients, we will end up with the same filter $\\tt{binary}([-0.3, 0.2, 0.5, 0.15]) = \\tt{binary}([-0.5, 0.4, 0.1, 0.65]) = [-1,1,1,1] \\times 0.5$. In this case we are assuming the $\\tt{scale}$ is a power-of-2 number so that it can be efficiently implemented by a shift operation;\n",
    "\n",
    "- **more filters**: it is clear that quantization will drop information (just look at the example above) and deep quantization will drop more information, so to recover some of the boundary regions in layers that perform feature extraction, we may need to add more filters to the layer when we quantize it.\n",
    "\n",
    "We do not want to quantize the `softmax` layer, which is the last layer of the network. In `AutoQKeras`, you can specify the indexes that you want to perform quantization by specifying the corresponding index of the layer in `Keras`, i.e. if you can get the layer as `model.layers[i]` in `Keras`, `i` is the index of the layer.\n",
    "\n",
    "Finally, for data parallel distributed training, we should pass the strategy in `distribution_strategy` to `KerasTuner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1591840405963,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "2-fyACb2dIAN",
    "outputId": "a180fa3f-8cc3-4f70-ce70-c05c28f88d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantizing layers: []\n"
     ]
    }
   ],
   "source": [
    "run_config = {\n",
    "  \"output_dir\": tempfile.mkdtemp(),\n",
    "  \"goal\": goal,\n",
    "  \"quantization_config\": quantization_config,\n",
    "  \"learning_rate_optimizer\": False,\n",
    "  \"transfer_weights\": False,\n",
    "  \"mode\": \"random\",\n",
    "  \"seed\": 42,\n",
    "  \"limit\": limit,\n",
    "  \"tune_filters\": \"layer\",\n",
    "  \"tune_filters_exceptions\": \"^dense\",\n",
    "  \"distribution_strategy\": cur_strategy,\n",
    "  # first layer is input, layer two layers are softmax and flatten\n",
    "  \"layer_indexes\": range(1, len(model.layers) - 1),\n",
    "  \"max_trials\": 20\n",
    "\n",
    "}\n",
    "\n",
    "print(\"quantizing layers:\", [model.layers[i].name for i in run_config[\"layer_indexes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 471192,
     "status": "ok",
     "timestamp": 1591840877167,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "MxlZFpa3fBv2",
    "outputId": "4d339846-1832-4a79-89b3-c9c4944dd47a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit configuration:{\"SimpleRNN\": [12, 12, 12, 12], \"Dense\": [16, 16, 16], \"BatchNormalization\": [], \"Activation\": [12]}\n",
      "name SimpleRNN_kernel_quantizer\n",
      "name dense_kernel_quantizer\n",
      "abovetrial_size\n",
      "target 4\n",
      "self.trial_size 4\n",
      "learning_rate: 7.812500371073838e-06\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " SimpleRNN (SimpleRNN)       (None, 8)                 80        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "self.trial_size 4\n",
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=4 reference_size=4\n",
      "       delta=0.00%\n",
      "Total Cost Reduction:\n",
      "       4 vs 4 (0.00%)\n",
      "\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "SimpleRNN_kernel_quantizer (Choice)\n",
      "{'default': 'quantized_bits(8,0,1,alpha=1.0)', 'conditions': [], 'values': ['quantized_bits(8,0,1,alpha=1.0)', 'quantized_bits(10,0,1,alpha=1.0)', 'quantized_bits(12,0,1,alpha=1.0)', 'quantized_po2(8,1)'], 'ordered': False}\n",
      "dense_kernel_quantizer (Choice)\n",
      "{'default': 'quantized_bits(8,0,1,alpha=1.0)', 'conditions': [], 'values': ['quantized_bits(8,0,1,alpha=1.0)', 'quantized_bits(10,0,1,alpha=1.0)', 'quantized_bits(12,0,1,alpha=1.0)', 'quantized_po2(8,1)'], 'ordered': False}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "SimpleRNN_kerne...|quantized_bits(...|?                 \n",
      "dense_kernel_qu...|quantized_bits(...|?                 \n",
      "\n",
      "name SimpleRNN_kernel_quantizer\n",
      "name dense_kernel_quantizer\n",
      "abovetrial_size\n",
      "target 4\n",
      "self.trial_size 4\n",
      "learning_rate: 7.812500371073838e-06\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " SimpleRNN (SimpleRNN)       (None, 8)                 80        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "self.trial_size 4\n",
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=4 reference_size=4\n",
      "       delta=0.00%\n",
      "Total Cost Reduction:\n",
      "       4 vs 4 (0.00%)\n",
      "\n",
      "Epoch 1/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 3.1309e-04 - mse: 3.1309e-04 - trial: 4.0000 - score: 1.0000 - val_loss: 2.0424e-04 - val_mse: 2.0424e-04 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 2/100\n",
      "14063/14063 [==============================] - 31s 2ms/step - loss: 1.4700e-04 - mse: 1.4700e-04 - trial: 4.0000 - score: 1.0000 - val_loss: 1.0207e-04 - val_mse: 1.0207e-04 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 3/100\n",
      "14063/14063 [==============================] - 31s 2ms/step - loss: 7.4008e-05 - mse: 7.4008e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 5.5818e-05 - val_mse: 5.5818e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 4/100\n",
      "14063/14063 [==============================] - 30s 2ms/step - loss: 4.2644e-05 - mse: 4.2644e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 3.6973e-05 - val_mse: 3.6973e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 5/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 3.0314e-05 - mse: 3.0314e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 2.9956e-05 - val_mse: 2.9956e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 6/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 2.5540e-05 - mse: 2.5540e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 2.6851e-05 - val_mse: 2.6851e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 7/100\n",
      "14063/14063 [==============================] - 31s 2ms/step - loss: 2.3138e-05 - mse: 2.3138e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 2.4667e-05 - val_mse: 2.4667e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 8/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 2.1340e-05 - mse: 2.1340e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 2.2934e-05 - val_mse: 2.2934e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 9/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.9879e-05 - mse: 1.9879e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 2.1458e-05 - val_mse: 2.1458e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 10/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.8712e-05 - mse: 1.8712e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 2.0282e-05 - val_mse: 2.0282e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 11/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.7768e-05 - mse: 1.7768e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.9276e-05 - val_mse: 1.9276e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 12/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.7040e-05 - mse: 1.7040e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.8492e-05 - val_mse: 1.8492e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 13/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.6504e-05 - mse: 1.6504e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.7898e-05 - val_mse: 1.7898e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 14/100\n",
      "14063/14063 [==============================] - 35s 2ms/step - loss: 1.6107e-05 - mse: 1.6107e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.7427e-05 - val_mse: 1.7427e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 15/100\n",
      "14063/14063 [==============================] - 35s 3ms/step - loss: 1.5804e-05 - mse: 1.5804e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.7100e-05 - val_mse: 1.7100e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 16/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.5569e-05 - mse: 1.5569e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.6817e-05 - val_mse: 1.6817e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 17/100\n",
      "14063/14063 [==============================] - 31s 2ms/step - loss: 1.5386e-05 - mse: 1.5386e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.6581e-05 - val_mse: 1.6581e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 18/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.5229e-05 - mse: 1.5229e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.6382e-05 - val_mse: 1.6382e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 19/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.5085e-05 - mse: 1.5085e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.6213e-05 - val_mse: 1.6213e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 20/100\n",
      "14063/14063 [==============================] - 35s 2ms/step - loss: 1.4956e-05 - mse: 1.4956e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.6065e-05 - val_mse: 1.6065e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 21/100\n",
      "14063/14063 [==============================] - 31s 2ms/step - loss: 1.4847e-05 - mse: 1.4847e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5950e-05 - val_mse: 1.5950e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 22/100\n",
      "14063/14063 [==============================] - 31s 2ms/step - loss: 1.4755e-05 - mse: 1.4755e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5841e-05 - val_mse: 1.5841e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 23/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.4676e-05 - mse: 1.4676e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5759e-05 - val_mse: 1.5759e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 24/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.4611e-05 - mse: 1.4611e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5675e-05 - val_mse: 1.5675e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 25/100\n",
      "14063/14063 [==============================] - 31s 2ms/step - loss: 1.4549e-05 - mse: 1.4549e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5597e-05 - val_mse: 1.5597e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 26/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.4493e-05 - mse: 1.4493e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5552e-05 - val_mse: 1.5552e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 27/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.4434e-05 - mse: 1.4434e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5470e-05 - val_mse: 1.5470e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 28/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.4376e-05 - mse: 1.4376e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5417e-05 - val_mse: 1.5417e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 29/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.4316e-05 - mse: 1.4316e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5352e-05 - val_mse: 1.5352e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 30/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.4254e-05 - mse: 1.4254e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5299e-05 - val_mse: 1.5299e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 31/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.4197e-05 - mse: 1.4197e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5234e-05 - val_mse: 1.5234e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 32/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.4138e-05 - mse: 1.4138e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5181e-05 - val_mse: 1.5181e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 33/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.4090e-05 - mse: 1.4090e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5147e-05 - val_mse: 1.5147e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 34/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.4040e-05 - mse: 1.4040e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5102e-05 - val_mse: 1.5102e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 35/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3994e-05 - mse: 1.3994e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5067e-05 - val_mse: 1.5067e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 36/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3946e-05 - mse: 1.3946e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.5017e-05 - val_mse: 1.5017e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 37/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3901e-05 - mse: 1.3901e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4975e-05 - val_mse: 1.4975e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 38/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.3858e-05 - mse: 1.3858e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4905e-05 - val_mse: 1.4905e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 39/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.3818e-05 - mse: 1.3818e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4847e-05 - val_mse: 1.4847e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 40/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3783e-05 - mse: 1.3783e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4792e-05 - val_mse: 1.4792e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 41/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3747e-05 - mse: 1.3747e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4736e-05 - val_mse: 1.4736e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 42/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3713e-05 - mse: 1.3713e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4651e-05 - val_mse: 1.4651e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 43/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3682e-05 - mse: 1.3682e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4590e-05 - val_mse: 1.4590e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 44/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.3654e-05 - mse: 1.3654e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4526e-05 - val_mse: 1.4526e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 45/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3624e-05 - mse: 1.3624e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4479e-05 - val_mse: 1.4479e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 46/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3602e-05 - mse: 1.3602e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4430e-05 - val_mse: 1.4430e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 47/100\n",
      "14063/14063 [==============================] - 36s 3ms/step - loss: 1.3573e-05 - mse: 1.3573e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4388e-05 - val_mse: 1.4388e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 48/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3547e-05 - mse: 1.3547e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4354e-05 - val_mse: 1.4354e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 49/100\n",
      "14063/14063 [==============================] - 35s 2ms/step - loss: 1.3523e-05 - mse: 1.3523e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4291e-05 - val_mse: 1.4291e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 50/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3502e-05 - mse: 1.3502e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4246e-05 - val_mse: 1.4246e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 51/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3478e-05 - mse: 1.3478e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4209e-05 - val_mse: 1.4209e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 52/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3455e-05 - mse: 1.3455e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4160e-05 - val_mse: 1.4160e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 53/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.3431e-05 - mse: 1.3431e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4144e-05 - val_mse: 1.4144e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 54/100\n",
      "14063/14063 [==============================] - 32s 2ms/step - loss: 1.3409e-05 - mse: 1.3409e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4075e-05 - val_mse: 1.4075e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 55/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3388e-05 - mse: 1.3388e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4040e-05 - val_mse: 1.4040e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 56/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3367e-05 - mse: 1.3367e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.4013e-05 - val_mse: 1.4013e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 57/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3347e-05 - mse: 1.3347e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3986e-05 - val_mse: 1.3986e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 58/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3324e-05 - mse: 1.3324e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3946e-05 - val_mse: 1.3946e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 59/100\n",
      "14063/14063 [==============================] - 35s 3ms/step - loss: 1.3306e-05 - mse: 1.3306e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3905e-05 - val_mse: 1.3905e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 60/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3284e-05 - mse: 1.3284e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3876e-05 - val_mse: 1.3876e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 61/100\n",
      "14063/14063 [==============================] - 35s 2ms/step - loss: 1.3265e-05 - mse: 1.3265e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3855e-05 - val_mse: 1.3855e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 62/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3245e-05 - mse: 1.3245e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3802e-05 - val_mse: 1.3802e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 63/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3224e-05 - mse: 1.3224e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3790e-05 - val_mse: 1.3790e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 64/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3207e-05 - mse: 1.3207e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3737e-05 - val_mse: 1.3737e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 65/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3188e-05 - mse: 1.3188e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3749e-05 - val_mse: 1.3749e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 66/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3170e-05 - mse: 1.3170e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3688e-05 - val_mse: 1.3688e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 67/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3155e-05 - mse: 1.3155e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3655e-05 - val_mse: 1.3655e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 68/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3139e-05 - mse: 1.3139e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3637e-05 - val_mse: 1.3637e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 69/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3121e-05 - mse: 1.3121e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3623e-05 - val_mse: 1.3623e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 70/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.3106e-05 - mse: 1.3106e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3589e-05 - val_mse: 1.3589e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 71/100\n",
      "14063/14063 [==============================] - 36s 3ms/step - loss: 1.3086e-05 - mse: 1.3086e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3564e-05 - val_mse: 1.3564e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 72/100\n",
      "14063/14063 [==============================] - 35s 3ms/step - loss: 1.3073e-05 - mse: 1.3073e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3551e-05 - val_mse: 1.3551e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 73/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3057e-05 - mse: 1.3057e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3519e-05 - val_mse: 1.3519e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 74/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3042e-05 - mse: 1.3042e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3497e-05 - val_mse: 1.3497e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 75/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.3028e-05 - mse: 1.3028e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3491e-05 - val_mse: 1.3491e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 76/100\n",
      "14063/14063 [==============================] - 35s 2ms/step - loss: 1.3012e-05 - mse: 1.3012e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3463e-05 - val_mse: 1.3463e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 77/100\n",
      "14063/14063 [==============================] - 36s 3ms/step - loss: 1.2998e-05 - mse: 1.2998e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3448e-05 - val_mse: 1.3448e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 78/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.2983e-05 - mse: 1.2983e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3440e-05 - val_mse: 1.3440e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 79/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.2970e-05 - mse: 1.2970e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3412e-05 - val_mse: 1.3412e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 80/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.2955e-05 - mse: 1.2955e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3386e-05 - val_mse: 1.3386e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 81/100\n",
      "14063/14063 [==============================] - 35s 2ms/step - loss: 1.2943e-05 - mse: 1.2943e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3367e-05 - val_mse: 1.3367e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 82/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.2928e-05 - mse: 1.2928e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3349e-05 - val_mse: 1.3349e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 83/100\n",
      "14063/14063 [==============================] - 33s 2ms/step - loss: 1.2913e-05 - mse: 1.2913e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3334e-05 - val_mse: 1.3334e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 84/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.2902e-05 - mse: 1.2902e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3321e-05 - val_mse: 1.3321e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 85/100\n",
      "14063/14063 [==============================] - 34s 2ms/step - loss: 1.2889e-05 - mse: 1.2889e-05 - trial: 4.0000 - score: 1.0000 - val_loss: 1.3309e-05 - val_mse: 1.3309e-05 - val_trial: 4.0000 - val_score: 1.0000\n",
      "Epoch 86/100\n",
      " 5903/14063 [===========>..................] - ETA: 18s - loss: 1.2981e-05 - mse: 1.2981e-05 - trial: 4.0000 - score: 1.0000"
     ]
    }
   ],
   "source": [
    "autoqk = AutoQKeras(model, metrics=[\"mse\"], custom_objects=custom_objects, **run_config)\n",
    "autoqk.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=64, epochs=100, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.3'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner\n",
    "keras_tuner.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LW_qN8-lOwL0"
   },
   "source": [
    "Now, let's see which model is the best model we got.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3961,
     "status": "ok",
     "timestamp": 1591840881173,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "1L7KivAoffaL",
    "outputId": "f44b07a3-027d-4d69-9864-b3670815c407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name SimpleRNN_kernel_quantizer\n",
      "name dense_kernel_quantizer\n",
      "abovetrial_size\n",
      "target 4\n",
      "self.trial_size 4\n",
      "learning_rate: 7.812500371073838e-06\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " SimpleRNN (SimpleRNN)       (None, 8)                 80        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "self.trial_size 4\n",
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=4 reference_size=4\n",
      "       delta=0.00%\n",
      "Total Cost Reduction:\n",
      "       4 vs 4 (0.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel = autoqk.get_best_model()\n",
    "qmodel.save_weights(\"qmodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RB2xBRhJiwoh"
   },
   "source": [
    "We got here >90% reduction in energy when compared to 8-bit tensors and 32-bit accumulators. Remember that our original number was 3.3 uJ for fp32.  The end model has 11 nJ for the quantized model as opposed to 204 nJ for the 8-bit original quantized model. As these energy numbers are from high-level energy models, you should remember to consider the relations between them, and not the actual numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy0zcqvQoBnb"
   },
   "source": [
    "Let's train this model to see how much accuracy we can get of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 71353,
     "status": "ok",
     "timestamp": 1591840952535,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "_ipZSEfgoGdb",
    "outputId": "b184269d-1161-417a-e1ae-e852dc451561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 5270/14063 [==========>...................] - ETA: 16s - loss: 8.6858e-04 - mse: 8.6858e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113843/1097602159.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mqmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mqmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda3/envs/hls4ml-tutorial/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    910\u001b[0m                           expand_composites=expand_composites)\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qmodel.load_weights(\"qmodel.h5\")\n",
    "with cur_strategy.scope():\n",
    "  optimizer = Adam(lr=0.001)\n",
    "  qmodel.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "  qmodel.fit(x_train, y_train, epochs=100, batch_size=64, validation_data=(x_val, y_val),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fr95jcPROz7p"
   },
   "source": [
    "One of problems of trying to quantize the whole thing in one shot is that we may end up with too many choices to make, which will make the entire search space very high. In order to reduce the search space, `AutoQKeras` has two methods to enable users to cope with the explosion of choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zc7ZrnbPIJA"
   },
   "source": [
    "## Grouping Layers to Use the Same Choice\n",
    "\n",
    "In this case, we can provide regular expressions to `limit` to specify layer names that should be grouped together. In our example, suppose we want to group  convolution layers (except the first one) and all activations except the last one to use the same quantization.\n",
    "\n",
    "For the first convolution layer, we want to limit the quantization types to fewer choices as the input is already an 8-bit number.  The last activation will be fed to a feature classifier layer, so we may leave it with more bits. Because our `dense` is actually a `Conv2D` operation, we will enable 8-bits for the weights by layer name. \n",
    "\n",
    "We first need to look at the names of the layers for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 428
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1591840952867,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "w-d8nhG0pJF0",
    "outputId": "6529b630-f382-4e2a-94ef-ba3d9e3f875c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SimpleRNN', 'dense']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint([layer.name for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32Enp890pU_4"
   },
   "source": [
    "Convolution layers for `mnist` have names specified as `conv2d_[01234]`. Activation layers have names specified as `act_[01234]`. So, we can create the following regular expressions to reduce the search space in our model.\n",
    "\n",
    "Please note that layer class names always select different quantizers, so the user needs to specify a pattern for layer names if he/she wants to use the same quantization for the group of layers.\n",
    "\n",
    "You can see here another feature of the limit. You can specify the maximum number of bits, or cherry pick which quantizers you want to try for a specific layer if instead of the maximum number of bits you specify a list of quantizers fron `quantization_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5XItp95PHW6"
   },
   "outputs": [],
   "source": [
    "limit = {\n",
    "    \"Dense\": [8, 8, 4],\n",
    "    \"Conv2D\": [4, 8, 4],\n",
    "    \"DepthwiseConv2D\": [4, 8, 4],\n",
    "    \"Activation\": [4],\n",
    "    \"BatchNormalization\": [],\n",
    "\n",
    "    \"^conv2d_0$\": [\n",
    "                   [\"binary\", \"ternary\", \"quantized_bits(2,1,1,alpha=1.0)\"],\n",
    "                   8, 4\n",
    "    ],\n",
    "    \"^conv2d_[1234]$\": [4, 8, 4],\n",
    "    \"^act_[0123]$\": [4],\n",
    "    \"^act_4$\": [8],\n",
    "    \"^dense$\": [8, 8, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJs1L-jIie7w"
   },
   "outputs": [],
   "source": [
    "run_config = {\n",
    "  \"output_dir\": tempfile.mkdtemp(),\n",
    "  \"goal\": goal,\n",
    "  \"quantization_config\": quantization_config,\n",
    "  \"learning_rate_optimizer\": False,\n",
    "  \"transfer_weights\": False,\n",
    "  \"mode\": \"random\",\n",
    "  \"seed\": 42,\n",
    "  \"limit\": limit,\n",
    "  \"tune_filters\": \"layer\",\n",
    "  \"tune_filters_exceptions\": \"^dense\",\n",
    "  \"distribution_strategy\": cur_strategy,\n",
    "  \"layer_indexes\": range(1, len(model.layers) - 1),\n",
    "  \"max_trials\": 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 993665,
     "status": "ok",
     "timestamp": 1591841947161,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "c7eSwXyijhzc",
    "outputId": "6c76a21f-cbb3-4bc5-b899-b02c28821b78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 01m 06s]\n",
      "val_score: 0.17637935280799866\n",
      "\n",
      "Best val_score So Far: 0.17637935280799866\n",
      "Total elapsed time: 00h 02m 12s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "autoqk = AutoQKeras(model, metrics=[\"acc\"], custom_objects=custom_objects, **run_config)\n",
    "autoqk.fit(x_train, y_train, epochs=100, batch_size=64, validation_data=(x_val, y_val),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sYp8Z2pnLi1"
   },
   "source": [
    "Let's see the reduction now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7109,
     "status": "ok",
     "timestamp": 1591841954308,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "yj826gNhjsfK",
    "outputId": "2e7f17d7-794e-44f6-d23a-452759727a53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name ^dense$_kernel_quantizer\n",
      "abovetrial_size\n",
      "target 4\n",
      "self.trial_size 4\n",
      "learning_rate: 7.812500371073838e-06\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " SimpleRNN (SimpleRNN)       (None, 8)                 80        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "self.trial_size 4\n",
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=4 reference_size=4\n",
      "       delta=0.00%\n",
      "Total Cost Reduction:\n",
      "       4 vs 4 (0.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel = autoqk.get_best_model()\n",
    "qmodel.save_weights(\"qmodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXMcqxLAnY8t"
   },
   "source": [
    "Let's train this model for more time to see how much we can get in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68145,
     "status": "ok",
     "timestamp": 1591842022471,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "qpT8QgkJnQPa",
    "outputId": "61e711db-6187-4047-dae8-9ce2d093f56c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "220/220 [==============================] - 2s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 2/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 3/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 4/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 5/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 6/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 7/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 8/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 9/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 10/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 11/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 12/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 13/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 14/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 15/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 16/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 17/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 18/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 19/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 20/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 21/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 22/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 23/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 24/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 25/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 26/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 27/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 28/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 29/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 30/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 31/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 32/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 33/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 34/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 35/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 36/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 37/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 38/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 39/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 40/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 41/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 42/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 43/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 44/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 45/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 46/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 47/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 48/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 49/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 50/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 51/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 52/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 53/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 54/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 55/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 56/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 57/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 58/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 59/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 60/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 61/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 62/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 63/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 64/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 65/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 66/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 67/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 68/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 69/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 70/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 71/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 72/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 73/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 74/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 75/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 76/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 77/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 78/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 79/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 80/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 81/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 82/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 83/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 84/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 85/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 86/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 87/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 88/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 89/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 90/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 91/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 92/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 93/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 94/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 95/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 96/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 97/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 98/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 99/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 100/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 101/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 102/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 103/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 104/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 105/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 106/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 107/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 108/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 109/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 110/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 111/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 112/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 113/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 114/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 115/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 116/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 117/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 118/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 119/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 120/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 121/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 122/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 123/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 124/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 125/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 126/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 127/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 128/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 129/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 130/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 131/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 132/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 133/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 134/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 135/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 136/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 137/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 138/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 139/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 140/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 141/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 142/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 143/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 144/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 145/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 146/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 147/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 148/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 149/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 150/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 151/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 152/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 153/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 154/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 155/200\n",
      "220/220 [==============================] - 1s 6ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 156/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 157/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 158/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 159/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 160/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 161/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 162/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 163/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 164/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 165/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 166/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 167/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 168/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 169/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 170/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 171/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 172/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 173/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 174/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 175/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 176/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 177/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 178/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 179/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 180/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 181/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 182/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 183/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 184/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 185/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 186/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 187/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 188/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 189/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 190/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 191/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 192/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 193/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 194/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 195/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 196/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 197/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 198/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 199/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n",
      "Epoch 200/200\n",
      "220/220 [==============================] - 1s 5ms/step - loss: nan - acc: 0.1760 - val_loss: nan - val_acc: 0.1764\n"
     ]
    }
   ],
   "source": [
    "qmodel.load_weights(\"qmodel.h5\")\n",
    "with cur_strategy.scope():\n",
    "  optimizer = Adam(lr=0.02)\n",
    "  qmodel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "  qmodel.fit(x_train, y_train, epochs=100, batch_size=64, validation_data=(x_val, y_val),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAV6Kw0QoODq"
   },
   "source": [
    "## Quantization by Blocks\n",
    "\n",
    "In the previous section, we enforced that all decisions were the same in order to reduce the number of options to quantize a model. \n",
    "\n",
    "Another approach is still to allow models to have each block of layers to makde their own choice, but quantizing the blocks sequentially, either from inputs to outputs, or by quantizing higher energy blocks first.\n",
    "\n",
    "The rationale for this method is that if we quantize the blocks one by one, and assuming that each block has $N$ choices, and $B$ blocks, we end up trying $N B$ options, instead of $N^B$ choices.  The reader should note that this is an approximation as there is no guarantee that we will obtain the best quantization possible.\n",
    "\n",
    "Should you do sequential from inputs to outputs or starting from the block that has the highest impact?\n",
    "\n",
    "If you have a network like ResNet, and if you want to do filter tuning, you need to block the layers by the resnet definition of a block, i.e. including full identity or convolutional blocks, and quantize the model from inputs to outputs, so that you can preserve at each stage the number of channels for the residual block. \n",
    "\n",
    "In order to perform quantization by blocks, you need to specify two other parameters in our `run_config`. `blocks` is a list of regular expressions of the groups you want to quantize. If a layer does not match the block pattern, it will not be quantized.  `schedule_block` specifies the mode for block quantization scheduling. It can be `sequential` or `cost` if you want to schedule first the blocks by decreasing cost size (energy or bits).\n",
    "\n",
    "In this model, there are a few optimizations that we perform automatically. First, we dynamically reduce the learning rate of the blocks that we have already quantized as setting them to not-trainable does not seem to work, so we still allow them to train, but at a slower pace. In addition, we try to dynamically adjust the learning rate for the layer we are trying to quantize as opposed to the learning rate of the unquantized layers. Finally, we transfer the weights of the models we have already quantized whenever we can do (if the shapes remain the same). \n",
    "\n",
    "Regardless on how we schedule the operations, we amortize the nubmer of trials for the cost of the block (energy or bits with respect to the total energy or number of bits of the network).\n",
    "\n",
    "Instead of invoking `AutoQKeras` now, we will invoke `AutoQKeras` scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUz4A6SKnhUf"
   },
   "outputs": [],
   "source": [
    "run_config = {\n",
    "  \"output_dir\": tempfile.mkdtemp(),\n",
    "  \"goal\": goal,\n",
    "  \"quantization_config\": quantization_config,\n",
    "  \"learning_rate_optimizer\": False,\n",
    "  \"transfer_weights\": False,\n",
    "  \"mode\": \"random\",\n",
    "  \"seed\": 42,\n",
    "  \"limit\": limit,\n",
    "  \"tune_filters\": \"layer\",\n",
    "  \"tune_filters_exceptions\": \"^dense\",\n",
    "  \"distribution_strategy\": cur_strategy,\n",
    "  \"layer_indexes\": range(1, len(model.layers) - 1),\n",
    "  \"max_trials\": 40,\n",
    "\n",
    "  \"blocks\": [\n",
    "    \"^.*_0$\",\n",
    "    \"^.*_1$\",\n",
    "    \"^.*_2$\",\n",
    "    \"^.*_3$\",\n",
    "    \"^.*_4$\",\n",
    "    \"^dense\"\n",
    "  ],\n",
    "  \"schedule_block\": \"cost\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWJiZZ9vsORJ"
   },
   "source": [
    "Because specifying regular expressions is error prone, we recommend that you first try to run `AutoQKerasScheduler` in debug mode to print the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 737
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1591842023212,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "uSOxKQGwsqf2",
    "outputId": "18647e4f-ef7a-4c6a-aeb8-0c9c2039fdbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SimpleRNN', 'dense']\n",
      "... block cost: 0 / 4\n",
      "... adjusting max_trials for this block to 10\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113843/839304910.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mautoqk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoQKerasScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mautoqk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/code/qkeras/qkeras/autoqkeras/autoqkeras_internal.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0;31m# user.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       \u001b[0;32massert\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pattern {} is : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pprint.pprint([layer.name for layer in model.layers])\n",
    "autoqk = AutoQKerasScheduler(model, metrics=[\"acc\"], custom_objects=custom_objects, debug=True, **run_config)\n",
    "autoqk.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=1024, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQPUKPZhC_SI"
   },
   "source": [
    "All blocks seem to be fine. Let's find the best quantization now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1938883,
     "status": "ok",
     "timestamp": 1591843962106,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "sXt-cRKvDEaL",
    "outputId": "36db3217-86ff-4425-ee12-f637a4fc1841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 50s]\n",
      "val_score: 1.4008370637893677\n",
      "\n",
      "Best val_score So Far: 1.416741967201233\n",
      "Total elapsed time: 00h 07m 44s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in /tmp/tmp7td4spyh_1/5\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x7f43efea96d0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_bits(8,0,1,alpha=1.0)\n",
      "^(dense)$_bias_quantizer: quantized_bits(8,3,1)\n",
      "Score: 1.416741967201233\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_bits(4,0,1,alpha=1.0)\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4149357080459595\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: stochastic_binary\n",
      "^(dense)$_bias_quantizer: quantized_bits(8,3,1)\n",
      "Score: 1.414434552192688\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: binary\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4135743379592896\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: stochastic_binary\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4087834358215332\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: ternary\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4080085754394531\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_po2(4,1)\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.4034206867218018\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_bits(2,1,1,alpha=1.0)\n",
      "^(dense)$_bias_quantizer: quantized_po2(4,8)\n",
      "Score: 1.4017080068588257\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: quantized_bits(2,1,1,alpha=1.0)\n",
      "^(dense)$_bias_quantizer: quantized_bits(8,3,1)\n",
      "Score: 1.4008370637893677\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "^(dense)$_kernel_quantizer: stochastic_ternary\n",
      "^(dense)$_bias_quantizer: quantized_bits(4,0,1)\n",
      "Score: 1.40045964717865\n",
      "learning_rate: 0.019999999552965164\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_0 (QConv2D)          (None, 14, 14, 8)         72        \n",
      "                                                                 \n",
      " bn_0 (QBatchNormalization)  (None, 14, 14, 8)         32        \n",
      "                                                                 \n",
      " act_0 (QActivation)         (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " drop_0 (Dropout)            (None, 14, 14, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (QConv2D)          (None, 7, 7, 16)          1152      \n",
      "                                                                 \n",
      " bn_1 (QBatchNormalization)  (None, 7, 7, 16)          64        \n",
      "                                                                 \n",
      " act_1 (QActivation)         (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " drop_1 (Dropout)            (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " conv2d_2 (QConv2D)          (None, 4, 4, 24)          3456      \n",
      "                                                                 \n",
      " bn_2 (QBatchNormalization)  (None, 4, 4, 24)          96        \n",
      "                                                                 \n",
      " act_2 (QActivation)         (None, 4, 4, 24)          0         \n",
      "                                                                 \n",
      " drop_2 (Dropout)            (None, 4, 4, 24)          0         \n",
      "                                                                 \n",
      " conv2d_3 (QConv2D)          (None, 2, 2, 32)          6912      \n",
      "                                                                 \n",
      " bn_3 (QBatchNormalization)  (None, 2, 2, 32)          128       \n",
      "                                                                 \n",
      " act_3 (QActivation)         (None, 2, 2, 32)          0         \n",
      "                                                                 \n",
      " drop_3 (Dropout)            (None, 2, 2, 32)          0         \n",
      "                                                                 \n",
      " conv2d_4 (QConv2D)          (None, 1, 1, 64)          18432     \n",
      "                                                                 \n",
      " bn_4 (QBatchNormalization)  (None, 1, 1, 64)          256       \n",
      "                                                                 \n",
      " act_4 (QActivation)         (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " drop_4 (Dropout)            (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (QDense)              (None, 10)                650       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,250\n",
      "Trainable params: 30,962\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n",
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=11506 reference_size=574175\n",
      "       delta=45.13%\n",
      "Total Cost Reduction:\n",
      "       11506 vs 574175 (-98.00%)\n",
      "conv2d_0             f=8 binary(alpha='auto_po2') \n",
      "bn_0                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_0                quantized_relu(4,2)\n",
      "conv2d_1             f=16 binary(alpha='auto_po2') \n",
      "bn_1                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_1                quantized_relu(3,1)\n",
      "conv2d_2             f=24 quantized_bits(4,0,1,alpha=1.0) \n",
      "bn_2                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_2                quantized_relu_po2(4,4)\n",
      "conv2d_3             f=32 quantized_bits(4,0,1,alpha=1.0) \n",
      "bn_3                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_3                quantized_relu(3,1)\n",
      "conv2d_4             f=64 ternary(alpha='auto_po2') \n",
      "bn_4                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_4                quantized_relu_po2(4,4)\n",
      "dense                u=10 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,3,1) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7td4spyh_1/model_block_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7td4spyh_1/model_block_5/assets\n"
     ]
    }
   ],
   "source": [
    "autoqk = AutoQKerasScheduler(model, metrics=[\"acc\"], custom_objects=custom_objects, **run_config)\n",
    "autoqk.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=1024, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 291
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1591843962540,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "ArdGbsXFDK-I",
    "outputId": "43730cd5-93fc-4838-c49a-1f3f4151fa54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=11506 reference_size=574175\n",
      "       delta=45.13%\n",
      "Total Cost Reduction:\n",
      "       11506 vs 574175 (-98.00%)\n",
      "conv2d_0             f=8 binary(alpha='auto_po2') \n",
      "bn_0                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_0                quantized_relu(4,2)\n",
      "conv2d_1             f=16 binary(alpha='auto_po2') \n",
      "bn_1                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_1                quantized_relu(3,1)\n",
      "conv2d_2             f=24 quantized_bits(4,0,1,alpha=1.0) \n",
      "bn_2                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_2                quantized_relu_po2(4,4)\n",
      "conv2d_3             f=32 quantized_bits(4,0,1,alpha=1.0) \n",
      "bn_3                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_3                quantized_relu(3,1)\n",
      "conv2d_4             f=64 ternary(alpha='auto_po2') \n",
      "bn_4                 QBN, mean=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "act_4                quantized_relu_po2(4,4)\n",
      "dense                u=10 quantized_bits(8,0,1,alpha=1.0) quantized_bits(8,3,1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel = autoqk.get_best_model()\n",
    "qmodel.save_weights(\"qmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 69779,
     "status": "ok",
     "timestamp": 1591844032332,
     "user": {
      "displayName": "Claudionor Coelho",
      "photoUrl": "",
      "userId": "01084525977535968041"
     },
     "user_tz": 420
    },
    "id": "RHGb6YHFEgtV",
    "outputId": "5578ce49-1ee9-4063-deab-1b3db9f4b66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "15/15 [==============================] - 5s 158ms/step - loss: 1.3593 - acc: 0.5442 - val_loss: 2.8060 - val_acc: 0.4612\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.6045 - acc: 0.8052 - val_loss: 2.0059 - val_acc: 0.6075\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.4102 - acc: 0.8723 - val_loss: 1.0139 - val_acc: 0.7363\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.3345 - acc: 0.8951 - val_loss: 0.3237 - val_acc: 0.9072\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.2875 - acc: 0.9101 - val_loss: 0.1854 - val_acc: 0.9455\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.2643 - acc: 0.9179 - val_loss: 0.1530 - val_acc: 0.9547\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.2408 - acc: 0.9247 - val_loss: 0.1298 - val_acc: 0.9598\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.2356 - acc: 0.9272 - val_loss: 0.1254 - val_acc: 0.9604\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.2288 - acc: 0.9286 - val_loss: 0.1057 - val_acc: 0.9688\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.2184 - acc: 0.9317 - val_loss: 0.0924 - val_acc: 0.9702\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.2049 - acc: 0.9359 - val_loss: 0.1151 - val_acc: 0.9660\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.2027 - acc: 0.9370 - val_loss: 0.0893 - val_acc: 0.9738\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1987 - acc: 0.9384 - val_loss: 0.0844 - val_acc: 0.9736\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1917 - acc: 0.9416 - val_loss: 0.0917 - val_acc: 0.9713\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1880 - acc: 0.9425 - val_loss: 0.0760 - val_acc: 0.9740\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1789 - acc: 0.9450 - val_loss: 0.0941 - val_acc: 0.9714\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1799 - acc: 0.9432 - val_loss: 0.0783 - val_acc: 0.9768\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1762 - acc: 0.9459 - val_loss: 0.0856 - val_acc: 0.9720\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1725 - acc: 0.9470 - val_loss: 0.0734 - val_acc: 0.9773\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1696 - acc: 0.9467 - val_loss: 0.0680 - val_acc: 0.9770\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1738 - acc: 0.9460 - val_loss: 0.0692 - val_acc: 0.9758\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1735 - acc: 0.9466 - val_loss: 0.0720 - val_acc: 0.9773\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1718 - acc: 0.9479 - val_loss: 0.0779 - val_acc: 0.9749\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1685 - acc: 0.9466 - val_loss: 0.0717 - val_acc: 0.9758\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1680 - acc: 0.9481 - val_loss: 0.0798 - val_acc: 0.9751\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1634 - acc: 0.9493 - val_loss: 0.0848 - val_acc: 0.9753\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1622 - acc: 0.9492 - val_loss: 0.0610 - val_acc: 0.9788\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1592 - acc: 0.9516 - val_loss: 0.0706 - val_acc: 0.9769\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1624 - acc: 0.9495 - val_loss: 0.0712 - val_acc: 0.9789\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1557 - acc: 0.9517 - val_loss: 0.0678 - val_acc: 0.9774\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1633 - acc: 0.9498 - val_loss: 0.0848 - val_acc: 0.9720\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1620 - acc: 0.9495 - val_loss: 0.0708 - val_acc: 0.9770\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1668 - acc: 0.9482 - val_loss: 0.0764 - val_acc: 0.9768\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1565 - acc: 0.9509 - val_loss: 0.0705 - val_acc: 0.9776\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1553 - acc: 0.9516 - val_loss: 0.0638 - val_acc: 0.9798\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1562 - acc: 0.9515 - val_loss: 0.0744 - val_acc: 0.9767\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1562 - acc: 0.9513 - val_loss: 0.0645 - val_acc: 0.9786\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1545 - acc: 0.9517 - val_loss: 0.0626 - val_acc: 0.9801\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1517 - acc: 0.9535 - val_loss: 0.0667 - val_acc: 0.9786\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1607 - acc: 0.9507 - val_loss: 0.0738 - val_acc: 0.9771\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1542 - acc: 0.9525 - val_loss: 0.0689 - val_acc: 0.9786\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1540 - acc: 0.9514 - val_loss: 0.0673 - val_acc: 0.9789\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1557 - acc: 0.9522 - val_loss: 0.0643 - val_acc: 0.9801\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1528 - acc: 0.9528 - val_loss: 0.0727 - val_acc: 0.9776\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1563 - acc: 0.9518 - val_loss: 0.0746 - val_acc: 0.9768\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1497 - acc: 0.9534 - val_loss: 0.0628 - val_acc: 0.9799\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1509 - acc: 0.9532 - val_loss: 0.0671 - val_acc: 0.9784\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1528 - acc: 0.9526 - val_loss: 0.0664 - val_acc: 0.9793\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1551 - acc: 0.9518 - val_loss: 0.0590 - val_acc: 0.9815\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1473 - acc: 0.9542 - val_loss: 0.0630 - val_acc: 0.9788\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1489 - acc: 0.9536 - val_loss: 0.0636 - val_acc: 0.9792\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1586 - acc: 0.9503 - val_loss: 0.0627 - val_acc: 0.9793\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1528 - acc: 0.9532 - val_loss: 0.0646 - val_acc: 0.9795\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1508 - acc: 0.9535 - val_loss: 0.0615 - val_acc: 0.9800\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1529 - acc: 0.9530 - val_loss: 0.0707 - val_acc: 0.9786\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.1552 - acc: 0.9520 - val_loss: 0.0595 - val_acc: 0.9797\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1539 - acc: 0.9521 - val_loss: 0.0718 - val_acc: 0.9776\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1501 - acc: 0.9531 - val_loss: 0.0752 - val_acc: 0.9747\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1515 - acc: 0.9532 - val_loss: 0.0649 - val_acc: 0.9784\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1472 - acc: 0.9536 - val_loss: 0.0610 - val_acc: 0.9812\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1506 - acc: 0.9541 - val_loss: 0.0599 - val_acc: 0.9812\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1494 - acc: 0.9531 - val_loss: 0.0650 - val_acc: 0.9808\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1439 - acc: 0.9566 - val_loss: 0.0651 - val_acc: 0.9793\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1484 - acc: 0.9539 - val_loss: 0.0625 - val_acc: 0.9809\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.1491 - acc: 0.9534 - val_loss: 0.0605 - val_acc: 0.9796\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1476 - acc: 0.9544 - val_loss: 0.0841 - val_acc: 0.9743\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1466 - acc: 0.9546 - val_loss: 0.0568 - val_acc: 0.9827\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1492 - acc: 0.9531 - val_loss: 0.0656 - val_acc: 0.9787\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1444 - acc: 0.9547 - val_loss: 0.0654 - val_acc: 0.9801\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1394 - acc: 0.9567 - val_loss: 0.0619 - val_acc: 0.9787\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1449 - acc: 0.9552 - val_loss: 0.0592 - val_acc: 0.9824\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1464 - acc: 0.9546 - val_loss: 0.0577 - val_acc: 0.9821\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1452 - acc: 0.9550 - val_loss: 0.0636 - val_acc: 0.9804\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1451 - acc: 0.9550 - val_loss: 0.0637 - val_acc: 0.9798\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1478 - acc: 0.9534 - val_loss: 0.0792 - val_acc: 0.9761\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1473 - acc: 0.9542 - val_loss: 0.0593 - val_acc: 0.9818\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1458 - acc: 0.9551 - val_loss: 0.0601 - val_acc: 0.9801\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1450 - acc: 0.9546 - val_loss: 0.0571 - val_acc: 0.9806\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1457 - acc: 0.9535 - val_loss: 0.0616 - val_acc: 0.9808\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1485 - acc: 0.9534 - val_loss: 0.0619 - val_acc: 0.9813\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1479 - acc: 0.9533 - val_loss: 0.0615 - val_acc: 0.9797\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1456 - acc: 0.9547 - val_loss: 0.0643 - val_acc: 0.9784\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1492 - acc: 0.9532 - val_loss: 0.0631 - val_acc: 0.9804\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1436 - acc: 0.9547 - val_loss: 0.0625 - val_acc: 0.9804\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1439 - acc: 0.9561 - val_loss: 0.0580 - val_acc: 0.9812\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1459 - acc: 0.9544 - val_loss: 0.0668 - val_acc: 0.9775\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1465 - acc: 0.9543 - val_loss: 0.0648 - val_acc: 0.9803\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1426 - acc: 0.9560 - val_loss: 0.0583 - val_acc: 0.9821\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1412 - acc: 0.9566 - val_loss: 0.0634 - val_acc: 0.9807\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1444 - acc: 0.9552 - val_loss: 0.0598 - val_acc: 0.9810\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1436 - acc: 0.9560 - val_loss: 0.0614 - val_acc: 0.9798\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1456 - acc: 0.9543 - val_loss: 0.0614 - val_acc: 0.9796\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1437 - acc: 0.9553 - val_loss: 0.0601 - val_acc: 0.9802\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1430 - acc: 0.9558 - val_loss: 0.0619 - val_acc: 0.9797\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1442 - acc: 0.9559 - val_loss: 0.0620 - val_acc: 0.9797\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1412 - acc: 0.9565 - val_loss: 0.0513 - val_acc: 0.9838\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1445 - acc: 0.9551 - val_loss: 0.0564 - val_acc: 0.9822\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1399 - acc: 0.9564 - val_loss: 0.0970 - val_acc: 0.9708\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1419 - acc: 0.9561 - val_loss: 0.0549 - val_acc: 0.9834\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1351 - acc: 0.9582 - val_loss: 0.0627 - val_acc: 0.9799\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1397 - acc: 0.9564 - val_loss: 0.0704 - val_acc: 0.9779\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1426 - acc: 0.9557 - val_loss: 0.0546 - val_acc: 0.9832\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1437 - acc: 0.9557 - val_loss: 0.0596 - val_acc: 0.9822\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1429 - acc: 0.9557 - val_loss: 0.0555 - val_acc: 0.9823\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1457 - acc: 0.9546 - val_loss: 0.0589 - val_acc: 0.9824\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1438 - acc: 0.9564 - val_loss: 0.0540 - val_acc: 0.9817\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1435 - acc: 0.9544 - val_loss: 0.0604 - val_acc: 0.9801\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1457 - acc: 0.9550 - val_loss: 0.0618 - val_acc: 0.9796\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1406 - acc: 0.9562 - val_loss: 0.0603 - val_acc: 0.9810\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1386 - acc: 0.9566 - val_loss: 0.0530 - val_acc: 0.9833\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1402 - acc: 0.9555 - val_loss: 0.0564 - val_acc: 0.9817\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1445 - acc: 0.9556 - val_loss: 0.0552 - val_acc: 0.9827\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1418 - acc: 0.9563 - val_loss: 0.0577 - val_acc: 0.9824\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1447 - acc: 0.9548 - val_loss: 0.0552 - val_acc: 0.9820\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1431 - acc: 0.9551 - val_loss: 0.0607 - val_acc: 0.9805\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1412 - acc: 0.9560 - val_loss: 0.0655 - val_acc: 0.9803\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1385 - acc: 0.9567 - val_loss: 0.0541 - val_acc: 0.9823\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1424 - acc: 0.9564 - val_loss: 0.0604 - val_acc: 0.9804\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1353 - acc: 0.9575 - val_loss: 0.0553 - val_acc: 0.9831\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1359 - acc: 0.9568 - val_loss: 0.0539 - val_acc: 0.9837\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1357 - acc: 0.9577 - val_loss: 0.0587 - val_acc: 0.9817\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1415 - acc: 0.9553 - val_loss: 0.0728 - val_acc: 0.9768\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1386 - acc: 0.9571 - val_loss: 0.0608 - val_acc: 0.9805\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1413 - acc: 0.9556 - val_loss: 0.0625 - val_acc: 0.9806\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1413 - acc: 0.9567 - val_loss: 0.0584 - val_acc: 0.9811\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1438 - acc: 0.9551 - val_loss: 0.0566 - val_acc: 0.9803\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1399 - acc: 0.9554 - val_loss: 0.0538 - val_acc: 0.9829\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1382 - acc: 0.9567 - val_loss: 0.0536 - val_acc: 0.9824\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1377 - acc: 0.9568 - val_loss: 0.0555 - val_acc: 0.9817\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1387 - acc: 0.9576 - val_loss: 0.0686 - val_acc: 0.9801\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1387 - acc: 0.9568 - val_loss: 0.0509 - val_acc: 0.9822\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1378 - acc: 0.9573 - val_loss: 0.0599 - val_acc: 0.9806\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1436 - acc: 0.9556 - val_loss: 0.0586 - val_acc: 0.9810\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1390 - acc: 0.9570 - val_loss: 0.0534 - val_acc: 0.9827\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1363 - acc: 0.9579 - val_loss: 0.0520 - val_acc: 0.9835\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1366 - acc: 0.9571 - val_loss: 0.0597 - val_acc: 0.9810\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1435 - acc: 0.9558 - val_loss: 0.0566 - val_acc: 0.9807\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1419 - acc: 0.9559 - val_loss: 0.0588 - val_acc: 0.9818\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1409 - acc: 0.9559 - val_loss: 0.0611 - val_acc: 0.9801\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1434 - acc: 0.9561 - val_loss: 0.0613 - val_acc: 0.9790\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1431 - acc: 0.9551 - val_loss: 0.0557 - val_acc: 0.9829\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1426 - acc: 0.9557 - val_loss: 0.0593 - val_acc: 0.9828\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1344 - acc: 0.9585 - val_loss: 0.0612 - val_acc: 0.9799\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1406 - acc: 0.9560 - val_loss: 0.0620 - val_acc: 0.9798\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1423 - acc: 0.9563 - val_loss: 0.0600 - val_acc: 0.9803\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1419 - acc: 0.9558 - val_loss: 0.0617 - val_acc: 0.9801\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1421 - acc: 0.9560 - val_loss: 0.0572 - val_acc: 0.9823\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 2s 107ms/step - loss: 0.1431 - acc: 0.9546 - val_loss: 0.0679 - val_acc: 0.9783\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1361 - acc: 0.9575 - val_loss: 0.0560 - val_acc: 0.9818\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1424 - acc: 0.9563 - val_loss: 0.0590 - val_acc: 0.9819\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1406 - acc: 0.9555 - val_loss: 0.0535 - val_acc: 0.9827\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1385 - acc: 0.9568 - val_loss: 0.0570 - val_acc: 0.9818\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1369 - acc: 0.9573 - val_loss: 0.0586 - val_acc: 0.9806\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1400 - acc: 0.9557 - val_loss: 0.0511 - val_acc: 0.9836\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1426 - acc: 0.9560 - val_loss: 0.0677 - val_acc: 0.9777\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1408 - acc: 0.9562 - val_loss: 0.0639 - val_acc: 0.9811\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1398 - acc: 0.9570 - val_loss: 0.0568 - val_acc: 0.9813\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1404 - acc: 0.9565 - val_loss: 0.0722 - val_acc: 0.9766\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1360 - acc: 0.9572 - val_loss: 0.0541 - val_acc: 0.9818\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1422 - acc: 0.9564 - val_loss: 0.0606 - val_acc: 0.9807\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1353 - acc: 0.9582 - val_loss: 0.0524 - val_acc: 0.9834\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1385 - acc: 0.9577 - val_loss: 0.0585 - val_acc: 0.9817\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1375 - acc: 0.9574 - val_loss: 0.0571 - val_acc: 0.9818\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1420 - acc: 0.9561 - val_loss: 0.0633 - val_acc: 0.9799\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1400 - acc: 0.9566 - val_loss: 0.0750 - val_acc: 0.9762\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1357 - acc: 0.9571 - val_loss: 0.0563 - val_acc: 0.9823\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1383 - acc: 0.9571 - val_loss: 0.0624 - val_acc: 0.9801\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1410 - acc: 0.9554 - val_loss: 0.0521 - val_acc: 0.9829\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1360 - acc: 0.9578 - val_loss: 0.0613 - val_acc: 0.9806\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1376 - acc: 0.9575 - val_loss: 0.0553 - val_acc: 0.9825\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1436 - acc: 0.9557 - val_loss: 0.0637 - val_acc: 0.9804\n",
      "Epoch 172/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1397 - acc: 0.9572 - val_loss: 0.0576 - val_acc: 0.9806\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1447 - acc: 0.9549 - val_loss: 0.0533 - val_acc: 0.9828\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1391 - acc: 0.9571 - val_loss: 0.0530 - val_acc: 0.9818\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1382 - acc: 0.9572 - val_loss: 0.0637 - val_acc: 0.9795\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1464 - acc: 0.9543 - val_loss: 0.0514 - val_acc: 0.9826\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1419 - acc: 0.9557 - val_loss: 0.0577 - val_acc: 0.9812\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1410 - acc: 0.9562 - val_loss: 0.0561 - val_acc: 0.9817\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1377 - acc: 0.9565 - val_loss: 0.0557 - val_acc: 0.9818\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.1399 - acc: 0.9568 - val_loss: 0.0678 - val_acc: 0.9786\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1403 - acc: 0.9561 - val_loss: 0.1045 - val_acc: 0.9689\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1383 - acc: 0.9571 - val_loss: 0.0578 - val_acc: 0.9820\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1365 - acc: 0.9575 - val_loss: 0.0533 - val_acc: 0.9825\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1404 - acc: 0.9574 - val_loss: 0.0507 - val_acc: 0.9826\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1397 - acc: 0.9574 - val_loss: 0.0538 - val_acc: 0.9828\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1420 - acc: 0.9557 - val_loss: 0.0531 - val_acc: 0.9828\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1401 - acc: 0.9556 - val_loss: 0.0598 - val_acc: 0.9810\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1364 - acc: 0.9573 - val_loss: 0.0529 - val_acc: 0.9840\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.1420 - acc: 0.9557 - val_loss: 0.0550 - val_acc: 0.9841\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1429 - acc: 0.9552 - val_loss: 0.0507 - val_acc: 0.9838\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1331 - acc: 0.9584 - val_loss: 0.0567 - val_acc: 0.9818\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.1395 - acc: 0.9578 - val_loss: 0.0589 - val_acc: 0.9820\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.1447 - acc: 0.9548 - val_loss: 0.0610 - val_acc: 0.9803\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.1398 - acc: 0.9562 - val_loss: 0.0544 - val_acc: 0.9819\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.1390 - acc: 0.9570 - val_loss: 0.0528 - val_acc: 0.9822\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 2s 111ms/step - loss: 0.1397 - acc: 0.9563 - val_loss: 0.0575 - val_acc: 0.9818\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 2s 115ms/step - loss: 0.1380 - acc: 0.9566 - val_loss: 0.0561 - val_acc: 0.9828\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 2s 114ms/step - loss: 0.1380 - acc: 0.9574 - val_loss: 0.0574 - val_acc: 0.9823\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 2s 117ms/step - loss: 0.1339 - acc: 0.9588 - val_loss: 0.0587 - val_acc: 0.9805\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.1373 - acc: 0.9570 - val_loss: 0.0586 - val_acc: 0.9804\n"
     ]
    }
   ],
   "source": [
    "qmodel.load_weights(\"qmodel.h5\")\n",
    "with cur_strategy.scope():\n",
    "  optimizer = Adam(lr=0.02)\n",
    "  qmodel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "  qmodel.fit(x_train, y_train, epochs=200, batch_size=4096, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJCkMdAcjnoh"
   },
   "source": [
    "Perfect! You have learned how to perform automatic quantization using AutoQKeras with QKeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/dm_python:dm_notebook3_tpu",
    "kind": "private"
   },
   "name": "AutoQKeras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('hls4ml-tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "978a78fee93b9f75d300423e922c5a4da2d32993b15c09db9f940a22d4b78528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
